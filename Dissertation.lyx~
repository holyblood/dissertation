#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass amsbook
\begin_preamble

\usepackage{verbatim}
%\usepackage[framed,numbered,autolinebreaks,useliterate]{mcode}
%\usepackage{SAS}
%\usepackage[noautotitles-sas,countbysection]{SASnRdisplay} 
% need SASnRdisplay in your dir
%\usepackage{tikz}
%\usetikzlibrary{positioning}
%\usetikzlibrary{bayesnet}
%\usepackage{pgfplots}
%\usepackage{beamerthemesplit}
%\usetheme{Berkeley}
%\usecolortheme{dolphin}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Lp}[1][p]{\mathcal{L}_{#1}}
\newcommand{\diff}{\,\mathrm{d}}
\newcommand{\iid}{\mathrm{iid}\,}
\newcommand{\ascv}{\,\mathrm{a.s.}\,}
\newcommand{\Xitn}[2][X]{{#1}_1 \mathrm{,} \ldots \mathrm{,} {#1}_{#2}}
\newcommand{\convdist}[1][d]{\overset{#1}{\rightarrow}}
%\renewcommand{\labelenumi}{(\alph{enumi})}
\renewcommand{\labelenumii}{(\roman{enumii})}
\renewcommand{\labelenumi}{\arabic{enumi}}
%\renewcommand{\ttdefault}{txtt}% make SAScode bold keyword
 \newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\bibpunct{(}{)}{;}{a}{,}{;}
\end_preamble
\use_default_options true
\begin_removed_modules
theorems-ams
\end_removed_modules
\begin_modules
eqs-within-sections
figs-within-sections
theorems-ams-bytype
theorems-ams-extended-bytype
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
Literature Review
\end_layout

\begin_layout Section
Empirical Likelihood
\end_layout

\begin_layout Standard
Empirical likelihood is a nonparametric method of inference based on data-driven
 likelihood ratio function.
 Like the bootstrap and jackknife, empirical likelihood inference does not
 require us to specify a family of distributions for the data.
 Like parametric likelihood methods, empirical likelihood makes an automatic
 determination of the shape of confidence regions; it straightforwardly
 incorporates side information expressed through constraints or prior distributi
ons; it extends to biased sampling and censored data, and it has very favorable
 asymptotic power properties.
\end_layout

\begin_layout Standard
Empirical likelihood was first proposed by 
\begin_inset CommandInset citation
LatexCommand citet
key "owen1988empirical"

\end_inset

 as follows.
 For i.i.d sample 
\begin_inset Formula $X_{1},\ldots,X_{n},$
\end_inset

 the empirical likelihood ration function is 
\begin_inset Formula 
\[
R\left(\mu\right)=\max\left\{ \left.\prod_{i=1}^{n}nw_{i}\right|\sum_{i=1}^{n}w_{i}X_{i}=\mu,w_{i}\ge0,\sum_{i=1}^{n}w_{i}=1\right\} ,
\]

\end_inset

and the resulting asymptotic confidence region is 
\begin_inset Formula 
\begin{equation}
\left\{ \mu\mid R\left(\mu\right)\ge\chi_{1,\alpha}^{2}\right\} =\left\{ \left.\sum_{i=1}^{n}w_{i}X_{i}\right|\prod_{i=1}^{n}nw_{i}\ge\chi_{1,\alpha}^{2},w_{i}\ge0,\sum_{i=1}^{n}w_{i}=1\right\} .\label{eq:ci-original-el}
\end{equation}

\end_inset


\begin_inset Note Comment
status open

\begin_layout Plain Layout
rubin 1981 bayesian bootstrap in wu changbao.
 ? lit review asymp exp obj prior.
 
\end_layout

\end_inset

The following research generalizes this method to more complex settings,
 
\begin_inset Formula 
\[
R\left(\theta\right)=\max\left\{ \left.d\left(\sum_{i=1}^{n}w_{i}I_{\left[x=X_{i}\right]},\sum_{i=1}^{n}\frac{1}{n}I_{\left[x=X_{i}\right]}\right)\right|\sum_{i=1}^{n}w_{i}g\left(X_{i},\theta\right)=0,w_{i}\ge0,\sum_{i=1}^{n}w_{i}=1\right\} ,
\]

\end_inset

where 
\begin_inset Formula $g\left(\cdot,\cdot\right)$
\end_inset

 is general estimating function, and 
\begin_inset Formula $d\left(\cdot,\cdot\right)$
\end_inset

 is some divergence measure.
\begin_inset Note Comment
status open

\begin_layout Plain Layout
prob cov ch2 ch3
\end_layout

\end_inset

 For more detail reference, authors suggest 
\begin_inset CommandInset citation
LatexCommand citet
key "owen2010empirical"

\end_inset

.
\end_layout

\begin_layout Subsection
Different 
\begin_inset Formula $g$
\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "owen1991empirical"

\end_inset

 first considered using normal equations in linear regression as 
\begin_inset Formula $g$
\end_inset

.
 Laterly, almost all the facets in traditional linear regression were transplant
ed into empirical likelihood.
 
\begin_inset CommandInset citation
LatexCommand citet
key "chen1993accuracy,chen1994empirical"

\end_inset

 developed confidence regions for regression coefficients.
 
\begin_inset CommandInset citation
LatexCommand citet
key "jing1995two,adimari1995empirical"

\end_inset

 considered the problem of comparing the means of two populations.
 
\begin_inset CommandInset citation
LatexCommand citet
key "davidian1987variance"

\end_inset

 investigated different variance structures in regression.
 
\begin_inset CommandInset citation
LatexCommand citet
key "kolaczyk1995information"

\end_inset

 formulated empirical information criterion as an empirical likelihood version
 of AIC.
 
\end_layout

\begin_layout Standard
Besides linear regression, the authors in this area also take effort to
 incorporate more complicate models into empirical likelihood.
 
\begin_inset CommandInset citation
LatexCommand citet
key "chen1993smoothed"

\end_inset

 used kernel smoothing technique to construct 
\begin_inset Formula $g$
\end_inset

 for quantiles.
 
\begin_inset CommandInset citation
LatexCommand citet
key "whang2006smoothed"

\end_inset

 extended this technique into empirical likelihood quantile regression.
 
\begin_inset CommandInset citation
LatexCommand citet
key "kolaczyk1994empirical"

\end_inset

 further extended 
\begin_inset Formula $g$
\end_inset

 into generalized linear model.
 
\begin_inset CommandInset citation
LatexCommand citet
key "hall1993empirical"

\end_inset

 studied empirical likelihood confidence bands for kernel density estimates.
 
\begin_inset CommandInset citation
LatexCommand citet
key "chen2000empirical"

\end_inset

 studied the empirical likelihood confidence intervals for local linear
 kernel smoothing.
 
\begin_inset CommandInset citation
LatexCommand citet
key "peng2004empirical"

\end_inset

 investigated the empirical likelihood confidence intervals in heavy-tailed
 distribution.
\end_layout

\begin_layout Standard
Data drawn from sophisticate designs are also taken into consideration.
 
\begin_inset CommandInset citation
LatexCommand citet
key "qin1993empirical"

\end_inset

 applied empirical likelihood to biased sampling data, whose distribution
 
\begin_inset Formula $G$
\end_inset

 related the distribution of interest 
\begin_inset Formula $F$
\end_inset

 through a biasing function 
\begin_inset Formula $u$
\end_inset

, 
\begin_inset Formula 
\[
G\left(A\right)=\frac{\int_{A}u\left(y\right)\diff F\left(y\right)}{\int u\left(y\right)\diff F\left(y\right)}.
\]

\end_inset

This result was further generalized by 
\begin_inset CommandInset citation
LatexCommand citet
key "qin1997goodness,qin1999empirical,qin1998semiparametric"

\end_inset

 into different situations with biased sampling.
 
\begin_inset CommandInset citation
LatexCommand citet
key "chen1993empirical"

\end_inset

 presented empirical likelihood for samples from a finite population.
 
\begin_inset CommandInset citation
LatexCommand citet
key "chen1999pseudo"

\end_inset

 formulated an empirical likelihood that respected design weights.
 
\begin_inset CommandInset citation
LatexCommand citet
key "wu2001model"

\end_inset

 considered a setting where the entire population is know but only the sample
 are available.
 
\begin_inset CommandInset citation
LatexCommand citet
key "loh1996latin"

\end_inset

 investigated Latin hypercube samples by empirical likelihood confidence
 region.
 Empirical likelihood analysis of cumulative hazard function was established
 by 
\begin_inset CommandInset citation
LatexCommand citet
key "murphy1995likelihood"

\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand citet
key "adimari1997empirical"

\end_inset

 considered empirical likelihood inferences for the mean under independent
 right censoring and later 
\begin_inset CommandInset citation
LatexCommand citet
key "pan2002empirical"

\end_inset

 extended it to do inference about more general functionals of the hazard
 function 
\begin_inset Formula $\int q_{n}\left(x\right)\diff\Lambda\left(x\right)$
\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand citet
key "murphy1997semiparametric"

\end_inset

 brought in general double censoring cases and proportional hazard model.
\end_layout

\begin_layout Standard
Inference on infinite dimension subject, such as CDF and quantile function
 are also visited.
 
\begin_inset CommandInset citation
LatexCommand citet
key "owen1995nonparametric"

\end_inset

 built exact confidence bands for CDF based on empirical likelihood.
 
\begin_inset CommandInset citation
LatexCommand citet
key "hollander1997likelihood"

\end_inset

 found asymptotic confidence bands for survival function from right censoring
 data.
 
\begin_inset CommandInset citation
LatexCommand citet
key "zhang1996confidence,zhang1999bootstrapping"

\end_inset

 constructed confidence bands when auxiliary information was available.
 
\end_layout

\begin_layout Standard
Finally, for independent data, authors refer 
\begin_inset CommandInset citation
LatexCommand citet
key "qin1994empirical"

\end_inset

 as an useful general case, which analyzed the behavior of empirical likelihood
 when 
\begin_inset Formula $g$
\end_inset

 was a smooth estimating function and number of parameters was less than
 number of constraint equations.
 They proposed the maximum empirical likelihood estimator which could be
 viewed as an alternative to least square method when the data could not
 square with the model.
 
\begin_inset CommandInset citation
LatexCommand citet
key "molanes2009empirical"

\end_inset

 extended this work into non-smooth criterion function.
 
\end_layout

\begin_layout Standard
Dependent data can also be analyzed by empirical likelihood.
 
\begin_inset CommandInset citation
LatexCommand citet
key "chuang2002empirical"

\end_inset

 applied empirical likelihood to unstable auto-regressions.
 
\begin_inset CommandInset citation
LatexCommand citet
key "kitamura1997empirical"

\end_inset

 developed block-wise empirical likelihood for weakly dependent data and
 extended the result in 
\begin_inset CommandInset citation
LatexCommand citet
key "qin1994empirical"

\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand citet
key "chen2009smoothed"

\end_inset

 combined block-wise and smooth techniques in order to estimate quantiles
 for weakly dependent data.
 
\begin_inset CommandInset citation
LatexCommand citet
key "nordman2007empirical"

\end_inset

 applied block-wise empirical likelihood for long-range dependence process
 but the limit distribution became multiple Wiener integral instead of simple
 
\begin_inset Formula $\chi^{2}$
\end_inset

.
 The spectral approach to empirical likelihood was due to 
\begin_inset CommandInset citation
LatexCommand citet
key "monti1997empirical"

\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand citet
key "mykland1995dual"

\end_inset

 extended empirical likelihood into martingale by dual likelihood technique.
 Suppose 
\begin_inset Formula $m_{n}\left(\theta\right)$
\end_inset

 is a zero mean martingale depending on data and model, for example, a sample
 estimating equation or martingale leading to Nelson--Aalen estimator, then
 the log dual likelihood 
\begin_inset Formula $l\left(\mu\right)$
\end_inset

 for dual parameter 
\begin_inset Formula $\mu$
\end_inset

 and fixed 
\begin_inset Formula $\theta$
\end_inset

 is defined by partial differential equations 
\begin_inset Formula 
\[
m_{n}\left(\theta\right)=\left.\nabla l_{\theta}\left(\mu\right)\right|_{\mu=0},
\]

\end_inset

 or through Dol
\begin_inset IPA

\begin_layout Standard
́e
\end_layout

\end_inset

ans--Dade multiplicative martingale 
\begin_inset Formula 
\[
l_{\theta}\left(\mu\right)=-\mu^{T}\Lambda_{t}\left(\theta\right)+\sum_{s\le t}\left(1+\mu^{T}\Delta m_{s}\left(\theta\right)\right).
\]

\end_inset

and the corresponding likelihood ratio test is 
\begin_inset Formula $\mathrm{LR}=2\sup_{\mu}l_{\theta}\left(\mu\right)$
\end_inset

.
 This definition coincides empirical likelihood when data is i.i.d and time
 is discrete.
 
\begin_inset CommandInset citation
LatexCommand citet
key "wang2011empirical"

\end_inset

 analyzed quantile regression with longitudinal data by empirical likelihood.
 A more recent study by 
\begin_inset CommandInset citation
LatexCommand citet
key "bandyopadhyay2015frequency"

\end_inset

 introduced empirical likelihood into spatial data analysis.
 
\end_layout

\begin_layout Subsection
Different 
\begin_inset Formula $d$
\end_inset

 
\end_layout

\begin_layout Standard
There is also literature on the effect of different empirical divergence
 measures in empirical likelihood settings.
 
\begin_inset CommandInset citation
LatexCommand citet
key "hall1999biased"

\end_inset

 used empirical entropy to construct robust confidence regions for non-robust
 statistics like the mean.
 
\begin_inset CommandInset citation
LatexCommand citet
key "baggerly1998empirical"

\end_inset

 extended 
\begin_inset Formula $d$
\end_inset

 to Cressie-Read family in sample mean case and proved empirical likelihood
 was the only Bartlett correctable member of that family.
 In econometrics, 
\begin_inset CommandInset citation
LatexCommand citet
key "mittelhammer2000econometric"

\end_inset

 used empirical entropy and Cressie--Read divergence measure in 
\begin_inset Formula $d$
\end_inset

 and linked maximum empirical likelihood estimator to general method of
 moment.
 To overcome the drawback that empirical likelihood domain 
\begin_inset Formula $\Theta$
\end_inset

 was usually smaller than the full parameter space 
\begin_inset Formula $\mathbb{R}^{p}$
\end_inset

, 
\begin_inset CommandInset citation
LatexCommand citet
key "tsao2014extended"

\end_inset

 defined a surjective composite similarity mapping 
\begin_inset Formula 
\[
h^{C}\left(\theta\right)=\hat{\theta}_{\mathrm{MELE}}+\left(1+\frac{R\left(\theta\right)}{2n}\right)\left(\theta-\hat{\theta}_{\mathrm{MELE}}\right),
\]

\end_inset

from 
\begin_inset Formula $\Theta$
\end_inset

 to 
\begin_inset Formula $\mathbb{R}^{p}$
\end_inset

, and used its generalized inverse to adjust original empirical likelihood
 into 
\begin_inset Formula $R\left(\left(h^{C}\right)^{-1}\left(\theta\right)\right)$
\end_inset

.
 
\end_layout

\begin_layout Subsection
Higher-Order Properties 
\end_layout

\begin_layout Standard
Higher order properties are also comprehensively investigated.
 Bartlett correctable is one of the favorite aspect of using empirical likelihoo
d.
 This correction is usually based on Edgeworth expansions of empirical likelihoo
d version, and has the following form: instead of using 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:ci-original-el"

\end_inset

, the more accurate version is 
\begin_inset Formula 
\[
\left\{ \theta\mid-2\log R\left(\theta\right)\le\left(1+\frac{a}{n}\right)\chi_{p,1-\alpha}^{2}\right\} ,
\]

\end_inset

or 
\begin_inset Formula 
\[
\left\{ \theta\mid-2\log R\left(\theta\right)\le\left(1-\frac{a}{n}\right)^{-1}\chi_{p,1-\alpha}^{2}\right\} .
\]

\end_inset

The fist Bartlett correction was published by 
\begin_inset CommandInset citation
LatexCommand citet
key "diciccio1991empirical"

\end_inset

 for sample mean.
 
\begin_inset CommandInset citation
LatexCommand citet
key "zhang1996accuracy"

\end_inset

 extended it to general estimating function.
 However, 
\begin_inset CommandInset citation
LatexCommand citet
key "lazar1999empirical"

\end_inset

 reported failure of Bartlett correction when there were nuisance parameters.
 This is the principle way in which empirical likelihood shows different
 behavior from parametric likelihoods.
 This problem was solved by 
\begin_inset CommandInset citation
LatexCommand citet
key "chen2006bartlett"

\end_inset

.
 Other Bartlett correction usually follows the development of the empirical
 likelihood model.
 
\begin_inset CommandInset citation
LatexCommand citet
key "biao1998note"

\end_inset

 showed that there was no first order asymptotic benefit from global side
 constraints in kernel density estimation.
 However, 
\begin_inset CommandInset citation
LatexCommand citet
key "chen1997empirical"

\end_inset

 proved the second order asymptotic benefit could be expected via imposing
 side information.
 
\begin_inset CommandInset citation
LatexCommand citet
key "newey2004higher"

\end_inset

 considered the higher order properties of maximum empirical likelihood
 estimators in general method of moment setting.
 
\begin_inset CommandInset citation
LatexCommand citet
key "kitamura2001asymptotic"

\end_inset

 gave some theorems on the large deviation properties of empirical likelihood,
 and his simulation suggested empirical likelihood ranked top when at hypotheses
 father from the null.
 
\end_layout

\begin_layout Subsection
High Dimension and Sparsity
\end_layout

\begin_layout Standard
High dimensional data and sparsity are pivot topics in recent years and
 they also shed light on empirical likelihood area.
 
\begin_inset CommandInset citation
LatexCommand citet
key "tsao2004bounds"

\end_inset

 pointed out the failure of empirical likelihood even if the number of parameter
s was moderately larger than number of observations 
\begin_inset Formula $p/n>1/2$
\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand citet
key "hjort2009extending"

\end_inset

 rescaled the empirical likelihood itself by number of parameter and asserted
 a normal limit distribution.
 Their work was generalized by 
\begin_inset CommandInset citation
LatexCommand citet
key "chen2009effects"

\end_inset

 with less restrictions.
 
\begin_inset CommandInset citation
LatexCommand citet
key "chen2008adjusted,emerson2009calibration"

\end_inset

 proposed adjusted empirical likelihood by adding pseudo observations.
 
\begin_inset CommandInset citation
LatexCommand citet
key "bartolucci2007penalized"

\end_inset

 added Tikhonov regularization defined sample covariance matrix and succeeded
 when growing 
\begin_inset Formula $p<n$
\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand citet
key "lahiri2012penalized"

\end_inset

 added component wise penalties and extended the result into weak, long-range
 dependency and 
\begin_inset Formula $p>n$
\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand citet
key "tang2010penalized"

\end_inset

 added popular SCAD penalty to traditional empirical likelihood for population
 mean and validated sparsity in high dimensional settings.
 
\begin_inset CommandInset citation
LatexCommand citet
key "leng2012penalized"

\end_inset

 extended this work to generalized estimating equations and more general
 penalties.
 An interesting result from 
\begin_inset CommandInset citation
LatexCommand citet
key "chang2015high"

\end_inset

 shows the usual block-wise empirical likelihood is still working if the
 number of constraints is growing with number of parameters in some rate.
\end_layout

\begin_layout Subsection
Bayesian 
\end_layout

\begin_layout Standard
Comparing with fruitful frequentist's research in empirical likelihood,
 Bayesian counterparts are just at the beginning.
 
\begin_inset CommandInset citation
LatexCommand citet
key "lazar2003bayesian"

\end_inset

 started the Bayesian empirical likelihood and did many simulations to prove
 the validation.
 
\begin_inset CommandInset citation
LatexCommand citet
key "schennach2005bayesian,schennach2007point"

\end_inset

 explained the Bayesian exponentially tilted empirical likelihood as a limit
 of nonparametric procedure.
 
\begin_inset CommandInset citation
LatexCommand citet
key "grendar2009asymptotic"

\end_inset

 found the equivalence between maximum empirical likelihood estimators and
 Bayesian maximum a posteriori probability estimators under model misspecificati
on.
 
\begin_inset CommandInset citation
LatexCommand citet
key "lancaster2010bayesian"

\end_inset

 used Bayesian exponentially tilted empirical likelihood for quantile regression.
 
\begin_inset CommandInset citation
LatexCommand citet
key "fang2005expected,fang2006empirical"

\end_inset

 established probability matching prior for empirical likelihood for population
 mean case, which allowed the credible intervals had also validity in frequentis
t's sense.
 
\begin_inset CommandInset citation
LatexCommand citet
key "chang2008bayesian"

\end_inset

 consolidated this result by showing empirical likelihood was the only member
 who enjoyed both Bayesian and frequentist's validity.
 
\begin_inset CommandInset citation
LatexCommand citet
key "vexler2013nonparametric"

\end_inset

 used empirical likelihood as a non-parametric method to estimate Bayes
 factor in model selection.
 
\begin_inset CommandInset citation
LatexCommand citet
key "vexler2014posterior"

\end_inset

 showed some James-Stein phenomenon for Bayesian empirical likelihood.
 
\begin_inset CommandInset citation
LatexCommand citet
key "rao2010bayesian"

\end_inset

 implemented another way to put prior, that is, a prior on empirical weights
 
\begin_inset Formula $w_{i}$
\end_inset

 in complex survey settings.
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
more papers in abc folder, pro and con of el
\end_layout

\end_inset


\end_layout

\begin_layout Section
Option Pricing
\end_layout

\begin_layout Standard
Since the seminal works by Black and Scholes (1973) and Merton (1973), option
 valuation methodologies have been extensively developed.
 The Black-Scholes model has become one of the most well-known discoveries
 in finance literature, which relates the cross-sectional properties of
 option prices with the underlying assets returns distribution.
 However, Rubinstein (1985), Melino and Turnbull (1990) pointed out several
 limitations in the Black-Scholes model due to the strong assumptions, such
 as non-normality of the returns, stochastic volatility (implied volatility
 smile), jumps and others.
 Both parametric and nonparametric approaches have been proposed to deal
 with these issue.
 
\end_layout

\begin_layout Standard
Scott (1987), Hull and White (1987) and Wiggins (1987) extended the Black
 and Scholes model and allowed the volatility to be stochastic.
 Heston (1993) developed a closed-form solution for option pricing with
 the underlying assets volatility being stochastic.
 Duan (1995) proposed a GARCH option pricing model in an attempt to explain
 some systematic biases associated with the Black-Scholes model.
 Later Heston and Nandi (2000) provided a closed-form solution for option
 pricing with the underlying assets volatility following GARCH(p,q) process.
 Bates (1996), Bakshi, Cao and Chen (1997) derived an option pricing model
 with stochastic volatility and jumps.
 Kou (2002) provided a solution to pricing the option with the double exponentia
l jumps diffusion process.
 Carr and Madan (1999) introduced the fast Fourier transform approach to
 option pricing given a specified characteristic function of the return,
 which provides an efficient computational algorithm to calculate the option
 prices.
 For further reference, see Duffie et al.
 (2000), Bakshi and Madan (2000) and Carr and Madan (2009) among others.
 All these methods are parametric based, which assume a parametric form
 of either the distribution of the underlying assets returns or the characterist
ic function of the underlying assets returns.
 
\end_layout

\begin_layout Standard
Nonparametric approaches have also been proposed to capture the underlying
 asset and option price data to reconstruct the structure of the diffusion
 process.
 For example, Hutchinson, Lo and Poggio (1994) applied the neural network
 techniques to price the derivatives.
 Ait-Sahalia and Lo (1998) used the kernel regression to fit the state-price
 density implicitly in option pricing.
 Ait-Sahalia (1996) proposed a nonparametric pricing estimation procedure
 for interest rate derivative securities under the assumption that the unknown
 volatility is independent of time.
 Stutzer (1996) adopted the canonical valuation method, which incorporats
 the no-arbitrary principle embodied in the formula for calculating the
 expectation of the discounted value of assets under the risk-neutral probabilit
y distribution.
 
\begin_inset Note Comment
status open

\begin_layout Plain Layout
add to bibiograph
\end_layout

\end_inset


\end_layout

\begin_layout Section
Approximate Bayesian Computation
\end_layout

\begin_layout Standard
Approximate Bayesian Computation (ABC) methods, also known as likelihood-free
 techniques, have appeared in the past ten years as the most satisfactory
 approach to intractable likelihood problems, first in genetics then in
 a broader spectrum of applications.
 Intractable likelihood is a common phenomenon in statistical modeling.
 
\end_layout

\begin_layout Itemize
The likelihood is expressed as a multidimensional integral, 
\begin_inset Formula 
\[
l\left(\theta\mid Y\right)=\int l^{*}\left(\theta\mid Y,u\right)\diff u,
\]

\end_inset

where 
\begin_inset Formula $Y$
\end_inset

 is observation, 
\begin_inset Formula $u$
\end_inset

 is latent variable and 
\begin_inset Formula $\theta$
\end_inset

 is the parameter of interest, for example, coalecent model in population
 genetics.
 Typically when the dimension of 
\begin_inset Formula $u$
\end_inset

 is large, the convergence properties of MCMC like Gibbs sampler and Metropolis-
-Hastings algorithm are too poor to use in practice.
 
\end_layout

\begin_layout Itemize
The normalizing constant is unknown.
 This is typically the case of Gibbs random fields in order to model spatially
 correlated data such as epidemiology and image analysis.
 
\end_layout

\begin_layout Itemize
The likelihood function is not completely known, that is 
\begin_inset Formula 
\[
l\left(\theta\mid Y\right)=l_{1}\left(\theta\mid Y\right)l_{2}\left(\theta\right).
\]

\end_inset

In the past, Laplace approximations by 
\begin_inset CommandInset citation
LatexCommand citet
key "tierney1986accurate"

\end_inset

 or variational Bayes solutions by 
\begin_inset CommandInset citation
LatexCommand citet
key "jaakkola2000bayesian"

\end_inset

 have been advanced for such problems.
 However, Laplace approximations require some analytic knowledge of the
 posterior distribution, while variational Bayes solutions replace the true
 model with another pseudo-model which is usually much simpler and thus
 misses some of the features of the original model.
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
abc machine learning
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The idea of ABC dated back to 
\begin_inset CommandInset citation
LatexCommand citet
key "rubin1984bayesianly"

\end_inset

 as an intuitive way to understand posterior distributions from a frequentist's
 perspective, because parameters from the posterior are more likely to be
 those that could have generated the observed data.
 The first ABC algorithm was born in 
\begin_inset CommandInset citation
LatexCommand citet
key "tavare1997inferring"

\end_inset

 and  
\begin_inset CommandInset citation
LatexCommand citet
key "pritchard1999population"

\end_inset

 as 
\begin_inset Float algorithm
placement h
wide false
sideways false
status open

\begin_layout Enumerate
Sample parameters 
\begin_inset Formula $\theta_{i}$
\end_inset

 from the prior distribution 
\begin_inset Formula $\pi\left(\theta\right)$
\end_inset

;
\end_layout

\begin_layout Enumerate
Sample data 
\begin_inset Formula $Z_{i}$
\end_inset

 based on the model 
\begin_inset Formula $f\left(z\mid\theta_{i}\right)$
\end_inset

;
\end_layout

\begin_layout Enumerate
Accept 
\begin_inset Formula $\theta_{i}$
\end_inset

 if 
\begin_inset Formula $\rho\left(S\left(Z_{i}\right),S\left(X_{\mathrm{obs}}\right)\right)\le\varepsilon$
\end_inset

,
\color red
 
\color black
for some metric 
\begin_inset Formula $\rho$
\end_inset

 and summary statistics 
\begin_inset Formula $S$
\end_inset

.
 
\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:Prichard-ABC"

\end_inset

Prichard's Modified ABC
\end_layout

\end_inset


\end_layout

\end_inset

For more detail reference, authors suggest 
\begin_inset CommandInset citation
LatexCommand citet
key "marin2012approximate"

\end_inset

.
\end_layout

\begin_layout Subsection
Different Sampling Schemes
\end_layout

\begin_layout Standard
In practice, if one uses non-informative prior, simulation would be very
 inefficient, because of high rejection rate of prior sample locating in
 low posterior probability regions.
 As an answer to this problem, 
\begin_inset CommandInset citation
LatexCommand citet
key "marjoram2003markov"

\end_inset

 applied Metropolis--Hastings algorithm in sampling from prior distribution
 and built MCMC-ABC algorithm.
 
\begin_inset CommandInset citation
LatexCommand citet
key "picchini2014inference"

\end_inset

 used this method to analysis data from stochastic differential equations.
 
\begin_inset CommandInset citation
LatexCommand citet
key "lee2014variance"

\end_inset

 backed up this method by several theoretical results including variance
 bound and geometric ergodicity.
 
\begin_inset CommandInset citation
LatexCommand citet
key "ratmann2009model"

\end_inset

 used tolerance level 
\begin_inset Formula $\varepsilon=\rho\left(S_{i},S_{\mathrm{obs}}\right)$
\end_inset

 as an additional parameter of the model and proposed ABC
\begin_inset Formula $_{\mu}$
\end_inset

, as method to assess model uncertainty.
 
\begin_inset CommandInset citation
LatexCommand citet
key "wilkinson2013approximate"

\end_inset

 replaced the hard accept-reject scheme by soft kernel smoothing, called
 noisy ABC, 
\begin_inset Formula 
\[
\pi_{\varepsilon}\left(\theta,z\mid Y\right)\propto\pi\left(\theta\right)f\left(z\mid\theta\right)K_{\varepsilon}\left(Y-z\right),
\]

\end_inset

where 
\begin_inset Formula $K_{\varepsilon}$
\end_inset

 is a well-chosen kernel parameterized by the bandwidth 
\begin_inset Formula $\varepsilon$
\end_inset

.
 He also made the valuable point that noisy ABC simulated exactly from the
 posterior conditioning on observations with errors.
 
\begin_inset CommandInset citation
LatexCommand citet
key "sisson2007sequential"

\end_inset

 combine partial rejection control and ABC to solve the inefficiency when
 prior and posterior are dissimilar.
 In order to analysis hidden Markov models, 
\begin_inset CommandInset citation
LatexCommand citet
key "jasra2012filtering"

\end_inset

 proposed ABC filtering incorporated sequential Monte Carlo (SMC) method.
 This procedure was theoretical justified in parameter estimation by 
\begin_inset CommandInset citation
LatexCommand citet
key "dean2014parameter"

\end_inset

.
 
\begin_inset Note Comment
status open

\begin_layout Plain Layout
detail of justification
\end_layout

\end_inset

Using SMC sampler will result in a bias in approximation to the posterior.
 To overcome this problem, 
\begin_inset CommandInset citation
LatexCommand citet
key "beaumont2009adaptive"

\end_inset

 incorporated population Monte Carlo method into ABC-PRC by modifying the
 importance sampling weights with an component-wise random walk estimator
 of likelihood and using decreasing tolerance levels.
 MCMC-ABC also suffers from poor mixture properties when tolerance level
 is small.
 To answer this weakness, 
\begin_inset CommandInset citation
LatexCommand citet
key "baragatti2013likelihood"

\end_inset

 proposed ABC parallel tempering scheme.
 The basic technique is running several MCMC-ABC chains with different tolerance
 levels and swap some chains under certain conditions.
 They recommended ABC-PT when the posterior was multi-modal.
 
\end_layout

\begin_layout Subsection
Calibration
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "mckinley2009inference"

\end_inset

 performed a simulation comparing ABC-MCMC and ABC-SMC.
 The conclusions are the choice of the distance, the summary statistics
 are paramount to the success of ABC, while the tolerance level does not
 seem to have a strong influence.
\end_layout

\begin_layout Standard
For parameter estimation, the ideal summary statistics would be the sufficient
 statistics.
 However, for most real problems, it is impossible to find them.
 
\begin_inset CommandInset citation
LatexCommand citet
key "joyce2008approximately"

\end_inset

 considered sequential inclusion of summary statistics based on likelihood
 ratios.
 Nevertheless, their method does not address the issue of construction of
 summary statistics and does not take into account the sequential nature
 of likelihood ratios.
 
\begin_inset CommandInset citation
LatexCommand citet
key "aeschbacher2012novel"

\end_inset

 advocated inclusion via boosting.
 
\begin_inset CommandInset citation
LatexCommand citet
key "blum2013comparative"

\end_inset

 summarized several other methods to select summary statistics, such as
 information criterion, partial least square regression, neural network.
 
\begin_inset CommandInset citation
LatexCommand citet
key "fearnhead2012constructing"

\end_inset

 used polynomial regression to estimate the posterior mean and used it as
 a summary statistic.
 As far as the authors know, this is the first method which is able to automatic
 construct summary statistics.
 
\begin_inset CommandInset citation
LatexCommand citet
key "ruli2013approximate"

\end_inset

 suggested using score function of composite likelihood as summary statistics.
 
\begin_inset CommandInset citation
LatexCommand citet
key "barthelme2014expectation"

\end_inset

 applied expectation propagation approximation in ABC, that is approximating
 the posterior by 
\begin_inset Formula 
\[
\pi\left(\theta\right)\prod_{i=1}^{n}\int f\left(Z_{i}\mid Y_{1},\ldots,Y_{i-1},\theta\right)I_{\left[\left|S_{i}\left(Z_{i}\right)-S_{i}\left(Y_{i}\right)\right|\le\varepsilon\right]}\diff Z_{i},
\]

\end_inset

where 
\begin_inset Formula $S_{i}$
\end_inset

 is a local summary statistics for a lower dimensional data 
\begin_inset Formula $Z_{i}$
\end_inset

, typically 
\begin_inset Formula $S_{i}\left(Z_{i}\right)=Z_{i}$
\end_inset

.
 By replacing global summary statistics with local ones, they proved their
 algorithm EP-ABC was faster than usual ABC because the accept rate might
 be higher.
\end_layout

\begin_layout Standard
For model selection, the situation is complex.
 
\begin_inset CommandInset citation
LatexCommand citet
key "grelaud2009abc"

\end_inset

 used sufficient statistics in model selection between Gibbs random fields.
 However, 
\begin_inset CommandInset citation
LatexCommand citet
key "marin2014relevant"

\end_inset

 suggested the statistics auxiliary under all candidate models as the best
 summary statistics in model selection based on Bayesian factor.
\begin_inset Note Comment
status open

\begin_layout Plain Layout
add the paper post the problem of model selection in abc
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Calibration of tolerance levels also attracts many attention.
 
\begin_inset CommandInset citation
LatexCommand citet
key "biau2012new"

\end_inset

 viewed the rejection based on metric as a 
\begin_inset Formula $k$
\end_inset

-nearest neighbor procedure and then calibration on 
\begin_inset Formula $\varepsilon$
\end_inset

 is equivalent to calibration of 
\begin_inset Formula $k$
\end_inset

.
 Their results favor 
\begin_inset Formula $k\approx N^{\left(p+4\right)/\left(p+d+4\right)}$
\end_inset

, where 
\begin_inset Formula $N$
\end_inset

 is the number of simulations from model, 
\begin_inset Formula $p$
\end_inset

 is the dimension of the parameters of interest, and 
\begin_inset Formula $d>4$
\end_inset

 is the dimension of the summary statistics, and under this calibration,
 they derived rate of convergence of mean square error of density estimation.
 
\begin_inset CommandInset citation
LatexCommand citet
key "ratmann2013statistical"

\end_inset

 treated the calibration of 
\begin_inset Formula $\varepsilon$
\end_inset

 as a statistical hypothesis testing problem and obtained 
\begin_inset Formula $\varepsilon$
\end_inset

 as a critique value in hypothesis testing.
 The advantage of their method is the MAP estimate is the same under full
 posterior and ABC posterior and the Kullback--Leibler divergence of the
 two distributions is small.
 
\end_layout

\begin_layout Subsection
Post-Process
\end_layout

\begin_layout Standard
Besides modifying sampling scheme and summary statistics, one could also
 improve the inference by carefully processing the output from ABC.
 Viewing approximation to the posterior as a conditional density estimation
 problem, 
\begin_inset CommandInset citation
LatexCommand citet
key "beaumont2002approximate"

\end_inset

 applied local linear regression by replacing the simulated raw 
\begin_inset Formula $\theta$
\end_inset

 by 
\begin_inset Formula 
\[
\theta^{*}=\theta-\left(S\left(z\right)-S\left(Y\right)\right)^{T}\hat{\beta},
\]

\end_inset

where 
\begin_inset Formula $\hat{\beta}$
\end_inset

 is obtained by a weighted least square regression, using weights of the
 form 
\begin_inset Formula $K_{\varepsilon}\left(\rho\left(S\left(z\right),S\left(Y\right)\right)\right)$
\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand citet
key "blum2010non"

\end_inset

 generalized this idea to nonlinear regression with heteroskedasticity estimated
 by a neural net with one hidden layer.
 
\begin_inset CommandInset citation
LatexCommand citet
key "leuenberger2010bayesian"

\end_inset

 addressed the same issue by inverse regression.
\end_layout

\begin_layout Section
Sufficient Dimension Reduction
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
Sufficient dimension reduction (SDR) is an emerging topic in recent statistical
 area.
 As one of the answers to high dimension problem , SDR usually has solid
 theoretical background to guarantee large sample consistency and valid
 statistical procedures to select the dimension of results, comparing with
 machine learning techniques such as manifold learning.
 The original problem is formulated as following.
 Let 
\begin_inset Formula $X\in\mathbb{R}^{p}$
\end_inset

 and 
\begin_inset Formula $Y\in\mathbb{R}$
\end_inset

, there is a unknown lower dimensional transformation 
\begin_inset Formula $S:\mathbb{R}^{p}\rightarrow\mathbb{R}^{d}$
\end_inset

, where 
\begin_inset Formula $d<p$
\end_inset

, which we need to estimate, such that 
\begin_inset Formula 
\[
P\left(Y\in y\mid X\right)=P\left(Y\in Y\mid S\left(X\right)\right),\forall y\in Y.
\]

\end_inset

Most papers in SDR focus on the case called linear, when 
\begin_inset Formula $S$
\end_inset

 is a linear transformation, that is 
\begin_inset Formula $S\left(X\right)=\beta^{T}X$
\end_inset

, where 
\begin_inset Formula $\beta\in\mathbb{R}^{p\times d}$
\end_inset

.
 Note that even in linear SDR, the inference differs from transitional parameter
 estimation, because for any non-singular matrix 
\begin_inset Formula $T$
\end_inset

, 
\begin_inset Formula $\beta T$
\end_inset

 is still a valid dimension reduction transformation.
 As the result, the exact values of each entries in 
\begin_inset Formula $\beta$
\end_inset

 is not identifiable, but the column space of 
\begin_inset Formula $\beta$
\end_inset

 is unique determined.
 So the parameters in linear SDR should in essential be a space called central
 subspace denoted by 
\begin_inset Formula $S_{Y\mid X}$
\end_inset

 in 
\begin_inset CommandInset citation
LatexCommand citet
key "cook1994interpretation"

\end_inset

 and the optimization problems leading to estimations are in general constrained
 in the set of subspace called Grassmann manifold instead of usual Euclidean
 space.
 
\begin_inset Note Comment
status open

\begin_layout Plain Layout
non-linear dr by cond on sigma algebra
\end_layout

\end_inset

Sometimes, we are only interested in 
\begin_inset Formula $E\left(Y\mid X\right)$
\end_inset

, for example, in linear regression, then a weak assumption can be made
 as 
\begin_inset Formula 
\[
E\left(Y\mid X\right)=E\left(Y\mid\beta^{T}X\right).
\]

\end_inset

The corresponding space of 
\begin_inset Formula $\beta$
\end_inset

 is called central mean subspace 
\begin_inset Formula $S_{E\left(Y\mid X\right)}$
\end_inset

 in 
\begin_inset CommandInset citation
LatexCommand citet
key "cook2002dimension"

\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand citet
key "yin2002dimension"

\end_inset

 generalized the idea into central 
\begin_inset Formula $k$
\end_inset

-th moment subspace 
\begin_inset Formula $S_{Y\mid X}^{\left(k\right)}$
\end_inset

 defined as 
\begin_inset Formula 
\[
E\left(Y^{j}\mid X\right)=E\left(Y^{j}\mid\beta^{T}X\right),\mathrm{\: for\:}j=1,\ldots,k.
\]

\end_inset

 To estimate the conditional variance, 
\begin_inset CommandInset citation
LatexCommand citet
key "zhu2009dimension"

\end_inset

 introduced the notion of central variance subspace 
\begin_inset Formula $S_{\Var\left(Y\mid X\right)}$
\end_inset

, defined as 
\begin_inset Formula 
\[
\Var\left(Y\mid X\right)=E\left(\Var\left(Y\mid X\right)\mid\beta^{T}X\right).
\]

\end_inset


\end_layout

\begin_layout Standard
To estimate the defined space, statisticians innovate several methods, which
 can be roughly classified into three categories: inverse regression methods,
 non-parametric methods, and semi-parametric methods.
 For simplicity, they usually assume 
\begin_inset Formula $X$
\end_inset

 has zero mean and identity variance-covariance matrix.
 
\begin_inset CommandInset citation
LatexCommand citet
key "lee2013general"

\end_inset

 formulated the ideas by general measure theory and central 
\begin_inset Formula $\sigma$
\end_inset

-field 
\begin_inset Formula $\mathscr{G}_{Y\mid X}$
\end_inset

 as 
\begin_inset Formula 
\[
Y\independent X\mid\mathscr{G}_{Y\mid X}.
\]

\end_inset

 For more detail reference, authors suggest 
\begin_inset CommandInset citation
LatexCommand citet
key "ma2013review"

\end_inset

.
\end_layout

\begin_layout Subsection
Inverse Regression 
\end_layout

\begin_layout Standard
The first inverse regression method, sliced inverse regression (SIR) born
 in 
\begin_inset CommandInset citation
LatexCommand citet
key "li1991sliced"

\end_inset

, starts the pedigree of SDR.
 In that paper, he proved 
\begin_inset Formula 
\[
E\left(X\mid Y\right)\in S_{Y\mid X},
\]

\end_inset

and used principal component analysis (PCA) to get the main direction of
 several estimated 
\begin_inset Formula $\hat{E}\left(X\mid Y=y_{1}\right),\ldots,\hat{E}\left(X\mid Y=y_{s}\right)$
\end_inset

 by sliced mean.
 This also build a exemplary approach for other inverse regression based
 methods: first prove some key quantities, mostly conditional moments, belong
 to the corresponding central space, then use PCA to find the main direction
 of these key quantities.
 This paper also invents two standard conditions in linear SDR, named linearity
 condition 
\begin_inset Formula 
\[
E\left(X\mid\beta^{T}X\right)=L\beta^{T}X,
\]

\end_inset

and constant covariance condition 
\begin_inset Formula 
\[
\Var\left(X\mid\beta^{T}X\right)=Q,
\]

\end_inset

where 
\begin_inset Formula $Q$
\end_inset

 is a non-random matrix.
 The two condition restrict the usage of SIR to nearly normal 
\begin_inset Formula $X$
\end_inset

.
 Lately, 
\begin_inset CommandInset citation
LatexCommand citet
key "dong2010dimension"

\end_inset

 relaxed the linearity condition to polynomial condition, that is, 
\begin_inset Formula $E\left(X\mid\beta^{T}X\right)$
\end_inset

 is a polynomial function of 
\begin_inset Formula $\beta^{T}X$
\end_inset

.
 Inspired by SIR, 
\begin_inset CommandInset citation
LatexCommand citet
key "zhu1996asymptotics"

\end_inset

 proposed kernel inverse regression using kernel technique to estimate the
 same key quantities in SIR.
 
\begin_inset CommandInset citation
LatexCommand citet
key "wu2008kernel"

\end_inset

 generalized this to nonlinear SDR problem.
 
\begin_inset CommandInset citation
LatexCommand citet
key "wang2014transformed"

\end_inset

 applied probability integral transformation to SIR in order to solve nonlinear
 SDR.
 
\begin_inset CommandInset citation
LatexCommand citet
key "li2011principal"

\end_inset

 replaced moments key quantities by a more robust quantile-like quantities
 defined by a sliced SVM.
 They proved 
\begin_inset Formula $\psi\left(y\right)\in S_{Y\mid X}$
\end_inset

, if 
\begin_inset Formula $\psi$
\end_inset

 was a solution of a generalized SVM problem, 
\begin_inset Formula 
\[
\left(\psi\left(y\right),t\left(y\right)\right)=\argmin_{\psi,t}\psi^{T}\hat{\Sigma}_{X}\psi+\lambda E_{X,Y}\left(1-\left(I_{\left[Y\le y\right]}-I_{\left[Y>y\right]}\right)\left[\psi^{T}\left(X-\overline{X}\right)-t\right]\right)_{+}.
\]

\end_inset

Their method, called principal support vector machine, can be applied to
 both linear and kernelized nonlinear SDR problem.As pointed in 
\begin_inset CommandInset citation
LatexCommand citet
key "1991"

\end_inset

, SIR fails when there are some symmetric patterns, , so they proposed sliced
 average variance estimation (SAVE) using the second conditional moments,
 
\begin_inset Formula 
\[
\mathrm{span}\left(I_{p}-\Var\left(X\mid Y\right)\right)\subset S_{Y\mid X}.
\]

\end_inset


\begin_inset CommandInset citation
LatexCommand citet
key "zhu2007hybrid"

\end_inset

 suggested a hybrid of SIR and SAVE by a convex combination.
 
\begin_inset CommandInset citation
LatexCommand citet
key "li2007directional"

\end_inset

 proposed direction regression (DR) based on 
\begin_inset Formula 
\[
\mathrm{span}\left(2I_{p}-E\left(\left.\left(X-\tilde{X}\right)\left(X-\tilde{X}\right)^{T}\right|Y,\tilde{Y}\right)\right)\subset S_{Y\mid X}.
\]

\end_inset

To avoid tuning parameters such as the number of slices and bandwidth of
 kernel, 
\begin_inset CommandInset citation
LatexCommand citet
key "zhu2010sufficient"

\end_inset

 proposed discretization-expectation procedure.
 
\begin_inset CommandInset citation
LatexCommand citet
key "zhu2010dimension"

\end_inset

 proposed cumulative slicing estimation and 
\begin_inset CommandInset citation
LatexCommand citet
key "li2005contour"

\end_inset

 proposed contour regression.
 The above methods usually use non-parametric or semi-parametric method
 to estimate the key quantities, to take the advantage of explicit likelihood,
 
\begin_inset CommandInset citation
LatexCommand citet
key "cook2009likelihood"

\end_inset

 introduced likelihood acquired directions as MLE of inverse regression.
 
\end_layout

\begin_layout Standard
For central mean subspace, 
\begin_inset CommandInset citation
LatexCommand citet
key "li1989regression"

\end_inset

 proved the column space of ordinary least squares is a subspace of 
\begin_inset Formula $S_{E\left(Y\mid X\right)}$
\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand citet
key "li1992principal"

\end_inset

 proposed principal Hessian directions and 
\begin_inset CommandInset citation
LatexCommand citet
key "cook2002dimension"

\end_inset

 proposed iterative Hessian transformations to recover 
\begin_inset Formula $S_{E\left(Y\mid X\right)}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Multivariate response settings are also considered.
 
\begin_inset Note Comment
status open

\begin_layout Plain Layout
add envelope and dr ask dr su 
\end_layout

\end_inset

Three main stream methods are developed.
 The first is generalizing sliceing into hypercubes defined by different
 topologies, for instance, 
\begin_inset CommandInset citation
LatexCommand citet
key "aragon1997gauss,hsing1999nearest,setodji2004k"

\end_inset

.The second is recovering the joint central subspace from marginal central
 subspaces, for instance, 
\begin_inset CommandInset citation
LatexCommand citet
key "cook2003model,saracco2005asymptotics,yin2006moment"

\end_inset

.
 The last one is projecting multivariate response onto lower dimensional
 space, for instance, 
\begin_inset CommandInset citation
LatexCommand citet
key "li2008projective,zhu2010dimension"

\end_inset

.
 
\end_layout

\begin_layout Subsection
Non-Parametric Methods
\end_layout

\begin_layout Standard
Non-parametric methods do not require the linearity condition or constant
 covariance condition.
 And thus these methods are more flexible than inverse regression methods.
 However, they still rely on continuous 
\begin_inset Formula $X$
\end_inset

, and hence could not be applied to categorical predictors.
\end_layout

\begin_layout Standard
The first non-parametric method is the minimum average variance estimation
 (MAVE) by 
\begin_inset CommandInset citation
LatexCommand citet
key "xia2002adaptive"

\end_inset

, which estimated the central mean subspace by kernel weighted least square
 subject to Grassmann manifold restriction.
 The advantage of this method is exhaustiveness, meaning that it would recover
 the whole 
\begin_inset Formula $S_{E\left(Y\mid X\right)}$
\end_inset

 if 
\begin_inset Formula $d$
\end_inset

 is correctly specified.
 
\begin_inset Note Comment
status open

\begin_layout Plain Layout
add bayesian method khare
\end_layout

\end_inset

This method settles the characters of almost all non-parametric methods,
 that is, smoothing approach to unknown link function 
\begin_inset Formula $m$
\end_inset

 defined as 
\begin_inset Formula 
\[
Y=m\left(\beta^{T}X\right)+\varepsilon.
\]

\end_inset

Lately, 
\begin_inset CommandInset citation
LatexCommand citet
key "xia2007constructive"

\end_inset

 proposed density based MAVE to estimate central subspace, basically replacing
 response 
\begin_inset Formula $Y$
\end_inset

 by kernel smoothing 
\begin_inset Formula $K_{b}\left(Y-y\right)$
\end_inset

, and 
\begin_inset CommandInset citation
LatexCommand citet
key "wang2008sliced"

\end_inset

 proposed sliced regression.
 
\begin_inset CommandInset citation
LatexCommand citet
key "hernandez2005dimension,yin2005direction,yin2008successive"

\end_inset

 replaced the weighted least square by other loss functions.
\end_layout

\begin_layout Subsection
Semi-Parametric Methods
\end_layout

\begin_layout Standard
As far as the authors know, there is only one semi-parametric method by
 
\begin_inset CommandInset citation
LatexCommand citet
key "ma2012semiparametric"

\end_inset

 now.
 In the same way as in sufficient statistics , they decomposed the likelihood
 functions into two parts, one of which contained only predictors, and the
 other of which contained response, and the only interest in dimension reduction
 community was the latter part.
 Based on this observation, they formulated consistency estimating equations
 by influence function class defined as 
\begin_inset Formula 
\[
\left\{ f\left(Y,X\right)-E\left(f\left(Y,X\right)\mid\beta^{T}X,Y\right):E\left(f\left(Y,X\right)\mid X\right)=E\left(f\left(Y,X\right)\mid\beta^{T}X\right),\forall f\right\} ,
\]

\end_inset

particularly, the following forms were used 
\begin_inset Formula 
\[
f\left(Y,X\right)=\left(g\left(Y,\beta^{T}X\right)-E\left(g\left(Y,\beta^{T}\right)\mid\beta^{T}X\right)\right)\left(\alpha\left(X\right)-E\left(\alpha\left(X\right)\mid\beta^{T}X\right)\right),
\]

\end_inset

for any 
\begin_inset Formula $g$
\end_inset

 and 
\begin_inset Formula $\alpha$
\end_inset

.
 This approach does not require moment conditions like linearity condition
 or constant covariance condition, or continuous condition of predictors.
 The authors also shew in their paper several inverse regression methods
 could be derived by different settings of 
\begin_inset Formula $g$
\end_inset

 and 
\begin_inset Formula $\alpha,$
\end_inset

 and the statistical intuitive of linearity condition and constant covariance
 condition could also be explained under this framework.
\end_layout

\begin_layout Subsection
Inference about 
\begin_inset Formula $d$
\end_inset


\end_layout

\begin_layout Standard
Statistical inference about reduction dimension is a characteristic of sufficien
t dimension reduction.
 There are sequential test methods by 
\begin_inset CommandInset citation
LatexCommand citet
key "schott1994determining,velilla1998assessing,bura2001extending,cook2001special,cook2004determining,cook2005sufficient"

\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand citet
key "ye2003using"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand citet
key "zhu2006fourier"

\end_inset

 proposed bootstrap method to select 
\begin_inset Formula $d$
\end_inset

.
 BIC-type methods are considered in 
\begin_inset CommandInset citation
LatexCommand citet
key "zhu2006sliced,zhu2007kernel,luo2009contour"

\end_inset

.
 A more fashion method is proposed by 
\begin_inset CommandInset citation
LatexCommand citet
key "zhu2010sparse"

\end_inset

 as sparse eigen-decomposition strategy.
 They imposed adaptive LASSO penalty to spectral decomposition problem in
 inverse regression, and the minimization could be solved very effectively
 by LARS algorithm in 
\begin_inset CommandInset citation
LatexCommand citet
key "efron2004least"

\end_inset

.
 
\end_layout

\begin_layout Subsection
High Dimension
\end_layout

\begin_layout Standard
The laurel of modern statistics should be crowned to inference under increasing
 number of parameters.
 To avoid singularity of marginal covariance matrix of 
\begin_inset Formula $X$
\end_inset

 when 
\begin_inset Formula $p>n$
\end_inset

, some authors incorporate partial least squares into inverse regression,
 such as in 
\begin_inset CommandInset citation
LatexCommand citet
key "li2007partial,cook2007dimension,zhu2009distribution,zhu2010dimension"

\end_inset

.
 Another strategy is to utilize the sparsity principle, such as LASSO, SCAD
 and Dantzig selector.
 This leads to sure independence ranking and screening procedure in 
\begin_inset CommandInset citation
LatexCommand citet
key "zhu2011model"

\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand citet
key "wu2008consistency"

\end_inset

 applied Tikhonov penalties to kernel sliced inverse regression to solve
 nonlinear SDR.
 
\begin_inset Note Comment
status open

\begin_layout Plain Layout
add wuchang bao setting consistency and normalty proof
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Higher-Order Properties of Bayesian Empirical Likelihood: Univariate Case
\end_layout

\begin_layout Chapter
Higher-Order Properties of Bayesian Empirical Likelihood: Multivariate Case
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Empirical likelihood, over the years, has become a very popular topic of
 statistical research.
 The name was coined by Owen in his classic 1986 paper, although similar
 ideas are found even earlier in the works of 
\begin_inset CommandInset citation
LatexCommand cite
key "hartley1968new"

\end_inset

, 
\begin_inset CommandInset citation
LatexCommand citet
key "thomas1975confidence"

\end_inset

, 
\begin_inset CommandInset citation
LatexCommand cite
key "rubin1981bayesian"

\end_inset

 and others.
 The main advantage of empirical likelihood is that is involves fewer assumption
s that a regular likelihood, and yet shares the same asymptotic properties
 of the latter.
 
\end_layout

\begin_layout Standard
Research in this area has primarily been frequentist with a long list of
 important theoretical developments accompanied by a large number of application
s.
 To our knowledge, the first Bayesian work in this general area appeared
 in the article of 
\begin_inset CommandInset citation
LatexCommand cite
key "lazar2003bayesian"

\end_inset

 followed by some related work in 
\begin_inset CommandInset citation
LatexCommand cite
key "schennach2005bayesian,schennach2007point"

\end_inset

 , the latter introducing the concept of 
\begin_inset Quotes eld
\end_inset

exponentially tilted empirical likelihood
\begin_inset Quotes erd
\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand cite
key "lazar2003bayesian"

\end_inset

 suggested using empirical likelihood as a substitute for the usual likelihood
 and carry out Bayesian analysis in the usual way.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "baggerly1998empirical"

\end_inset

 viewed empirical likelihood as a method of assigning probabilities to a
 
\begin_inset Formula $n$
\end_inset

-cell contingency table in order to minimize a goodness-of-fit criterion.
 He selected Cressie-Read power divergence statistics as one such criterion
 for construction of confidence regions in a number of situations and pointed
 out also how the usual empirical likelihood, exponentially tilted empirical
 likelihood and others can be viewed as special cases of the Cressie-Read
 criterion by appropriate choice of the the power parameter.
 This was also discussed in 
\begin_inset CommandInset citation
LatexCommand cite
key "owen2010empirical"

\end_inset

 who pointed our that all members of the Cressie-Read family lead to 
\begin_inset Quotes eld
\end_inset

empirical divergence analogues of the empirical likelihood in which asymptotic
 
\begin_inset Formula $\chi^{2}$
\end_inset

 calibration holds for the mean
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
The objective of this article is to provide an asymptotic expansion of the
 posterior distribution based on empirical likelihood and its variations
 under certain regularity conditions and a mean constraint.
 The work is inspired by the work of 
\begin_inset CommandInset citation
LatexCommand cite
key "fang2006empirical"

\end_inset

 who provided a somewhat different expansion subject to a mean constraint.
 Unlike 
\begin_inset CommandInset citation
LatexCommand cite
key "fang2005expected,fang2006empirical"

\end_inset

, our result is based on the derivatives of the pseudo likelihood with respect
 to the parameters of interest evaluated at the maximum EL estimator, and
 a rigorous expansion is provided with particular attention in handling
 the remainder terms.
 Moreover, we consider a general estimating equation which includes the
 mean example of 
\begin_inset CommandInset citation
LatexCommand citet
key "fang2006empirical"

\end_inset

 as a special case.
 The need for different pseudo-likelihoods for statistical inference is
 felt all the more in these days, especially for the analysis of high-dimensiona
l data, where the usual likelihood based analysis is hard to perform, These
 alternative likelihoods are equally valuable for approximate Bayesian computati
ons (ABC), a topic which has only recently surfaced in the statistics literature
 (see e.g.
 Comuet et al.
 , 2008)
\begin_inset Note Comment
status open

\begin_layout Plain Layout
paper not found or cornuet 2008
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
Asymptotic expansion of the posterior based on a regular likelihood was
 given earlier in 
\begin_inset CommandInset citation
LatexCommand cite
key "johnson1970asymptotic"

\end_inset

, and later in 
\begin_inset CommandInset citation
LatexCommand cite
key "ghosh1982expansions"

\end_inset

.
 We follow their approach with many necessary modifications in view of the
 fact that any meaningful prior needs to have support in the nondecreasing
 compact set 
\begin_inset Formula $H$
\end_inset

.
 As a special case of our result, we get the celebrated Bernstein von-Mises
 Theorem.
 The latter was mentioned in 
\begin_inset CommandInset citation
LatexCommand cite
key "lazar2003bayesian"

\end_inset

 for the special case of empirical likelihood, but here we provide a rigorous
  derivation with the needed regularity conditions.
 
\end_layout

\begin_layout Standard
Unlike in univariate case, where the posterior is centered at M-estimator,
 which guarantees the consistency under mild regular conditions, in multivariate
 case, the posterior is centralized at maximum generalized empirical likelihood
 estimator.
 This concept is generalized from empirical likelihood case which is fully
 explored in 
\begin_inset CommandInset citation
LatexCommand cite
key "qin1994empirical"

\end_inset

 to exponentially tilted empirical likelihood and even Cressie-Read case.
 A similar concept has been developed in 
\begin_inset CommandInset citation
LatexCommand cite
key "newey2004higher"

\end_inset

 under the similar name.
 There is slight different between their definition and ours in both intuition
 and mathematical definition, however, numerical study shows the performance
 of these two are quite similar.
 Furthermore, in order for the empirical likelihood sample moments to be
 finite, we need more restrictive conditions to guarantee not only the consisten
cy of generalized empirical likelihood estimator, but also its law of iterative
 logarithm.
 
\end_layout

\begin_layout Standard
The organization of the remaining sections of this paper is as follows.
 In Section 2 of this paper, we consider the basic settings of the empirical
 likelihood, exponentially tilted empirical likelihood, and finally the
 more general Cressie-Read divergence criterion.
 Section 3 contains some basic lemmas pertaining to these three formulations.
 While both empirical and exponentially tilted empirical likelihood are
 indeed limiting cases of the general Cressie-Read formulation, for technical
 reasons as as well as for the sake of transparency, we have presented some
 of these lemmas separately for these three cases.
 Section 4 contains the main result, namely the asymptotic expansion of
 the posterior and presents a unified derivation.
 Some simulation results are presented in Section 5.
 Section 6 contains some concluding remarks.
\end_layout

\begin_layout Section
Basic Settings
\end_layout

\begin_layout Standard
Suppose 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 are independent and identically distributed random vectors satisfying 
\begin_inset Formula $Eg\left(X_{1},\theta\right)=0$
\end_inset

, where 
\begin_inset Formula $\theta\in\mathbb{R}^{p}$
\end_inset

 and 
\begin_inset Formula $g\left(x,\theta\right)=\left(g_{1}\left(x,\theta\right),g_{2}\left(x,\theta\right),\ldots,g_{r}\left(x,\theta\right)\right)\in\mathbb{R}^{r}$
\end_inset

.
 Like in 
\begin_inset CommandInset citation
LatexCommand cite
key "qin1994empirical"

\end_inset

, we focus on the situation 
\begin_inset Formula $r>p$
\end_inset

.
 When 
\begin_inset Formula $r\le p$
\end_inset

, the posterior will still asymptotically center around M-estimator, and
 all the arguments are the same as univariate case.
 However, when 
\begin_inset Formula $r>p$
\end_inset

 , M-estimator may not even exist, we need further generalization.
 In this context, 
\begin_inset CommandInset citation
LatexCommand citet
key "owen1988empirical"

\end_inset

, formulated empirical likelihood as a nonparametric likelihood of the form
 
\begin_inset Formula $\prod_{i=1}^{n}w_{i}\left(\theta\right)$
\end_inset

, where 
\begin_inset Formula $w_{i}$
\end_inset

 is the probability mass assigned to 
\begin_inset Formula $X_{i}\:\left(i=1,\ldots,n\right)$
\end_inset

 satisfying the constraints 
\begin_inset Formula 
\begin{equation}
\begin{cases}
w_{i}>0,\:\mathrm{for\: all\: i;}\\
\sum_{i=1}^{n}w_{i}=1;\\
\sum_{i=1}^{n}w_{i}g\left(X_{i},\theta\right)=0.
\end{cases}\label{eq:constraint-el}
\end{equation}

\end_inset

The target is to maximize 
\begin_inset Formula $\prod_{i=1}^{n}w_{i}$
\end_inset

 or equivalently 
\begin_inset Formula $\sum_{i=1}^{n}\log w_{i}$
\end_inset

 with respect to 
\begin_inset Formula $w_{1},\ldots,w_{n}$
\end_inset

 subject to the constraints given in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:constraint-el"

\end_inset

.
 Applying the Lagrange multiplier method, the solution turns out to be 
\begin_inset Formula 
\begin{equation}
\hat{w}_{i}^{\mathrm{EL}}\left(\theta\right)=\frac{1}{n\left[1+\nu^{T}g\left(X_{i},\theta\right)\right]},\; i=1,2,\ldots,n,\label{eq:sol-emp-lik}
\end{equation}

\end_inset

where 
\begin_inset Formula $\nu\in\mathbb{R}^{r}$
\end_inset

, the Lagrange multiplier satisfies 
\begin_inset Formula 
\begin{equation}
\sum_{i=1}^{n}\frac{g\left(X_{i},\theta\right)}{1+\nu^{T}g\left(X_{i},\theta\right)}=0.\label{eq:lambda-eq}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
It may be noted that 
\begin_inset CommandInset citation
LatexCommand citet
key "fang2005expected,fang2006empirical"

\end_inset

 
\begin_inset Formula $g\left(X_{i},\theta\right)=X_{i}-\theta$
\end_inset

.
 
\end_layout

\begin_layout Standard
Closely related to the empirical likelihood is the exponentially tilted
 empirical likelihood where the objective is to maximize the Shannon entropy
 
\begin_inset Formula $-\sum_{i=1}^{n}w_{i}\log w_{i}$
\end_inset

 still subject to the constraints in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:constraint-el"

\end_inset

.
 The resulting solution is given by 
\begin_inset Formula 
\begin{equation}
\hat{w}_{i}^{\mathrm{ET}}\left(\theta\right)=\frac{\exp\left(-\nu^{T}g\left(X_{i},\theta\right)\right)}{\sum_{j=1}^{n}\exp\left(-\nu^{T}g\left(X_{i},\theta\right)\right)},\label{eq:sol-weight-etel}
\end{equation}

\end_inset

where 
\begin_inset Formula $\nu$
\end_inset

, the Lagrange multiplier, satisfies 
\begin_inset Formula 
\begin{equation}
\sum_{i=1}^{n}\exp\left(-\nu^{T}g\left(X_{i},\theta\right)\right)g\left(X_{i},\theta\right)=0.\label{eq:lag-mul-exp-tilt-el}
\end{equation}

\end_inset

The exponentially tilted empirical likelihood is related to Kullback-Leibler
 divergence between two empirical distributions, one with weights 
\begin_inset Formula $w_{i}$
\end_inset

 assigned to the 
\begin_inset Formula $n$
\end_inset

 sample points, and the other with uniform weights 
\begin_inset Formula $1/n$
\end_inset

 assigned to the sample points.
 
\end_layout

\begin_layout Standard
The general Cressie-Read divergence criterion given by 
\begin_inset Formula 
\[
\mathrm{CR}\left(\lambda\right)=\frac{2}{\lambda\left(\lambda+1\right)}\sum_{i=1}^{n}\left[\left(nw_{i}\right)^{-\lambda}-1\right].
\]

\end_inset

We focus on the cases 
\begin_inset Formula $\lambda\ge0$
\end_inset

 and 
\begin_inset Formula $\lambda\le-1$
\end_inset

, because in these cases, 
\begin_inset Formula $\mathrm{CR}\left(\lambda\right)$
\end_inset

 is a convex function of the 
\begin_inset Formula $w_{i}\:\left(i=1,\ldots,n\right)$
\end_inset

,hence the minimization problem will produce a unique solution.
 The following lemma also shows within this range, the resulting empirical
 weights behaviour more like a likelihood.
 The limiting cases 
\begin_inset Formula $\lambda\rightarrow0$
\end_inset

 and 
\begin_inset Formula $\lambda\rightarrow-1$
\end_inset

 correspond to the usual empirical likelihood and the exponentially tilted
 empirical likelihood as defined earlier.
 
\end_layout

\begin_layout Standard
For convex 
\begin_inset Formula $\mathrm{CR}\left(\lambda\right)$
\end_inset

, its minimum will be attained in the compact set 
\begin_inset Formula $H_{n}$
\end_inset

 determined by data.
 The Lagrange multiplication method now gives the weights 
\begin_inset Formula 
\begin{equation}
\hat{w}_{i}^{\mathrm{CR}}\left(\theta\right)=\frac{1}{n}\left(\mu+\nu^{T}g\left(X_{i},\theta\right)\right)^{-\frac{1}{\lambda+1}},\, i=1,2,\ldots,n,\label{eq:weight-cr-el}
\end{equation}

\end_inset

where 
\begin_inset Formula $\mu\equiv\mu\left(\theta\right)$
\end_inset

 and 
\begin_inset Formula $\nu\equiv\nu\left(\theta\right)$
\end_inset

 satisfy 
\begin_inset Formula 
\begin{equation}
\begin{cases}
\sum_{i=1}^{n}\left(\mu+\nu^{T}Tg\left(X_{i},\theta\right)\right)^{-\frac{1}{\lambda+1}}=n,\\
\sum_{i=1}^{n}\left(\mu+\nu^{T}g\left(X_{i},\theta\right)\right)^{-\frac{1}{\lambda+1}}X_{i}=0.
\end{cases}\label{eq:lag-mul-cr-el}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
We now introduce the posterior based on an empirical likelihood.
 The basic ideal was first introduced by 
\begin_inset CommandInset citation
LatexCommand cite
key "lazar2003bayesian"

\end_inset

 with bunch of numerical examples.
 The intuition relies on close relationship between empirical likelihood
 and empirical distribution.
 
\begin_inset CommandInset citation
LatexCommand cite
key "owen2010empirical"

\end_inset

 formulated 105 4 the two concepts under the same optimization framework,
 that is, they shared the same objective function, but the former one was
 solved under parametric constraints, while the latter was not.
 Considering this similarity, we can use the empirical likelihood as a valid
 distribution parametrized by inferential target.
 To concrete this intuition in Bayesian philosophy, writing 
\begin_inset Formula $\hat{w}_{i}\left(\theta\right)$
\end_inset

 as generic notation for either 
\begin_inset Formula $\hat{w}_{i}^{\mathrm{EL}}$
\end_inset

, 
\begin_inset Formula $\hat{w}_{i}^{\mathrm{ET}}$
\end_inset

 or 
\begin_inset Formula $\hat{w}_{i}^{\mathrm{CR}}$
\end_inset

, 
\begin_inset Formula $\pi$
\end_inset

 with a prior probability density function 
\begin_inset Formula $\rho\left(\theta\right)$
\end_inset

, with support in 
\begin_inset Formula $H_{n}$
\end_inset

, the profile (pseudo) posterior is given by 
\begin_inset Formula 
\begin{equation}
\pi\left(\theta\mid X_{1},X_{2},\ldots,X_{n}\right)=\frac{\prod_{i=1}^{n}\hat{w}_{i}\left(\theta\right)\rho\left(\theta\right)}{\int_{H_{n}}\prod_{i=1}^{n}\hat{w}_{i}\left(\theta\right)\rho\left(\theta\right)\diff\theta}.\label{eq:poster-el-expression}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The main objective of this paper is to provide an asymptotic expansion of
 
\begin_inset Formula $\pi\left(\theta\mid X_{1},X_{2},\ldots,X_{n}\right)$
\end_inset

.
 This will include in particular the Bernstein-von Mises theorem.
 Towards this end, we develop a few necessary lemmas in the next section.
\end_layout

\begin_layout Section
Lemmas
\end_layout

\begin_layout Standard
We first give an explanation of natural domain of 
\begin_inset Formula $\theta$
\end_inset

, under empirical likelihood settings.
 In practice, some values of 
\begin_inset Formula $\theta$
\end_inset

 will result in an empty feasible set in constraints 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:constraint-el"

\end_inset

.
 The which guarantees an non-empty feasible set, and thus a solution of
 the optimization problem constitutes 120 natural domain of empirical likelihood.
 One may questions whether the size of the natural domain is large enough
 to contain the true value.
 The following lemma alleviates this worry.
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:nondecreasing-compact-natural-domain"

\end_inset

Assume 
\begin_inset Formula $g\left(\cdot,\cdot\right)$
\end_inset

 is a continuous vector value function, then the natural domain defined
 by the constraints 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:constraint-el"

\end_inset

 is a compact set and nondecreasing with respect to sample size 
\begin_inset Formula $n$
\end_inset

.
\end_layout

\begin_layout Proof
By the third constraint of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:constraint-el"

\end_inset

, 
\begin_inset Formula $\theta$
\end_inset

 is a continuous vector value function of 
\begin_inset Formula $w_{1},w_{2},\ldots,w_{n}$
\end_inset

, but 
\begin_inset Formula $w_{i}$
\end_inset

 are defined on a simplex which is a compact set through the first constraint
 of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:constraint-el"

\end_inset

.
 We may recall that continuous function maps compact sets to compact sets.
 Hence, 
\begin_inset Formula $\theta$
\end_inset

 is naturally defined on a compact set denoted by 
\begin_inset Formula $H$
\end_inset

.
\end_layout

\begin_layout Proof
If for any 
\begin_inset Formula $j=1,2,\ldots,r$
\end_inset

, 
\begin_inset Formula $g_{j}\left(X_{i},\theta\right)$
\end_inset

, 
\begin_inset Formula $i=1,2,\ldots,n$
\end_inset

, are all non-positive or all non-negative, then the constraints 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:constraint-el"

\end_inset

 are violated and 
\begin_inset Formula $H=\emptyset$
\end_inset

.
 Hence ,
\begin_inset Formula 
\begin{eqnarray*}
H & = & \left\{ \bigcup_{j=1}^{r}\left\{ \left[\bigcap_{i=1}^{n}\left(g_{j}\left(X_{i},\theta\right)\ge0\right)\right]\bigcup\left[\bigcap_{i=1}^{n}\left(g_{j}\left(X_{i},\theta\right)\le0\right)\right]\right\} \right\} ^{c}\\
 & = & \bigcap_{j=1}^{r}\left\{ \left[\bigcup_{i=1}^{n}\left(g_{j}\left(X_{i},\theta\right)\ge0\right)^{c}\right]\bigcap\left[\bigcup_{i=1}^{n}\left(g_{j}\left(X_{i},\theta\right)\le0\right)^{c}\right]\right\} .
\end{eqnarray*}

\end_inset

With 
\begin_inset Formula $n$
\end_inset

 increases, both 
\begin_inset Formula $\bigcup_{i=1}^{n}\left(g_{j}\left(X_{i},\theta\right)\ge0\right)^{c}$
\end_inset

 and 
\begin_inset Formula $\bigcup_{i=1}^{n}\left(g_{j}\left(X_{i},\theta\right)\le0\right)^{c}$
\end_inset

 will increase, so does their intersection 
\begin_inset Formula $H$
\end_inset

.
 
\end_layout

\begin_layout Standard
Although, intuitively we expect the empirical likelihood to behave as the
 true likelihood, we need some theoretical support to show that the former
 enjoys some of the basic properties of the latter.
 In particular, we need to verify that 
\begin_inset Formula $\nu$
\end_inset

 and 
\begin_inset Formula $\mu$
\end_inset

 are smooth functions of 
\begin_inset Formula $\theta$
\end_inset

 and the (pseudo) Fisher Information based on the empirical likelihood is
 positive.
 
\end_layout

\begin_layout Standard
We first establish the positiveness of the Fisher information .
 We consider the three cases separately to introduce more transparency and
 continuity in our approach.
 
\end_layout

\begin_layout Standard
Our first lemma shows that the Lagrange multipliers 
\begin_inset Formula $\nu\left(\theta\right)$
\end_inset

 and 
\begin_inset Formula $\mu\left(\theta\right)$
\end_inset

 are all smooth functions of 
\begin_inset Formula $\theta$
\end_inset

.
 
\begin_inset Note Comment
status open

\begin_layout Plain Layout
add condtions
\end_layout

\end_inset


\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:first-order-smooth-lagmul"

\end_inset

For empirical likelihood, exponentially tilted empirical likelihood and
 Cressie-Read with parameter
\begin_inset Formula $\left(\lambda\right)$
\end_inset

, the Lagrange multipliers 
\begin_inset Formula $\nu\left(\theta\right)$
\end_inset

 and 
\begin_inset Formula $\mu\left(\theta\right)$
\end_inset

 are smooth functions of 
\begin_inset Formula $\theta.$
\end_inset


\end_layout

\begin_layout Proof
We first consider empirical likelihood and observe that, 
\begin_inset Formula $\nu\left(\theta\right)$
\end_inset

 is a implicit function of 
\begin_inset Formula $\theta$
\end_inset

 in view of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:lambda-eq"

\end_inset

 .
 Further 
\begin_inset Formula 
\[
\frac{\partial}{\partial\nu}\sum_{i=1}^{n}\frac{g\left(X_{i},\theta\right)}{1+\nu^{T}g\left(X_{i},\theta\right)}=-\sum_{i=1}^{n}\frac{g\left(X_{i},\theta\right)g^{T}\left(X_{i},\theta\right)}{\left(1+\nu^{T}g\left(X_{i},\theta\right)\right)^{2}},
\]

\end_inset

is negative definite, so that by the implicit function theorem, 
\begin_inset Formula $\nu$
\end_inset

 is differentiable in 
\begin_inset Formula $\theta$
\end_inset

.
 Moreover, differentiating both sides of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:lambda-eq"

\end_inset

 with respect to 
\begin_inset Formula $\nu$
\end_inset

, one gets
\begin_inset Formula 
\begin{eqnarray*}
0 & = & \sum_{i=1}^{n}\frac{1}{1+\nu^{T}g\left(X_{i},\theta\right)}\frac{\partial g\left(X_{i},\theta\right)}{\partial\theta^{T}}\frac{\partial\theta}{\partial\nu}-\sum_{i=1}^{n}\frac{\nu^{T}g\left(X_{i},\theta\right)}{\left(1+\nu^{T}g\left(X_{i},\theta\right)\right)^{2}}\frac{\partial g\left(X_{i},\theta\right)}{\partial\theta^{T}}\frac{\partial\theta}{\partial\nu}\\
 &  & -\sum_{i=1}^{n}\frac{g\left(X_{i},\theta\right)g^{T}\left(X_{i},\theta\right)}{\left(1+\nu^{T}g\left(X_{i},\theta\right)\right)^{2}},
\end{eqnarray*}

\end_inset

which on simplification leads to 
\begin_inset Formula 
\begin{equation}
\frac{\partial\theta}{\partial\nu}=-\left[\sum_{i=1}^{n}\frac{1}{\left(1+\nu^{T}g\left(X_{i},\theta\right)\right)^{2}}\frac{\partial g\left(X_{i},\theta\right)}{\partial\theta}\right]^{-1}\sum_{i=1}^{n}\frac{g\left(X_{i},\theta\right)g^{T}\left(X_{i},\theta\right)}{\left(1+\nu^{T}g\left(X_{i},\theta\right)\right)^{2}},\label{eq:decreasing-theta-to-nu}
\end{equation}

\end_inset


\end_layout

\begin_layout Proof
Next, for exponentially tilted empirical likelihood, in view of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:lag-mul-exp-tilt-el"

\end_inset

 and the relation 
\begin_inset Formula 
\begin{eqnarray*}
 &  & \frac{\partial}{\partial\nu}\left(\sum_{i=1}^{n}\exp\left(-\nu^{T}g\left(X_{i},\theta\right)\right)g\left(X_{i},\theta\right)\right)\\
 & = & -\sum_{i=1}^{n}\exp\left(-\nu^{T}g\left(X_{i},\theta\right)\right)g\left(X_{i},\theta\right)g^{T}\left(X_{i},\theta\right).
\end{eqnarray*}

\end_inset

Note that this matrix is negative definite.
 Once again, the implicit function theorem guarantees the differentiability
 of 
\begin_inset Formula $\nu$
\end_inset

 in 
\begin_inset Formula $\theta$
\end_inset

.
 Further, differentiating both sides of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:lag-mul-exp-tilt-el"

\end_inset

 with respect to 
\begin_inset Formula $\theta$
\end_inset

, one gets 
\begin_inset Formula 
\begin{equation}
\frac{\partial\nu}{\partial\theta}=\left(\sum_{i=1}^{n}\frac{g\left(X_{i},\theta\right)g^{T}\left(X_{i},\theta\right)}{\exp\left(-\nu^{T}g\left(X_{i},\theta\right)\right)}\right)^{-1}\left(\sum_{i=1}^{n}\frac{1-\nu^{T}g\left(X_{i},\theta\right)}{\exp\left(-\nu^{T}g\left(X_{i},\theta\right)\right)}\frac{\partial g\left(X_{i},\theta\right)}{\partial\theta}\right).\label{eq:first-deri-lag-mult-exp-tilted-el}
\end{equation}

\end_inset


\end_layout

\begin_layout Proof
A similar conclusion is achieved for 
\begin_inset Formula $\nu\left(\theta\right)$
\end_inset

 and 
\begin_inset Formula $\mu\left(\theta\right)$
\end_inset

 defined in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:lag-mul-cr-el"

\end_inset

 in connection with CR
\begin_inset Formula $\left(\lambda\right)$
\end_inset

.
 Specifically, defining 
\begin_inset Formula 
\[
\begin{cases}
F_{1}=\sum_{i=1}^{n}\left(\mu+\nu^{T}g\left(X_{i},\theta\right)\right)^{-\frac{1}{\lambda+1}}-n,\\
F_{2}=\sum_{i=1}^{n}\left(\mu+\nu^{T}g\left(X_{i},\theta\right)\right)^{-\frac{1}{\lambda+1}}g\left(X_{i},\theta\right),
\end{cases}
\]

\end_inset

it follows that,
\begin_inset Formula 
\[
\frac{\partial\left(F_{1},F_{2}\right)}{\partial\left(\mu,\nu\right)}=-\frac{1}{\lambda+1}\left(\begin{array}{cc}
\sum_{i=1}^{n}q_{i} & \sum_{i=1}^{n}q_{i}g\left(X_{i},\theta\right)\\
\sum_{i=1}^{n}q_{i}g^{T}\left(X_{i},\theta\right) & \sum_{i=1}^{n}q_{i}g\left(X_{i},\theta\right)g^{T}\left(X_{i,},\theta\right)
\end{array}\right),
\]

\end_inset

 where 
\begin_inset Formula $q_{i}=\left(\mu+\nu^{T}g\left(X_{i},\theta\right)\right)^{-\frac{1}{\lambda+1}-1}$
\end_inset

 .
 Then the determinant of Jacobian is 
\begin_inset Formula 
\begin{eqnarray*}
\det\frac{\partial\left(F_{1},F_{2}\right)}{\partial\left(\mu,\nu\right)} & = & \left(\frac{1}{\lambda+1}\right)^{2}\left(\sum_{i=1}^{n}q_{i}\sum_{i=1}^{n}q_{i}g\left(X_{i},\theta\right)g^{T}\left(X_{i},\theta\right)-\sum_{i=1}^{n}q_{i}g\left(X_{i},\theta\right)\sum_{i=1}^{n}q_{i}g^{T}\left(X_{i},\theta\right)\right)\\
 & = & \left(\frac{1}{\lambda+1}\right)^{2}\left(\sum_{i=1}^{n}q_{i}\right)^{2}\sum_{i=1}^{n}\frac{q_{i}}{\sum_{j=1}^{n}q_{j}}\left[g\left(X_{i},\theta\right)-\left(\sum_{i=1}^{n}\frac{q_{i}}{\sum_{j=1}^{n}q_{j}}g\left(X_{i},\theta\right)\right)\right]\\
 &  & \times\left[g\left(X_{i},\theta\right)-\left(\sum_{i=1}^{n}\frac{q_{i}}{\sum_{j=1}^{n}q_{j}}g\left(X_{i},\theta\right)\right)\right]^{T},
\end{eqnarray*}

\end_inset

which is positive definite.
 Again, by implicit function theorem, one gets differentiability of 
\begin_inset Formula $\mu\left(\theta\right)$
\end_inset

 and 
\begin_inset Formula $\nu\left(\theta\right)$
\end_inset

with respect to 
\begin_inset Formula $\theta$
\end_inset

, and 
\begin_inset Formula 
\begin{eqnarray}
 &  & \left(\begin{array}{c}
\partial\mu/\partial\theta\\
\partial\nu/\partial\theta
\end{array}\right)\nonumber \\
 & = & \left(\frac{\partial\left(F_{1},F_{2}\right)}{\partial\left(\mu,\nu\right)}\right)^{-1}\left(\begin{array}{c}
\partial F_{1}/\partial\theta\\
\partial F_{2}/\partial\theta
\end{array}\right)\\
 & = & \left(-\frac{1}{\lambda+1}\right)\left(\lambda+1\right)^{2}\left[\sum_{i=1}^{n}q_{i}\sum_{i=1}^{n}q_{i}g\left(X_{i},\theta\right)g^{T}\left(X_{i},\theta\right)-\sum_{i=1}^{n}q_{i}g\left(X_{i},\theta\right)\sum_{i=1}^{n}q_{i}g^{T}\left(X_{i},\theta\right)\right]^{-1}\nonumber \\
 &  & \times\left(\begin{array}{cc}
\sum_{i=1}^{n}q_{i}g\left(X_{i},\theta\right)g^{T}\left(X_{i},\theta\right) & -\sum_{i=1}^{n}q_{i}g\left(X_{i},\theta\right)\\
-\sum_{i=1}^{n}q_{i}g^{T}\left(X_{i},\theta\right) & \sum_{i=1}^{n}q_{i}
\end{array}\right)\nonumber \\
 &  & \times\left(\begin{array}{c}
-\left(\lambda+1\right)^{-1}\sum_{i=1}^{n}q_{i}\nu^{T}\partial g\left(X_{i},\theta\right)/\partial\theta\\
-\left(\lambda+1\right)\sum_{i=1}^{n}q_{i}\nu^{T}g\left(X_{i},\theta\right)\partial g\left(X_{i},\theta\right)/\partial\theta+\sum_{i=1}^{n}\left(\mu+\nu^{T}g\left(X_{i},\theta\right)\right)^{\frac{1}{\lambda+1}}\partial g\left(X_{i},\theta\right)/\partial\theta
\end{array}\right)\label{eq:first-der-lag-mul-crel}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
The next result shows that all the derivatives of the Lagrange multipliers
 
\begin_inset Formula $\nu\left(\theta\right)$
\end_inset

 and 
\begin_inset Formula $\mu\left(\theta\right)$
\end_inset

 are smooth functions of 
\begin_inset Formula $\theta\in H$
\end_inset

.
 We provide a unified proof for all three cases where we utilize the previous
 lemma.
 
\end_layout

\begin_layout Assumption
\begin_inset CommandInset label
LatexCommand label
name "assu:high-derive-cond"

\end_inset

Assume for any 
\begin_inset Formula $\left(k_{1},k_{2},\ldots,k_{p}\right)\in\mathbb{N}^{p}$
\end_inset

, satisfying 
\begin_inset Formula $\sum_{i=1}^{p}k_{i}=k\le K+4$
\end_inset

, the higher-order mixture partial derivatives 
\begin_inset Formula 
\[
\frac{\partial^{k}g\left(X,\theta\right)}{\partial\theta_{1}^{k_{1}}\partial\theta_{2}^{k_{2}}\cdots\partial\theta_{p}^{k_{p}}},
\]

\end_inset

exists and continuous in 
\begin_inset Formula $\theta$
\end_inset

, almost surely in 
\begin_inset Formula $X$
\end_inset

.
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:mul-el-smooth-lagrange-mul-1"

\end_inset

Under the Assumption 
\begin_inset CommandInset ref
LatexCommand ref
reference "assu:high-derive-cond"

\end_inset

, all partial derivatives of 
\begin_inset Formula $\nu\left(\theta\right)$
\end_inset

 and 
\begin_inset Formula $\mu\left(\theta\right)$
\end_inset

 are smooth functions of 
\begin_inset Formula $\theta$
\end_inset

 for 
\begin_inset Formula $\theta\in H$
\end_inset

.
\end_layout

\begin_layout Proof
The result is proved by induction.
 We have see already in Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:first-order-smooth-lagmul"

\end_inset

, the gradient 
\begin_inset Formula $\nabla\nu\left(\theta\right)$
\end_inset

 and 
\begin_inset Formula $\nabla\mu\left(\theta\right)$
\end_inset

 are smooth functions of 
\begin_inset Formula $\theta$
\end_inset

.
 Suppose the result holds for all 
\begin_inset Formula $k$
\end_inset

th partial derivatives of 
\begin_inset Formula $\nu\left(\theta\right)$
\end_inset

 and 
\begin_inset Formula $\mu\left(\theta\right)$
\end_inset

 for 
\begin_inset Formula $k\le K$
\end_inset

.
 The writing 
\begin_inset Formula 
\[
\nabla^{k}\nu\left(\theta\right)=h_{k}\left(\nu\left(\theta\right),\theta\right),1\le k\le K,
\]

\end_inset

 
\begin_inset Formula 
\[
\nabla^{k+1}\nu\left(\theta\right)=\frac{\partial h_{k}}{\partial\nu^{T}}\nabla\nu+\frac{\partial h_{k}}{\partial\theta}
\]

\end_inset

which is also a smooth function of 
\begin_inset Formula $\theta$
\end_inset

 by the induction hypothesis and Lemma 1.
 A similar proof works for 
\begin_inset Formula $\mu\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Section
Maximum Generalized Empirical Likelihood Estimator
\end_layout

\begin_layout Standard
One important justification of maximum likelihood estimator is the good
 performance substantiated by large sample theory, such as consistency and
 asymptotic normality.
 It can be shown the above theoretical results can be seamlessly transplanted
 onto the estimator resulted from maximum empirical weights.
 
\begin_inset CommandInset citation
LatexCommand cite
key "qin1994empirical"

\end_inset

 already pointed out the validity to use empirical likelihood to define
 maximum empirical likelihood estimator.
 We extend the same idea into more general Cressie-Read family.
 
\end_layout

\begin_layout Definition
\begin_inset CommandInset label
LatexCommand label
name "def:gmele"

\end_inset


\begin_inset Argument 1
status open

\begin_layout Plain Layout
Maximum Generalized Empirical Likelihood Estimator
\end_layout

\end_inset

Let 
\begin_inset Formula $\hat{w}_{i}\left(\theta\right)$
\end_inset

 be the empirical weights on sample 
\begin_inset Formula $X_{1},X_{2},\ldots,X_{n}$
\end_inset

, generating from empirical likelihood, exponentially tilted empirical likelihoo
d or Cressie-Read family.
 Then 
\begin_inset Formula 
\[
\tilde{\theta}=\argmin_{\theta\in H}-\sum_{i=1}^{n}\log\hat{w}_{i}\left(\theta\right),
\]

\end_inset

called  maximum generalized empirical likelihood estimator.
\end_layout

\begin_layout Standard
Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:mul-el-smooth-lagrange-mul-1"

\end_inset

 establishes the continuity of empirical likelihood and Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:nondecreasing-compact-natural-domain"

\end_inset

 reveals the compactness of 
\begin_inset Formula $H$
\end_inset

, then these results guarantee the existence of 
\begin_inset Formula $\tilde{\theta}$
\end_inset

.
 So this concept is well-defined.
 One advantage of this estimator over the traditional M-estimator is that
 it can always produce a good quality estimator even when traditional one
 has no solution.
 Before exploring the asymptotic properties of this new estimator, we derive
 an equivalent formulation of  maximum generalized empirical likelihood
 estimator, which may be more appropriate for numerical computation and
 theoretical derivation.
 
\end_layout

\begin_layout Standard
First, we define 
\begin_inset Formula $\Psi$
\end_inset

 functions under the three cases.
 In empirical likelihood case, let 
\begin_inset Formula 
\begin{eqnarray*}
\Psi_{1}\left(x\mid\theta,\nu\right) & = & \frac{g\left(x,\theta\right)}{1+\nu^{T}g\left(x,\theta\right)},\\
\Psi_{2}\left(x\mid\theta,\nu\right) & = & \frac{1}{1+\nu^{T}g\left(x,\theta\right)}\nu^{T}\frac{\partial g\left(x,\theta\right)}{\partial\theta}.
\end{eqnarray*}

\end_inset

Let 
\begin_inset Formula $\Psi^{\mathrm{EL}}\left(x\mid\theta,\nu\right)=\left(\Psi_{1},\Psi_{2}\right)^{T}$
\end_inset

.
 In exponentially tilted empirical likelihood, let 
\begin_inset Formula 
\begin{eqnarray*}
\Psi_{1}\left(x\mid\theta,\nu,\mu,\lambda_{1},\lambda_{2}\right) & = & \nu^{T}\frac{\partial g\left(x,\theta\right)}{\partial\theta}-\lambda_{1}\exp\left(-\mu-\nu^{T}g\left(x,\theta\right)\right)\nu^{T}\frac{\partial g\left(x,\theta\right)}{\partial\theta}\\
 &  & -\exp\left(-\mu-\nu^{T}g\left(x,\theta\right)\right)\nu^{T}\frac{\partial g\left(x,\theta\right)}{\partial\theta}\lambda_{2}^{T}g\left(x,\theta\right)+\exp\left(-\mu-\nu^{T}g\left(x,\theta\right)\right)\lambda_{2}^{T}\frac{\partial g\left(x,\theta\right)}{\partial\theta},\\
\Psi_{2}\left(x\mid\theta,\nu,\mu,\lambda_{1},\lambda_{2}\right) & = & g\left(x,\theta\right)-\lambda_{1}\exp\left(-\mu-\nu^{T}g\left(x,\theta\right)\right)g\left(x,\theta\right)-\exp\left(-\mu-\nu^{T}g\left(x,\theta\right)\right)g\left(x,\theta\right)\lambda_{2}^{T}g\left(x,\theta\right),\\
\Psi_{3}\left(x\mid\theta,\nu,\mu,\lambda_{1},\lambda_{2}\right) & = & 1-\lambda_{1}\exp\left(-\mu-\nu^{T}g\left(x,\theta\right)\right)-\exp\left(-\mu-\nu^{T}g\left(x,\theta\right)\right)\lambda_{2}^{T}g\left(x,\theta\right),\\
\Psi_{4}\left(x\mid\theta,\nu,\mu,\lambda_{1},\lambda_{2}\right) & = & \exp\left(-\mu-\nu^{T}g\left(x,\theta\right)\right)-\frac{e}{n},\\
\Psi_{4}\left(x\mid\theta,\nu,\mu,\lambda_{1},\lambda_{2}\right) & = & \exp\left(-\mu-\nu^{T}g\left(x,\theta\right)\right)g\left(x,\theta\right),
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $\lambda_{1}$
\end_inset

 and 
\begin_inset Formula $\lambda_{2}$
\end_inset

 are new Lagrange multipliers which we will define in later lemma.
 Let 
\begin_inset Formula $\Psi^{\mathrm{ET}}\left(x\mid\theta,\nu,\mu,\lambda_{1},\lambda_{2}\right)=\left(\Psi_{1},\ldots,\Psi_{5}\right)^{T}.$
\end_inset

 In Cressie-Read case, let 
\begin_inset Formula 
\begin{eqnarray*}
\Psi_{1}\left(x\mid\theta,\nu,\mu,\lambda_{1},\lambda_{2}\right) & = & \frac{1}{\lambda+1}\Bigg[\frac{\nu^{T}\partial g\left(x,\theta\right)/\partial\theta}{\mu+\nu^{T}g\left(x,\theta\right)}-\lambda_{1}\frac{\nu^{T}\partial g\left(x,\theta\right)/\partial\theta}{\left(\mu+\nu^{T}g\left(x,\theta\right)\right)^{1+1/\left(\lambda+1\right)}}\\
 &  & -\frac{\left(\nu^{T}\partial g\left(x,\theta\right)/\partial\theta\right)\left(\lambda_{2}^{T}g\left(x,\theta\right)\right)}{\left(\mu+\nu^{T}g\left(x,\theta\right)\right)^{1+1/\left(\lambda+1\right)}}+\frac{\lambda_{2}^{T}\partial g\left(x,\theta\right)/\partial\theta}{\left(\mu+\nu^{T}g\left(x,\theta\right)\right)^{1+1/\left(\lambda+1\right)}}\Bigg],\\
\Psi_{2}\left(x\mid\theta,\nu,\mu,\lambda_{1},\lambda_{2}\right) & = & \frac{1}{\lambda+1}\left[\frac{g\left(x,\theta\right)}{\mu+\nu^{T}g\left(x,\theta\right)}-\lambda_{1}\frac{g\left(x,\theta\right)}{\left(\mu+\nu^{T}g\left(x,\theta\right)\right)^{1+1/\left(\lambda+1\right)}}-\frac{g\left(x,\theta\right)\left(\lambda_{2}^{T}g\left(x,\theta\right)\right)}{\left(\mu+\nu^{T}g\left(x,\theta\right)\right)^{1+1/\left(\lambda+1\right)}}\right],\\
\Psi_{3}\left(x\mid\theta,\nu,\mu,\lambda_{1},\lambda_{2}\right) & = & \frac{1}{\lambda+1}\Bigg[\frac{1}{\mu+\nu^{T}g\left(x,\theta\right)}-\lambda_{1}\left(\mu+\nu^{T}g\left(x,\theta\right)\right)^{-1-1/\left(\lambda+1\right)}\\
 &  & -\left(\mu+\nu^{T}g\left(x,\theta\right)\right)^{-1-1/\left(\lambda+1\right)}\lambda_{2}^{T}g\left(x,\theta\right)\Bigg],\\
\Psi_{4}\left(x\mid\theta,\nu,\mu,\lambda_{1},\lambda_{2}\right) & = & \left(\mu+\nu^{T}g\left(x,\theta\right)\right)^{-1/\left(\lambda+1\right)}-1,\\
\Psi_{5}\left(x\mid\theta,\nu,\mu,\lambda_{1},\lambda_{2}\right) & = & \left(\mu+\nu^{T}g\left(x,\theta\right)\right)^{-1/\left(\lambda+1\right)}g\left(x,\theta\right).
\end{eqnarray*}

\end_inset

Let 
\begin_inset Formula $\Psi^{\mathrm{CR}}\left(x\mid\theta,\nu,\mu,\lambda_{1},\lambda_{2}\right)=\left(\Psi_{1},\ldots,\Psi_{5}\right)^{T}$
\end_inset

.
 Use above definitions, we elicit the following lemma.
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:extend-m-estimator"

\end_inset

The  maximum generalized empirical likelihood estimator is the solution
 of 
\begin_inset Formula $n^{-1}\sum_{i=1}^{n}\Psi\left(X_{i}\mid\theta,\nu,\mu,\lambda_{1},\lambda_{2}\right)=0$
\end_inset

.
 If we denote the solution as 
\begin_inset Formula $\left(\theta^{*},\nu^{*},\mu^{*},\lambda_{1}^{*},\lambda_{2}^{*}\right)$
\end_inset

, then 
\begin_inset Formula $\tilde{\theta}=\theta^{*}$
\end_inset

, 
\begin_inset Formula $\nu\left(\tilde{\theta}\right)=\nu^{*}$
\end_inset

 and 
\begin_inset Formula $\mu\left(\tilde{\theta}\right)=\mu^{*}$
\end_inset

.
\end_layout

\begin_layout Proof
For explicit, we state the proof separately under the three kinds of empirical
 likelihoods.
 
\end_layout

\begin_layout Proof
First in empirical likelihood.
 The relationship between 
\begin_inset Formula $\theta$
\end_inset

 and 
\begin_inset Formula $\nu$
\end_inset

 are restricted by equation constraint 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:lambda-eq"

\end_inset

.
 By the Definition 
\begin_inset CommandInset ref
LatexCommand ref
reference "def:gmele"

\end_inset

, the maximum generalized empirical likelihood estimator can also be the
 solution of 
\begin_inset Formula 
\[
\max_{\theta,\nu}l\left(\theta\right)=-\frac{1}{n}\sum_{i=1}^{n}\log\left(1+\nu^{T}g\left(X_{i},\theta\right)\right),
\]

\end_inset

subject to, 
\begin_inset Formula 
\[
\sum_{i=1}^{n}\frac{g\left(X_{i},\theta\right)}{1+\nu^{T}g\left(X_{i},\theta\right)}=0.
\]

\end_inset

Directly calculation shows 
\begin_inset Formula 
\[
\frac{\partial l\left(\theta\right)}{\partial\nu}=\frac{1}{n}\sum_{i=1}^{n}\frac{g\left(X_{i},\theta\right)}{1+\nu^{T}g\left(X_{i},\theta\right)}=\frac{1}{n}\sum_{i=1}^{n}\Psi_{1}\left(X_{i}\mid\theta,\nu\right).
\]

\end_inset

So the solution of unconstrained problem automatically satisfies the constraint
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:lambda-eq"

\end_inset

.
 Moreover, 
\begin_inset Formula 
\[
\frac{\partial l\left(\theta\right)}{\partial\theta}=\frac{1}{n}\sum_{i=1}^{n}\Psi_{2}\left(X_{i}\mid\theta,\nu\right).
\]

\end_inset

So the equation system 
\begin_inset Formula $n^{-1}\sum_{i=1}^{n}\Psi\left(X_{i}\mid\theta,\nu\right)=0$
\end_inset

 is the first order necessary condition of the optimization problem, hence
 the lemma holds in empirical likelihood case.
\end_layout

\begin_layout Proof
Next, we consider exponentially tilted empirical likelihood.
 Reviewing the Lagrange method to get the empirical weights 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:sol-weight-etel"

\end_inset

, we find 
\begin_inset Formula 
\[
\hat{w}_{i}\left(\theta\right)=\exp\left(-1-\mu-\nu^{T}g\left(X_{i},\theta\right)\right),
\]

\end_inset

where 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\nu$
\end_inset

 satisfy 
\begin_inset Formula 
\begin{eqnarray*}
\sum_{i=1}^{n}\exp\left(-1-\mu-\nu^{T}g\left(X_{i},\theta\right)\right) & = & 1,\\
\sum_{i=1}^{n}\exp\left(-1-\mu-\nu^{T}g\left(X_{i},\theta\right)\right)g\left(X_{i},\theta\right) & = & 0.
\end{eqnarray*}

\end_inset

 Hence, the optimization problem in Definition 
\begin_inset CommandInset ref
LatexCommand ref
reference "def:gmele"

\end_inset

 shares the same maximum as the problem 
\begin_inset Formula 
\[
\max_{\theta,\nu,\mu}l\left(\theta\right)=\frac{1}{n}\sum_{i=1}^{n}\left(-1-\mu-\nu^{T}g\left(X_{i},\theta\right)\right),
\]

\end_inset

subject to 
\begin_inset Formula 
\begin{eqnarray*}
\sum_{i=1}^{n}\exp\left(-1-\mu-\nu^{T}g\left(X_{i},\theta\right)\right) & = & 1,\\
\sum_{i=1}^{n}\exp\left(-1-\mu-\nu^{T}g\left(X_{i},\theta\right)\right)g\left(X_{i},\theta\right) & = & 0.
\end{eqnarray*}

\end_inset

Unlike in empirical likelihood case, here we do not have any shortcut.
 The Lagrange multiplier method is the only choice.
 The Lagrangian can be written as 
\begin_inset Formula 
\begin{eqnarray*}
L\left(\theta,\nu,\mu,\lambda_{1},\lambda_{2}\right) & = & \frac{1}{n}\sum_{i=1}^{n}\left(-1-\mu-\nu^{T}g\left(X_{i},\theta\right)\right)+\lambda_{1}\left(\sum_{i=1}^{n}\exp\left(-\mu-\nu^{T}g\left(X_{i},\theta\right)\right)-e\right)\\
 &  & +\lambda_{2}^{T}\left(\sum_{i=1}^{n}\exp\left(-1-\mu-\nu^{T}g\left(X_{i},\theta\right)\right)g\left(X_{i},\theta\right)\right).
\end{eqnarray*}

\end_inset

Calculation shows 
\begin_inset Formula $\nabla L=n^{-1}\sum_{i=1}^{n}\Psi\left(X_{i}\mid\theta,\nu,\mu,\lambda_{1},\lambda_{2}\right)$
\end_inset

.
 Hence, the same argument supporting empirical likelihood case validate
 the lemma in exponentially tilted empirical likelihood case.
\end_layout

\begin_layout Proof
The Cressie-Read case can follow exactly the same procedure in exponentially
 tilted empirical likelihood case.
\end_layout

\begin_layout Standard
This lemma offer a better numerical scheme to get the maximum generalized
 empirical likelihood estimator than the original definition.
 In original definition, for each iteration of 
\begin_inset Formula $\theta$
\end_inset

, one need to solve nonlinear equations to get Lagrange multiplier 
\begin_inset Formula $\nu$
\end_inset

, which will introduce another iteration.
 By using the first order condition as in this lemma, we can use usual nonlinear
 equation solver to get the result in a single layer iteration.
 The benefit of Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:extend-m-estimator"

\end_inset

 also enlighten a quick solution on asymptotic properties of maximum generalized
 empirical likelihood estimator.
 Indeed, it is trivial to see, 
\begin_inset Formula 
\begin{eqnarray*}
E\Psi^{\mathrm{EL}}\left(X\mid\theta_{0},0\right) & = & 0,\\
E\Psi^{\mathrm{ET}}\left(X\mid\theta_{0},0,-1,0\right) & = & 0,\\
E\Psi^{\mathrm{CR}}\left(X\mid\theta_{0},0,1,0,1\right) & = & 0.
\end{eqnarray*}

\end_inset

Hence, maximum generalized empirical likelihood estimator can also be interprete
d as an ordinary M-estimator defined by 
\begin_inset Formula $\Psi$
\end_inset

 function.
 The consistency and asymptotic normality can be easily extracted from ordinary
 M-estimator theory.
 However, in order to expand the posterior around maximum generalized empirical
 likelihood estimator, we need a slight stronger asymptotic property called
 law of iterative logarithm.
 Particularly, the consistency requires the following condition.
\end_layout

\begin_layout Assumption
\begin_inset CommandInset label
LatexCommand label
name "assu:consistency-m-est"

\end_inset


\begin_inset Note Comment
status open

\begin_layout Plain Layout
need to be specified
\end_layout

\end_inset

add later consistency
\end_layout

\begin_layout Standard
The assumptions we need come from 
\begin_inset CommandInset citation
LatexCommand cite
key "he1995law"

\end_inset

.
 We state them as follows.
\end_layout

\begin_layout Assumption
\begin_inset CommandInset label
LatexCommand label
name "assu:lil-m-est"

\end_inset

Let 
\begin_inset Formula $\eta=\left(\theta,\nu,\mu,\lambda_{1},\lambda_{2}\right)$
\end_inset

.
 Let 
\begin_inset Formula $\psi\left(\eta\right)=E\Psi\left(X\mid\eta\right)$
\end_inset

, 
\begin_inset Formula $u\left(x,\eta,d\right)=\sup_{\left|\tau-\eta\right|\le d}\left|\Psi\left(x\mid\tau\right)-\Psi\left(x\mid\eta\right)\right|$
\end_inset

, where 
\begin_inset Formula $\left|\cdot\right|$
\end_inset

 takes sup-norm 
\begin_inset Formula $\left|\eta\right|=\max_{1\le j\le p}\left|\eta_{j}\right|$
\end_inset

.
 Let 
\begin_inset Formula $\eta_{n}$
\end_inset

 satisfy 
\begin_inset Formula 
\[
\frac{1}{\sqrt{n\log\log n}}\sum_{i=1}^{n}\Psi\left(X_{i}\mid\eta_{n}\right)\rightarrow0,\ascv.
\]

\end_inset

 The following conditions guarantee law of iterative logarithm.
\end_layout

\begin_layout Enumerate
For each fixed 
\begin_inset Formula $\eta\in H$
\end_inset

, 
\begin_inset Formula $\Psi\left(x\mid\eta\right)$
\end_inset

 is square integrable in 
\begin_inset Formula $\eta$
\end_inset

 and separable in sense of Doob: there is zero measure set 
\begin_inset Formula $N$
\end_inset

 and a countable subset of 
\begin_inset Formula $H'\subset H$
\end_inset

, such that for every open set 
\begin_inset Formula $U\subset H$
\end_inset

, and every closed interval 
\begin_inset Formula $A$
\end_inset

, the sets 
\begin_inset Formula $\left\{ x:\Psi\left(x\mid\eta\right)\in A,\:\forall\eta\in U\right\} $
\end_inset

 and 
\begin_inset Formula $\left\{ x:\Psi\left(x\mid\eta\right)\in A,\:\forall\eta\in U\cap H'\right\} $
\end_inset

 differ by a subset of 
\begin_inset Formula $N$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Enumerate
There is a 
\begin_inset Formula $\eta_{0}\in H$
\end_inset

, such that 
\begin_inset Formula $\psi\left(\eta_{0}\right)=0$
\end_inset

, and 
\begin_inset Formula $\psi$
\end_inset

 has a non-singular derivative at 
\begin_inset Formula $\eta_{0}$
\end_inset

.
\end_layout

\begin_layout Enumerate
There exist positive numbers 
\begin_inset Formula $a,\: b,\: c,\: d,\:\alpha,\:\beta$
\end_inset

, and 
\begin_inset Formula $d_{0}$
\end_inset

 such that 
\begin_inset Formula $\alpha\ge\beta>2$
\end_inset

, and 
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $\left|\psi\left(\eta\right)\right|\ge a\left|\eta-\eta_{0}\right|$
\end_inset

, for 
\begin_inset Formula $\left|\eta-\eta_{0}\right|\le d_{0}$
\end_inset

,
\end_layout

\begin_layout Enumerate
\begin_inset Formula $Eu\left(x,\eta,d\right)\le bd$
\end_inset

, for 
\begin_inset Formula $\left|\eta-\eta_{0}\right|+d\le d_{0}$
\end_inset

,
\end_layout

\begin_layout Enumerate
\begin_inset Formula $Eu^{\alpha}\left(x,\eta,d\right)\le cd^{\beta}$
\end_inset

, for 
\begin_inset Formula $\left|\eta-\eta_{0}\right|+d\le d_{0}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
\begin_inset Formula $\left|\eta_{n}-\eta_{0}\right|\le d_{0}$
\end_inset

 almost surely as 
\begin_inset Formula $n$
\end_inset

 goes infinity.
\end_layout

\end_deeper
\begin_layout Standard
Actually, the second condition is merely requiring we have a theoretical
 target, and the fourth condition is automatically satisfied if we have
 consistency.
 Immediately by the theorem in 
\begin_inset CommandInset citation
LatexCommand cite
key "he1995law"

\end_inset

, we get the law of iterative logarithm for both maximum generalized empirical
 likelihood estimator and the Lagrange multipliers.
 Now we are ready to state another lemma to tie the empirical likelihood
 weights and likelihood together with following condition on the unknown
 distribution.
\end_layout

\begin_layout Assumption
\begin_inset CommandInset label
LatexCommand label
name "assu:finite-theoretic-moment"

\end_inset

 Let 
\begin_inset Formula $\left(k_{j1},k_{j2},\ldots,k_{jp}\right)\in\mathbb{N}^{p}$
\end_inset

, and 
\begin_inset Formula $\sum_{i=1}^{p}k_{ji}=k_{j}\le K+4$
\end_inset

, 
\begin_inset Formula $j=1,\ldots,l$
\end_inset

, then 
\begin_inset Formula 
\[
E\left(\prod_{j=1}^{l}\frac{\partial^{k_{l}}g\left(X,\theta_{0}\right)}{\partial\theta_{1}^{k_{l1}}\partial\theta_{2}^{k_{l2}}\cdots\partial\theta_{p}^{k_{lp}}}\right),
\]

\end_inset

exists and finite.
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:finite-empirical weight moment"

\end_inset

Let 
\begin_inset Formula $\omega_{i}\left(\theta\right)$
\end_inset

 be the unnormalized empirical weights from empirical likelihood, that is
 
\begin_inset Formula $\omega_{i}\left(\theta\right)=n\hat{w}_{i}\left(\theta\right)$
\end_inset

 in empirical likelihood or Cressie-Read case, and 
\begin_inset Formula $\omega_{i}\left(\theta\right)=\exp\left(-\nu\left(\theta\right)g\left(X_{i},\theta\right)\right)$
\end_inset

 in exponentially tilted empirical likelihood.
 Use the same notation in Assumption 
\begin_inset CommandInset ref
LatexCommand ref
reference "assu:high-derive-cond"

\end_inset

.
 Under the Assumption 
\begin_inset CommandInset ref
LatexCommand ref
reference "assu:high-derive-cond"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "assu:consistency-m-est"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "assu:lil-m-est"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "assu:finite-theoretic-moment"

\end_inset

, then for any 
\begin_inset Formula $k\le K+4$
\end_inset

, any number 
\begin_inset Formula $s\in\mathbb{R}$
\end_inset

, 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\frac{1}{n}\sum_{i=1}^{n}\omega_{i}\left(\tilde{\theta}\right)^{s}\prod_{j=1}^{l}\frac{\partial^{k_{l}}g\left(X,\theta_{0}\right)}{\partial\theta_{1}^{k_{l1}}\partial\theta_{2}^{k_{l2}}\cdots\partial\theta_{p}^{k_{lp}}}=E\left(\prod_{j=1}^{l}\frac{\partial^{k_{l}}g\left(X,\theta_{0}\right)}{\partial\theta_{1}^{k_{l1}}\partial\theta_{2}^{k_{l2}}\cdots\partial\theta_{p}^{k_{lp}}}\right)\ascv.
\]

\end_inset


\end_layout

\begin_layout Proof
By the similar proof in 
\begin_inset Note Comment
status open

\begin_layout Plain Layout
add detail lemma number
\end_layout

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citet
key "owen2010empirical"

\end_inset

, we have 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\max_{1\le i\le n}n^{-1/3}g\left(X_{i},\theta_{0}\right)=0\ascv.
\]

\end_inset

By law of iterative logarithm on Lagrange multiplier 
\begin_inset Formula $\nu$
\end_inset

, there exists some constant 
\begin_inset Formula $C_{1}$
\end_inset

 
\begin_inset Formula 
\[
\varlimsup_{n\rightarrow\infty}\frac{\sqrt{n}\nu\left(\tilde{\theta}\right)}{\sqrt{2\log\log n}}=C_{1}\ascv.
\]

\end_inset

Hence 
\begin_inset Formula 
\[
\varlimsup_{n\rightarrow\infty}\nu^{T}\left(\tilde{\theta}\right)\max_{1\le i\le n}g\left(X_{i},\tilde{\theta}\right)=o\left(\sqrt{\frac{2\log\log n}{n}}\times n^{1/3}\right)=o\left(\frac{\sqrt{2\log\log n}}{n^{1/6}}\right)=0.
\]

\end_inset

So 
\begin_inset Formula $\nu^{T}\left(\tilde{\theta}\right)g\left(X_{i},\tilde{\theta}\right)$
\end_inset

 are uniformly going to zero.
 Hence 
\begin_inset Formula $\omega_{i}\left(\tilde{\theta}\right)$
\end_inset

 are uniformly going to 1 and the lemma holds.
\end_layout

\begin_layout Standard
This lemma supports consistency of empirical sample moment to true moment
 and further justifies the intuition to use empirical likelihood as a valid
 likelihood.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "newey2004higher"

\end_inset

 proposes a similar estimator replacing the objective function in Definition
 
\begin_inset CommandInset ref
LatexCommand ref
reference "def:gmele"

\end_inset

 with divergence measure when one calculates empirical weights.
 Their definition coincides with us when the divergence measure is empirical
 likelihood.
 Although, their starting point is generalized method of moment estimator
 and the intuition is slight different from us, the simulation hardly differs
 the performance.
 In Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:mgele-mde"

\end_inset

, we present the simulation result.
 
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout

% latex table generated in R 3.1.0 by xtable 1.7-3 package % Fri Apr 03 14:30:16
 2015
\end_layout

\begin_layout Plain Layout

%
\backslash
begin{table}[ht]
\end_layout

\begin_layout Plain Layout


\backslash
centering
\end_layout

\begin_layout Plain Layout


\backslash
begin{tabular}{rrrrr}
\end_layout

\begin_layout Plain Layout

  
\backslash
hline
\end_layout

\begin_layout Plain Layout

 & MGELE ETEL & MDE ETEL & MGELE CR & MDE CR 
\backslash

\backslash
 
\end_layout

\begin_layout Plain Layout

  
\backslash
hline
\end_layout

\begin_layout Plain Layout

N=20 & 0.1394 & 0.1320 & 0.2130 & 0.1408 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

   N=50 & 0.0759 & 0.0759 & 0.1867 & 0.0766 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

   N=100 & 0.0536 & 0.0536 & 0.2099 & 0.0539 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

   N=200 & 0.0392 & 0.0401 & 0.2214 & 0.0396 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

   N=500 & 0.0274 & 0.0298 & 0.1824 & 0.0274 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

    
\backslash
hline
\end_layout

\begin_layout Plain Layout


\backslash
end{tabular}
\end_layout

\begin_layout Plain Layout

%
\backslash
end{table}
\end_layout

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:mgele-mde"

\end_inset

Compare Performance MGELE and MDE
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
old stuff have not been modified
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:bell-shape-el"

\end_inset

 
\begin_inset Note Comment
status open

\begin_layout Plain Layout
need condition 
\end_layout

\end_inset


\begin_inset Formula $\diff^{2}\tilde{l}\left(\tilde{\theta}\right)/\diff\theta^{2}<0$
\end_inset

 where 
\begin_inset Formula $\tilde{l}\left(\theta\right)=n^{-1}\sum_{i=1}^{n}\log\hat{w}_{i}\left(\theta\right)$
\end_inset

 where 
\begin_inset Formula $\hat{w}_{i}$
\end_inset

 is either 
\begin_inset Formula $\hat{w}_{i}^{\mathrm{EL}}$
\end_inset

, 
\begin_inset Formula $\hat{w}_{i}^{\mathrm{ET}}$
\end_inset

 or 
\begin_inset Formula $\hat{w}_{i}^{\mathrm{CR}}$
\end_inset

 
\begin_inset Formula $\left(i=1,2,\ldots,n\right)$
\end_inset

.
\end_layout

\begin_layout Proof
We begin with 
\begin_inset Formula $\tilde{l}\left(\theta\right)=n^{-1}\sum_{i=1}^{n}\log\hat{w}_{i}^{\mathrm{EL}}\left(\theta\right)=-\sum_{i=1}^{n}\log\left[1+\nu\left(X_{i}-\theta\right)\right]-\log n$
\end_inset

.
 Hence by 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:sol-emp-lik"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:lambda-eq"

\end_inset

, 
\begin_inset Formula 
\[
\frac{\diff\tilde{l}\left(\theta\right)}{\diff\theta}=\frac{1}{n}\nu\sum_{i=1}^{n}\frac{1}{1+\nu g\left(X_{i}\theta\right)}\frac{\diff g\left(X_{i},\theta\right)}{\diff\theta}-\frac{1}{n}\sum_{i=1}^{n}\frac{g\left(X_{i}\theta\right)}{1+\nu g\left(X_{i}\theta\right)}\frac{\diff\nu}{\diff\theta}=\frac{1}{n}\nu\left(\theta\right)\sum_{i=1}^{n}\frac{1}{1+\nu g\left(X_{i}\theta\right)}\frac{\diff g\left(X_{i},\theta\right)}{\diff\theta}.
\]

\end_inset


\end_layout

\begin_layout Proof
Thus 
\begin_inset Formula 
\[
\left.\frac{\diff^{2}\tilde{l}\left(\theta\right)}{\diff\theta^{2}}\right|_{\theta=\tilde{\theta}}=-\frac{\left(\sum_{i=1}^{n}\diff g\left(X_{i},\tilde{\theta}\right)/\diff\theta\right)^{2}}{n\sum_{i=1}^{n}g\left(X_{i},\tilde{\theta}\right)^{2}}<0.
\]

\end_inset


\end_layout

\begin_layout Proof
Next we consider 
\begin_inset Formula $\tilde{l}\left(\theta\right)=n^{-1}\sum_{i=1}^{n}\log\hat{w}_{i}^{\mathrm{ET}}\left(\theta\right)=-\left(\nu n^{-1}\sum_{i=1}^{n}g\left(X_{i},\theta\right)+\log\sum_{i=1}^{n}\exp\left(-\nu g\left(X_{i},\theta\right)\right)\right)$
\end_inset

.
 Then 
\begin_inset Formula 
\begin{eqnarray*}
\frac{\diff\tilde{l}\left(\theta\right)}{\diff\theta} & = & -\frac{\diff\nu}{\diff\theta}\frac{1}{n}\sum_{i=1}^{n}g\left(X_{i},\theta\right)\\
 &  & +\frac{\sum_{i=1}^{n}\exp\left(-\nu g\left(X_{i},\theta\right)\right)\left(\diff\nu/\diff\theta g\left(X_{i},\theta\right)+\nu\diff g\left(X_{i},\theta\right)/\diff\theta\right)}{\sum_{i=1}^{n}\exp\left(-\nu g\left(X_{i},\theta\right)\right)}\\
 & = & -\frac{\diff\nu}{\diff\theta}\frac{1}{n}\sum_{i=1}^{n}g\left(X_{i},\theta\right)+\nu\frac{\sum_{i=1}^{n}\exp\left(-\nu g\left(X_{i},\theta\right)\right)\diff g\left(X_{i},\theta\right)/\diff\theta}{\sum_{i=1}^{n}\exp\left(-\nu g\left(X_{i},\theta\right)\right)}\\
 &  & -\nu n^{-1}\sum_{i=1}^{n}\frac{\diff g\left(X_{i},\theta\right)}{\diff\theta}.
\end{eqnarray*}

\end_inset

Thus 
\begin_inset Formula 
\[
\left.\frac{\diff^{2}\tilde{l}\left(\theta\right)}{\diff\theta^{2}}\right|_{\theta=\tilde{\theta}}=-\frac{\diff\nu}{\diff\theta}\frac{1}{n}\sum_{i=1}^{n}\frac{\diff g\left(X_{i},\tilde{\theta}\right)}{\diff\theta}=-\frac{\left(\sum_{i=1}^{n}\diff g\left(X_{i},\tilde{\theta}\right)/\diff\theta\right)^{2}}{n\sum_{i=1}^{n}g\left(X_{i},\tilde{\theta}\right)^{2}}<0.
\]

\end_inset


\end_layout

\begin_layout Proof
Finally, for CR, 
\begin_inset Formula $\tilde{l}\left(\theta\right)=n^{-1}\sum_{i=1}^{n}\log\hat{w}_{i}^{\mathrm{CR}}\left(\theta\right)=-\left[n\left(\lambda+1\right)\right]^{-1}\sum_{i=1}^{n}\log\left(\mu+\nu g\left(X_{i},\theta\right)\right)$
\end_inset

.
 Then by 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:first-der-lag-mul-crel"

\end_inset

, 
\begin_inset Formula 
\[
\frac{\diff\tilde{l}\left(\theta\right)}{\diff\theta}=.
\]

\end_inset


\begin_inset Note Comment
status open

\begin_layout Plain Layout
need to compute the first and second order derivative
\end_layout

\end_inset

Thus 
\begin_inset Formula 
\[
\left.\frac{\diff^{2}\tilde{l}\left(\theta\right)}{\diff\theta^{2}}\right|_{\theta=\overline{X}}=-\frac{\left(\sum_{i=1}^{n}\diff g\left(X_{i},\tilde{\theta}\right)/\diff\theta\right)^{2}}{n\sum_{i=1}^{n}g\left(X_{i},\tilde{\theta}\right)^{2}}.
\]

\end_inset


\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $b=\left[\left(n^{-1}\sum_{i=1}^{n}\diff g\left(X_{i},\tilde{\theta}\right)/\diff\theta\right)^{2}/\left(n^{-1}\sum_{i=1}^{n}g\left(X_{i},\tilde{\theta}\right)^{2}\right)\right]^{-1/2}$
\end_inset

.
 Now we prove the main result in the next section.
\end_layout

\begin_layout Section
Assumptions
\end_layout

\begin_layout Enumerate
\begin_inset CommandInset label
LatexCommand label
name "enu:multv-diff-full-rank"

\end_inset

In multivariate empirical likelihood settings, matrix 
\begin_inset Formula $Z$
\end_inset

 has full column rank with probability 1.
 
\begin_inset Note Comment
status open

\begin_layout Plain Layout
brief discussion the stat meaning of assumption
\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset CommandInset label
LatexCommand label
name "enu:finite-moment-sample"

\end_inset


\begin_inset Formula $E\prod_{s=1}^{K}\left(X_{ij_{s}}-\theta_{j_{s},0}\right),\:\left(j_{1},j_{2},\ldots,j_{K}\right)\subset\left\{ 1,2,\ldots,K\right\} $
\end_inset

, where 
\begin_inset Formula $\theta_{0}$
\end_inset

 is the true expectation of 
\begin_inset Formula $X$
\end_inset

, exists and finite.
 
\begin_inset Note Comment
status open

\begin_layout Plain Layout
need a math induction lemma for finiteness of constant, need to be more
 specified
\end_layout

\end_inset


\begin_inset Note Comment
status open

\begin_layout Plain Layout
need a uniform bounded
\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset CommandInset label
LatexCommand label
name "enu:smooth-piror"

\end_inset

The support of prior contains 
\begin_inset Formula $H_{n}$
\end_inset

, and 
\begin_inset Formula $\rho\left(\theta\right)\in C^{K}$
\end_inset

.
 
\end_layout

\begin_layout Enumerate
\begin_inset CommandInset label
LatexCommand label
name "enu:pd-sample-var"

\end_inset

With probability 1 in 
\begin_inset Formula $P_{X}^{n}$
\end_inset

, 
\begin_inset Formula $n^{-1}\sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2}$
\end_inset

 is a positive definite matrix.
 
\end_layout

\begin_layout Section
main result
\begin_inset CommandInset label
LatexCommand label
name "sec:main-result"

\end_inset


\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\overline{X}$
\end_inset

 be the sample mean.
 Let 
\begin_inset Formula $\nu=\nu\left(\theta\right)$
\end_inset

 be Lagrange multiplier obtained from 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:lambda-eq"

\end_inset

 which are smooth functions of 
\begin_inset Formula $\theta$
\end_inset

 by 
\begin_inset Note Comment
status open

\begin_layout Plain Layout
smooth lemma
\end_layout

\end_inset

 in appendix.
 Let 
\begin_inset Formula $\hat{l}=\hat{l}_{n}\left(\theta\right)=n^{-1}\sum_{i=1}^{n}\ln\hat{w}_{i}\left(\theta\right)$
\end_inset

 be the logarithm of empirical likelihood.
 Let 
\begin_inset Formula $P_{X}^{n}$
\end_inset

 be the underlying probability measure for sample 
\begin_inset Formula $X_{1},X_{2},\ldots,X_{n}$
\end_inset

.
 
\end_layout

\begin_layout Standard
For multivariate empirical likelihood, let 
\begin_inset Formula 
\[
Z=\left(\begin{array}{cccccc}
X_{11}-X_{21} & X_{12}-X_{22} & \cdots & X_{1j}-X_{2j} & \cdots & X_{1p}-X_{2p}\\
X_{11}-X_{31} & X_{12}-X_{32} & \cdots & X_{1j}-X_{3j} & \cdots & X_{1p}-X_{3p}\\
\cdots & \cdots & \cdots & \cdots & \cdots & \cdots\\
X_{i1}-X_{j1} & X_{i2}-X_{l2} & \cdots & X_{ij}-X_{lj} & \cdots & X_{ip}-X_{lp}\\
\cdots & \cdots & \cdots & \cdots & \cdots & \cdots\\
X_{p-1,1}-X_{p1} & X_{p-1,2}-X_{p2} & \cdots & X_{p-1,j}-X_{pj} & \cdots & X_{p-1,p}-X_{pp}
\end{array}\right).
\]

\end_inset


\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $K$
\end_inset

 be any positive integer and 
\begin_inset Formula $K\ge3$
\end_inset

.
 For multivariate case in empirical likelihood, let 
\begin_inset Formula $B$
\end_inset

 be the Cholesky decomposition of negative of the Hessian matrix 
\begin_inset Formula 
\[
-\left.\frac{\partial^{2}\hat{l}}{\partial\theta^{T}\partial\theta}\right|_{\theta=\overline{X}}=B^{T}B.
\]

\end_inset

Let 
\begin_inset Formula $Y=\sqrt{n}B\left(\theta-\overline{X}\right)$
\end_inset

, define differential operators 
\begin_inset Formula 
\[
\delta_{i}=\frac{1}{i!}\left(Y^{T}B^{-T}\nabla\right)^{i},
\]

\end_inset

where 
\begin_inset Formula $\nabla$
\end_inset

 is the gradient operator.
 
\end_layout

\begin_layout Standard
We expand the prior around the sample mean 
\begin_inset Formula $\overline{X}$
\end_inset

 and have 
\begin_inset Formula 
\begin{eqnarray*}
\rho_{K}\left(\theta\right) & = & \rho\left(\overline{X}\right)+\delta_{1}\rho\left(\overline{X}\right)n^{-\frac{1}{2}}+\delta_{2}\rho\left(\overline{X}\right)n^{-1}+\cdots+\delta_{K}\rho\left(\overline{X}\right)n^{-\frac{K}{2}},
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $\delta_{i}\rho\left(\overline{X}\right)$
\end_inset

 are results from applying the differential operators prior density then
 evaluating at 
\begin_inset Formula $\overline{X}$
\end_inset

.
 We abbreviate 
\begin_inset Formula $\delta_{i}\rho\left(\overline{X}\right),\: i=2,3,\ldots K$
\end_inset

 as 
\begin_inset Formula $\delta_{i}\rho$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
add multivariate case coefficients
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $H_{n}$
\end_inset

 be the convex hull of samples.
 
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $I_{i,h}=\left\{ \left(m_{3,i},m_{4,i},\ldots,m_{K+3,i}\right)\in\mathbb{N}^{K}:\sum_{u=3}^{K+3}m_{u,i}=i,\:\sum_{u=3}^{K+3}m_{u,i}\left(u-2\right)=h\right\} $
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
the formula is wrong, need a new one.
 
\end_layout

\end_inset

Let 
\begin_inset Formula $P_{K}\left(A,n\right)$
\end_inset

 be the expansion polynomial 
\begin_inset Formula 
\begin{eqnarray*}
 &  & P_{K}\left(A,n\right)\\
 & = & \left(\rho\left(\overline{X}\right)\int_{A\cap H_{n}}\exp\left(-\frac{Y^{T}Y}{2}\right)\diff Y\right)n^{-\frac{1}{2}}+\left(\int_{A\cap H_{n}}\exp\left(-\frac{Y^{T}Y}{2}\right)\left(\delta_{1}\rho+\rho\left(\overline{X}\right)\delta_{3}\hat{l}\right)\diff Y\right)n^{-1}\\
 &  & +\sum_{h=2}^{K}\Bigg\{\int_{A\cap H_{n}}\exp\left(-\frac{Y^{T}Y}{2}\right)\Bigg[\delta_{h}\rho\\
 &  & +\sum_{j=0}^{h-1}\delta_{j}\rho\sum_{\frac{h-j}{K+1}\le i\le h-j}\frac{1}{i!}\sum_{I_{i,h-j}}\binom{i}{m_{3,i},m_{4,i},\ldots,m_{K+3,i}}\prod_{u=3}^{K+3}\left(\delta_{u}\hat{l}\right)^{m_{u,i}}\Bigg]\diff Y\Bigg\} n^{-\frac{h+1}{2}}\\
 & = & \int_{A\cap H_{n}}\exp\left(-\frac{Y^{T}Y}{2}\right)\sum_{h=0}^{K}\alpha_{h}\left(Y,n\right)n^{-\frac{h}{2}}\diff n^{-\frac{1}{2}}Y,
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $\alpha_{h}\left(Y,n\right)$
\end_inset

 are corresponding coefficients before each order of 
\begin_inset Formula $n$
\end_inset

.
 
\begin_inset Note Comment
status open

\begin_layout Plain Layout
add normal case and multivariate case
\end_layout

\end_inset


\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:main-theorem"

\end_inset


\begin_inset Argument 1
status collapsed

\begin_layout Plain Layout
main theorem
\end_layout

\end_inset

Under the assumptions 
\begin_inset CommandInset ref
LatexCommand formatted
reference "enu:finite-moment-sample"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "enu:multv-diff-full-rank"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "enu:pd-sample-var"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "enu:smooth-piror"

\end_inset

, there exist a positive constant 
\begin_inset Formula $M_{1}$
\end_inset

 and a large integer 
\begin_inset Formula $N_{1}$
\end_inset

, such that for any Borel set 
\begin_inset Formula $A\subset\mathbb{R}^{p}$
\end_inset

, and any 
\begin_inset Formula $n>N_{1}$
\end_inset


\begin_inset Note Comment
status open

\begin_layout Plain Layout
add subscript to constant
\end_layout

\end_inset

 
\begin_inset Formula 
\[
\left|\int_{A\cap H_{n}}\prod_{i=1}^{n}\hat{w}_{i}\left(\theta\right)\pi\left(\theta\right)\diff n^{-\frac{1}{2}}Y-P_{K}\left(A,n\right)\right|\le M_{1}n^{-\frac{K+2}{2}},\ascv.
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
add main theorem
\end_layout

\end_inset

This theorem can be used to prove many asymptotic results
\end_layout

\begin_layout Theorem
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Asymptotic expansion
\end_layout

\end_inset

Under the same assumptions as 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:main-theorem"

\end_inset

, there exist a positive constant 
\begin_inset Formula $M_{2}$
\end_inset

 and a large integer 
\begin_inset Formula $N_{2}$
\end_inset

, such that for any Borel set 
\begin_inset Formula $A\subset\mathbb{R}^{p}$
\end_inset

 and any 
\begin_inset Formula $n>N_{2}$
\end_inset

,
\begin_inset Note Comment
status open

\begin_layout Plain Layout
add subscript to constant
\end_layout

\end_inset

 
\begin_inset Formula 
\begin{equation}
\left|\Pi\left(B\left(\theta-\overline{X}\right)\in A|X_{1},X_{2},\ldots,X_{n}\right)-\Phi_{p}\left(A|H_{n}\right)-\sum_{i=1}^{K}\gamma_{i}\left(A,n\right)n^{-\frac{i}{2}}\right|\le M_{1}n^{-\frac{K+1}{2}},\ascv.\label{eq:asym-exp-poster-prob}
\end{equation}

\end_inset


\end_layout

\begin_deeper
\begin_layout Proof
By 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:main-theorem"

\end_inset

, we have 
\begin_inset Formula 
\begin{equation}
\left|\int_{A\cap H_{n}}\prod_{i=1}^{n}\hat{w}_{i}\left(\theta\right)\pi\left(\theta\right)\diff n^{-\frac{1}{2}}Y-P_{K}\left(A,n\right)\right|\le M_{1}n^{-\frac{K+2}{2}},\label{eq:main-theorem-result-A}
\end{equation}

\end_inset

and 
\begin_inset Formula 
\begin{equation}
\left|\int_{H_{n}}\prod_{i=1}^{n}\hat{w}_{i}\left(\theta\right)\pi\left(\theta\right)\diff n^{-\frac{1}{2}}Y-P_{K}\left(H_{n},n\right)\right|\le M_{1}n^{-\frac{K+2}{2}}.\label{eq:main-theorem-result-Hn}
\end{equation}

\end_inset

By definition 
\begin_inset Formula 
\[
\Pi\left(B\left(\theta-\overline{X}\right)\in A|X_{1},X_{2},\ldots,X_{n}\right)=\frac{\int_{A\cap H_{n}}\prod_{i=1}^{n}\hat{w}_{i}\left(\theta\right)\pi\left(\theta\right)\diff n^{-\frac{1}{2}}Y}{\int_{H_{n}}\prod_{i=1}^{n}\hat{w}_{i}\left(\theta\right)\pi\left(\theta\right)\diff n^{-\frac{1}{2}}Y}.
\]

\end_inset


\begin_inset Note Comment
status open

\begin_layout Plain Layout
add more detail in bounded.
 bounded above and below.
\end_layout

\end_inset

We know that all terms in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:main-theorem-result-A"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:main-theorem-result-Hn"

\end_inset

, are almost surely bounded by some constant 
\begin_inset Formula $C_{1}$
\end_inset

 for all 
\begin_inset Formula $n>N_{1}$
\end_inset

, then there exists a constant 
\begin_inset Formula 
\begin{eqnarray}
 &  & \left|\Pi\left(B\left(\theta-\overline{X}\right)\in A|X_{1},X_{2},\ldots,X_{n}\right)-\frac{P_{K}\left(A,n\right)}{P_{K}\left(H_{n},n\right)}\right|\nonumber \\
 & \le & \left|\frac{\int_{A\cap H_{n}}\prod_{i=1}^{n}\hat{w}_{i}\left(\theta\right)\pi\left(\theta\right)\diff n^{-\frac{1}{2}}Y}{\int_{H_{n}}\prod_{i=1}^{n}\hat{w}_{i}\left(\theta\right)\pi\left(\theta\right)\diff n^{-\frac{1}{2}}Y}-\frac{P_{K}\left(A,n\right)}{P_{K}\left(H_{n},n\right)}\right|\nonumber \\
 & = & \Bigg|\frac{\int_{A\cap H_{n}}\prod_{i=1}^{n}\hat{w}_{i}\left(\theta\right)\pi\left(\theta\right)\diff n^{-\frac{1}{2}}Y-P_{K}\left(A,n\right)}{\int_{H_{n}}\prod_{i=1}^{n}\hat{w}_{i}\left(\theta\right)\pi\left(\theta\right)\diff n^{-\frac{1}{2}}Y}\nonumber \\
 &  & -\frac{\left(\int_{H_{n}}\prod_{i=1}^{n}\hat{w}_{i}\left(\theta\right)\pi\left(\theta\right)\diff n^{-\frac{1}{2}}Y-P_{K}\left(H_{n},n\right)\right)P_{K}\left(A,n\right)}{\int_{H_{n}}\prod_{i=1}^{n}\hat{w}_{i}\left(\theta\right)\pi\left(\theta\right)\diff n^{-\frac{1}{2}}YP_{K}\left(H_{n},n\right)}\Bigg|\nonumber \\
 & \le & \frac{1}{\left|\int_{H_{n}}\prod_{i=1}^{n}\hat{w}_{i}\left(\theta\right)\pi\left(\theta\right)\diff n^{-\frac{1}{2}}Y\right|}\Bigg(\left|\int_{A\cap H_{n}}\prod_{i=1}^{n}\hat{w}_{i}\left(\theta\right)\pi\left(\theta\right)\diff n^{-\frac{1}{2}}Y-P_{K}\left(A,n\right)\right|\nonumber \\
 &  & +\left|\int_{H_{n}}\prod_{i=1}^{n}\hat{w}_{i}\left(\theta\right)\pi\left(\theta\right)\diff n^{-\frac{1}{2}}Y-P_{K}\left(H_{n},n\right)\right|\left|\frac{P_{K}\left(A,n\right)}{P_{K}\left(H_{n},n\right)}\right|\Bigg)\nonumber \\
 & \le & \frac{1}{C_{1}}\left(M_{1}n^{-\frac{K+2}{2}}+M_{1}n^{-\frac{K+2}{2}}\frac{C_{2}}{C_{1}}\right)=\frac{M_{1}}{C_{1}}\left(1+\frac{C_{1}}{C_{2}}\right)n^{-\frac{K+2}{2}}.\label{eq:two-quotient-close}
\end{eqnarray}

\end_inset

Now we find the quotient series of 
\begin_inset Formula $P_{K}\left(A,n\right)/P_{K}\left(H_{n},n\right)$
\end_inset

.
 Let 
\begin_inset Formula 
\[
\frac{P_{K}\left(A,n\right)}{P_{K}\left(H_{n},n\right)}=\sum_{i=0}^{\infty}\gamma_{i}\left(A,n\right)n^{-\frac{i}{2}},
\]

\end_inset

then by the product rule of series we have the coefficients 
\begin_inset Formula $\gamma_{i}\left(A,n\right)$
\end_inset

 are determined by 
\begin_inset Formula 
\[
\int_{A\cap H_{n}}\exp\left(-\frac{Y^{T}Y}{2}\right)\alpha_{h}\left(Y,n\right)\diff Y=\sum_{j=0}^{h}\int_{H_{n}}\exp\left(-\frac{Y^{T}Y}{2}\right)\alpha_{j}\left(Y,n\right)\diff Y\gamma_{h-j}\left(A,n\right).
\]

\end_inset

Through simple calculation, we can find first two items of 
\begin_inset Formula $\gamma_{i}\left(A,n\right)$
\end_inset

 is 
\begin_inset Formula 
\begin{eqnarray*}
\gamma_{0}\left(A,n\right) & = & \frac{\rho\left(\overline{X}\right)\int_{A\cap H_{n}}\exp\left(-\frac{Y^{T}Y}{2}\right)\diff Y}{\rho\left(\overline{X}\right)\int_{H_{n}}\exp\left(-\frac{Y^{T}Y}{2}\right)\diff Y}=\frac{\int_{A\cap H_{n}}\exp\left(-\frac{Y^{T}Y}{2}\right)\diff Y}{\int_{H_{n}}\exp\left(-\frac{Y^{T}Y}{2}\right)\diff Y}=\Phi_{p}\left(A|H_{n}\right),\\
\gamma_{1}\left(A,n\right) & = & \frac{\int_{A\cap H_{n}}\exp\left(-\frac{Y^{T}Y}{2}\right)\left(\delta_{1}\rho+\rho\left(\overline{X}\right)\delta_{3}\hat{l}\right)\diff Y}{\int_{H_{n}}\exp\left(-\frac{Y^{T}Y}{2}\right)\diff Y}\\
 &  & -\frac{-\int_{H_{n}}\exp\left(-\frac{Y^{T}Y}{2}\right)\left(\delta_{1}\rho+\rho\left(\overline{X}\right)\delta_{3}\hat{l}\right)\diff Y\Phi_{p}\left(A|H_{n}\right)}{\int_{H_{n}}\exp\left(-\frac{Y^{T}Y}{2}\right)\diff Y}\\
 & = & \frac{\int_{A\cap H_{n}}\left(\delta_{1}\rho+\rho\left(\overline{X}\right)\delta_{3}\hat{l}\right)\varphi_{p}\left(Y\right)\diff Y}{\Phi_{p}\left(H_{n}\right)}-\frac{\int_{H_{n}}\left(\delta_{1}\rho+\rho\left(\overline{X}\right)\delta_{3}\hat{l}\right)\varphi_{p}\left(Y\right)\diff Y}{\Phi_{p}\left(H_{n}\right)}\Phi_{p}\left(A|H_{n}\right),
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $\varphi_{p}$
\end_inset

 is the density of dimension 
\begin_inset Formula $p$
\end_inset

 standard normal distribution, 
\begin_inset Formula $\Phi_{p}$
\end_inset

 is the probability or conditional probability of dimension 
\begin_inset Formula $p$
\end_inset

 standard normal distribution.
 By the discussion following 
\begin_inset CommandInset ref
LatexCommand formatted
reference "lem:control-higher-order-derivative-l"

\end_inset

, we know that all 
\begin_inset Formula $\gamma_{i}$
\end_inset

 are almost surely uniformly bounded for all large 
\begin_inset Formula $n$
\end_inset

.
 Then there exists a constant 
\begin_inset Formula $M_{2}$
\end_inset

, such that 
\begin_inset Formula 
\begin{equation}
\left|\frac{P_{K}\left(A,n\right)}{P_{K}\left(H_{n},n\right)}-\Phi_{p}\left(A|H_{n}\right)-\sum_{i=1}^{K}\gamma_{i}\left(A,n\right)n^{-\frac{i}{2}}\right|\le M_{2}n^{-\frac{K+1}{2}}.\label{eq:quotient-serier-approx}
\end{equation}

\end_inset

Combine 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:two-quotient-close"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:quotient-serier-approx"

\end_inset

, we get 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:asym-exp-poster-prob"

\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
We will show detail proof of the this theorem in appendix.
 The proof are similar to 
\begin_inset CommandInset citation
LatexCommand cite
key "johnson1970asymptotic"

\end_inset

, with some modification to apply to empirical likelihood framework.
 
\end_layout

\begin_layout Corollary
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Asymptotic normality 
\end_layout

\end_inset


\end_layout

\begin_layout Section
application
\begin_inset CommandInset label
LatexCommand label
name "sec:application"

\end_inset


\end_layout

\begin_layout Subsection
Simulation study
\end_layout

\begin_layout Standard
For simplicity, we simulate the situation where 
\begin_inset Formula $p=1$
\end_inset

.
 In this case the first two terms in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:asym-exp-poster-prob"

\end_inset

 will be 
\begin_inset Formula $\Phi_{1}\left(\left(-\infty,\xi\right]|\left(X_{\left(1\right)},X_{\left(n\right)}\right)\right)$
\end_inset

 and 
\begin_inset Formula 
\begin{eqnarray*}
\gamma_{1}\left(\left(-\infty,\xi\right],n\right) & = & \frac{\rho'\left(\overline{X}\right)\hat{\sigma}\int_{\left(-\infty,\xi\right]\cap\left(X_{\left(1\right)},X_{\left(n\right)}\right)}y\varphi_{1}\left(y\right)\diff y+\rho\left(\overline{X}\right)\hat{\sigma}^{3}\hat{l}^{\left(3\right)}\left(\overline{X}\right)\int_{\left(-\infty,\xi\right]\cap\left(X_{\left(1\right)},X_{\left(n\right)}\right)}y^{3}\varphi\left(y\right)\diff y}{\Phi\left(\left(X_{\left(1\right)},X_{\left(n\right)}\right)\right)}\\
 &  & -\frac{\rho'\left(\overline{X}\right)\hat{\sigma}\int_{\left(X_{\left(1\right)},X_{\left(n\right)}\right)}y\varphi_{1}\left(y\right)\diff y+\rho\left(\overline{X}\right)\hat{\sigma}^{3}\hat{l}^{\left(3\right)}\left(\overline{X}\right)\int_{\left(X_{\left(1\right)},X_{\left(n\right)}\right)}y^{3}\varphi\left(y\right)\diff y}{\Phi\left(\left(X_{\left(1\right)},X_{\left(n\right)}\right)\right)}\\
 &  & \Phi_{1}\left(\left(-\infty,\xi\right]|\left(X_{\left(1\right)},X_{\left(n\right)}\right)\right),
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $X_{\left(i\right)}$
\end_inset

 are order statistics, and 
\begin_inset Formula 
\[
\hat{l}^{\left(3\right)}=\frac{2n^{2}\sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{3}}{\hat{\sigma}^{6}}.
\]

\end_inset


\end_layout

\begin_layout Section
discussion
\begin_inset CommandInset label
LatexCommand label
name "sec:discussion"

\end_inset


\end_layout

\begin_layout Chapter
On the Empirical Likelihood Option Pricing
\end_layout

\begin_layout Chapter
Approximate Bayesian Computation via Sufficient Dimension Reduction
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
There are two main objectives of this article.
 First, we want to provide some theoretical results related to the currently
 emerging topic of approximate Bayesian computation (ABC).
 The second is to show some connectivity between ABC and another important
 emerging topic of research, namely, sufficient dimension reduction (SDR).
 While the latter has surfaced primarily in the
\color black
 frequentist's
\color red
 
\color inherit
domain of research, it is possible to tie it with ABC as well.
 In particular, we want to show how ABC can be carried through nonlinear
 SDR.
 
\end_layout

\begin_layout Standard
Modern science invokes more and more Byzantine stochastic models, such as
 stochastic kinetic network (
\begin_inset CommandInset citation
LatexCommand citet
key "wilkinson2011stochastic"

\end_inset

), differential equation system (
\begin_inset CommandInset citation
LatexCommand citet
key "picchini2014inference"

\end_inset

) and multi-hierarchical model (
\begin_inset CommandInset citation
LatexCommand citet
key "jasra2012filtering"

\end_inset

), whose computational complexity and intractability challenge the application
 of classical statistical inference.
 Traditional maximum likelihood methods will malfunction when the evaluation
 of likelihoods becomes slow and inaccurate.
 Lack of analytical form of the likelihood also 
\color black
undermines 
\color inherit
 the usage of Bayesian inferential tools, such as Markov chain Monte Carlo
 (MCMC), Laplace approximation (
\begin_inset CommandInset citation
LatexCommand citet
key "tierney1986accurate"

\end_inset

), variational Bayes (
\begin_inset CommandInset citation
LatexCommand citet
key "jaakkola2000bayesian"

\end_inset

) and posterior expansion (
\begin_inset CommandInset citation
LatexCommand citet
key "johnson1970asymptotic"

\end_inset

, Zhong and Ghosh
\begin_inset Note Comment
status open

\begin_layout Plain Layout
need ask
\end_layout

\end_inset

).
 
\end_layout

\begin_layout Standard
The ABC methodology 
\color black
stems from
\color inherit
 the observation that the interpretability of the candidate model usually
 leads to an applicable sampler of data given parameters, and ingeniously
 circumvents the evaluation of likelihood functions.
 The idea behind ABC can be summarized as follows: 
\begin_inset Float algorithm
placement H
wide false
sideways false
status open

\begin_layout Enumerate
Sample parameters 
\begin_inset Formula $\theta_{i}$
\end_inset

 from the prior distribution 
\begin_inset Formula $\pi\left(\theta\right)$
\end_inset

;
\end_layout

\begin_layout Enumerate
Sample data 
\begin_inset Formula $Z_{i}$
\end_inset

 based on the model 
\begin_inset Formula $f\left(z\mid\theta_{i}\right)$
\end_inset

;
\end_layout

\begin_layout Enumerate
Compare the simulated data 
\begin_inset Formula $Z_{i}$
\end_inset

 and the observed data 
\begin_inset Formula $X_{i,\mathrm{obs}}$
\end_inset

, to accept or reject 
\begin_inset Formula $\theta_{i}$
\end_inset

.
\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Idea of ABC
\end_layout

\end_inset


\end_layout

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citet
key "rubin1984bayesianly"

\end_inset

 first mentioned this idea and 
\begin_inset CommandInset citation
LatexCommand citet
key "tavare1997inferring"

\end_inset

 proposed the first version of ABC, which studying population genetics.
 The prototype of ABC in recent research was given in 
\begin_inset CommandInset citation
LatexCommand citet
key "pritchard1999population"

\end_inset

, where the comparison of two data sets was simplified to a comparison of
 summary statistics 
\begin_inset Formula $S$
\end_inset

 and the accept-reject decision was made up to a certain error tolerance.
 
\begin_inset Float algorithm
placement h
wide false
sideways false
status open

\begin_layout Enumerate
Sample parameters 
\begin_inset Formula $\theta_{i}$
\end_inset

 from the prior distribution 
\begin_inset Formula $\pi\left(\theta\right)$
\end_inset

;
\end_layout

\begin_layout Enumerate
Sample data 
\begin_inset Formula $Z_{i}$
\end_inset

 based on the model 
\begin_inset Formula $f\left(z\mid\theta_{i}\right)$
\end_inset

;
\end_layout

\begin_layout Enumerate
Accept 
\begin_inset Formula $\theta_{i}$
\end_inset

 if 
\begin_inset Formula $\rho\left(S\left(Z_{i}\right),S\left(X_{\mathrm{obs}}\right)\right)\le\varepsilon$
\end_inset

,
\color red
 
\color black
for some metric 
\begin_inset Formula $\rho$
\end_inset


\color red
.
\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:Prichard-ABC-1"

\end_inset

Prichard's Modified ABC
\end_layout

\end_inset


\end_layout

\end_inset

We can view this algorithm as a modified version of accept-reject algorithm
 (
\begin_inset CommandInset citation
LatexCommand citet
key "robert2013monte"

\end_inset

).
 The posterior is sampled by altering the frequency of the proposal distribution
, that is, the prior.
 Now the full posterior distribution is approximated by the following two
 steps (
\begin_inset CommandInset citation
LatexCommand citet
key "fearnhead2012constructing"

\end_inset

): 
\begin_inset Formula 
\begin{equation}
\pi\left(\theta\mid X_{\mathrm{obs}}\right)\approx\pi\left(\theta\mid S_{\mathrm{obs}}\right)\approx\pi\left(\theta\mid S_{\mathrm{sim}}\in O\left(S_{\mathrm{obs}},\varepsilon\right)\right),\label{eq:two-step-approx-abc}
\end{equation}

\end_inset

where 
\begin_inset Formula $O\left(S_{\mathrm{obs}},\varepsilon\right)$
\end_inset

 means a neighborhood defined by the comparison measure 
\begin_inset Formula $\rho$
\end_inset

 and tolerance level 
\begin_inset Formula $\varepsilon$
\end_inset

.
 We may note that the first approximation is exact when 
\begin_inset Formula $S$
\end_inset

 is sufficient.
 Allowing the summary statistics to vary in an acceptable range sacrifices
 a little accuracy in exchange for a significant improvement in computational
 efficiency, which 
\color black
makes 
\color inherit
 the algorithm more practical and user-friendly.
 
\end_layout

\begin_layout Standard
Pursuant to Algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:Prichard-ABC-1"

\end_inset

, there are multiple generalizations in the statistical literatures.
 
\begin_inset CommandInset citation
LatexCommand citet
key "marjoram2003markov"

\end_inset

 
\color black
introduced 
\color inherit
 MCMC-ABC algorithm to concentrate the samples in high posterior probability
 region, thereby increasing the accept rate.
 Noisy ABC, proposed by 
\begin_inset CommandInset citation
LatexCommand citet
key "wilkinson2013approximate"

\end_inset

, makes use of all the prior samples by assigning kernel weights instead
 of hard-threshold accept-reject mechanism and hence reduces the computational
 burden.
 This perspective is corroborated in 
\begin_inset CommandInset citation
LatexCommand citet
key "fearnhead2012constructing"

\end_inset

 by convergence of Bayesian estimators.
 When the dependence structure between hierarchies is intractable, ABC filtering
 technique innovated by 
\begin_inset CommandInset citation
LatexCommand citet
key "jasra2012filtering"

\end_inset

 comes to the rescue.
 Later in 
\begin_inset CommandInset citation
LatexCommand citet
key "dean2014parameter"

\end_inset

, a consistency argument is established for the specific case of hidden
 Markov models.
 Moreover, many ABC algorithm above can be easily coded in a parallel way,
 and hence take advantages of modern CPU, GPU structures.
 This feature makes ABC algorithms extremely time-saving against long-establishe
d, looping-based MCMC and MLE algorithms.
 
\end_layout

\begin_layout Standard
Despite the fruitful results on ABC both from applied and theoretical points
 of view.
 However, there exist only a handful of papers which focus on the effect
 of the choice of summary statistics on the approximation quality.
 The quintessential case is the summary statistics are sufficient, and the
 resultant ABC sampler produces exact samples from the true posterior distributi
on when 
\begin_inset Formula $\varepsilon$
\end_inset

 goes to zero.
 Nevertheless, in a labyrinthine model, it is difficult to extract sufficient
 statistics, except for some very special case, such as exponential random
 graph models (e.g.
 
\begin_inset CommandInset citation
LatexCommand citet
key "grelaud2009abc"

\end_inset

).
 
\begin_inset CommandInset citation
LatexCommand citet
key "joyce2008approximately"

\end_inset

 proposed a concept called 
\begin_inset Formula $\varepsilon-$
\end_inset

sufficient to quantify the effect of statistics.
 Nonetheless, this property is also difficult to verify in complicated models.
 If we are interested only in model selection, 
\begin_inset CommandInset citation
LatexCommand citet
key "prangle2014semi"

\end_inset

 designs a semi-automatic algorithm to construct summary statistics via
 logistic regression.
 And laterly, 
\begin_inset CommandInset citation
LatexCommand citet
key "marin2014relevant"

\end_inset

 gives sufficient conditions on summary statistics in order to choose the
 right model based on the Bayes factor.
 They advocate that the ideal summary statistics are ancillary in both model
 candidates.
 One of our contribution comes from the mathematical analysis of the consequence
 of conditioning the parameters of interest on consistent statistics and
 intrinsically inconsistent statistics, and appraises the efficiency of
 the posterior approximation based on the former.
 Generally speaking, using consistent statistics results in right concentration
 of the approximate posterior, while less efficiency of statistics leads
 to less efficiency of approximation.
 One byproduct is our theorem vindicates the usage of the posterior mean
 as summary statistics as in 
\begin_inset CommandInset citation
LatexCommand citet
key "fearnhead2012constructing"

\end_inset

.
 
\end_layout

\begin_layout Standard
In addition to the pure theoretical contribution, we also extend the
\color red
 
\color black
two-step
\color red
 
\color inherit
algorithm in 
\begin_inset CommandInset citation
LatexCommand citet
key "fearnhead2012constructing"

\end_inset

 in a more flexible and nonparametric way for automatic constructing summary
 statistics.
 We borrow the idea from another thriving topic, namely sufficient dimension
 reduction (SDR).
 The motivation of SDR which generalizes the concept of sufficient statistics
 is to estimate
\color red
 
\color black
a
\color inherit
 transformation 
\begin_inset Formula $\varphi$
\end_inset

, either
\color red
 
\color inherit
linear or nonlinear, such that 
\begin_inset Formula 
\begin{equation}
Y\independent X\mid\varphi\left(X\right).\label{eq:sdr}
\end{equation}

\end_inset

 The first SDR method titled sliced inverse regression dates back to 
\begin_inset CommandInset citation
LatexCommand citet
key "li1991sliced"

\end_inset

, followed by principle Hessian direction in 
\begin_inset CommandInset citation
LatexCommand citet
key "li1992principal"

\end_inset

 and also by 
\begin_inset CommandInset citation
LatexCommand citet
key "1991"

\end_inset

, and 
\begin_inset CommandInset citation
LatexCommand citet
key "cook1998principal"

\end_inset

.
 As we step in the era of big data, this idea leads to a sea of papers on
 both linear and nonlinear, predictor and response.
 Among the more recent work, we refer to 
\begin_inset CommandInset citation
LatexCommand citet
key "cook2002dimension"

\end_inset

, 
\begin_inset CommandInset citation
LatexCommand citet
key "xia2002adaptive"

\end_inset

, 
\begin_inset CommandInset citation
LatexCommand citet
key "li2005contour"

\end_inset

, 
\begin_inset CommandInset citation
LatexCommand citet
key "li2009dimension"

\end_inset

, 
\begin_inset CommandInset citation
LatexCommand citet
key "wu2008kernel"

\end_inset

, 
\begin_inset CommandInset citation
LatexCommand citet
key "yeh2009nonlinear"

\end_inset

, 
\begin_inset CommandInset citation
LatexCommand citet
key "su2011partial"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand citet
key "su2012inner"

\end_inset

.
 The association between SDR and ABC relies on the shared mathematical formulati
on.
 If we think 
\begin_inset Formula $\theta$
\end_inset

 as the response and 
\begin_inset Formula $X$
\end_inset

 as the predictor, then an ideal summary statistics 
\begin_inset Formula $S\left(X\right)$
\end_inset

 will give 
\begin_inset Formula 
\[
\theta\independent X\mid S\left(X\right).
\]

\end_inset

This simple observation offers raison d'etre to use existing SDR methods
 in constructing summary statistics.
 The employment of dimension reduction methods in our algorithm is different
 from that in 
\begin_inset CommandInset citation
LatexCommand citet
key "blum2013comparative"

\end_inset

.
 In 
\begin_inset CommandInset citation
LatexCommand citet
key "blum2013comparative"

\end_inset

, dimension reduction methods, such as best subset selection, projection
 techniques and regularization approaches, are applied to reduce the dimension
 of existing summary statistics, but here, we try to reduce the size of
 the original data.
 Particularly in our paper, we incorporate the principal  support vector
 machine for nonlinear dimension reduction given in 
\begin_inset CommandInset citation
LatexCommand citet
key "li2011principal"

\end_inset

 into ABC, which uses the
\color red
 
\color black
principal 
\color inherit
 component of support vectors in reproducing kernel Hilbert space (RKHS)
 as a nonparametric estimator of 
\begin_inset Formula $\varphi$
\end_inset

.
 
\end_layout

\begin_layout Standard
The outline of remaining sections is as follows.
 Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:asymp-partial-post"

\end_inset

 contains asymptotic results on the partial posterior.
 We gradually relax the restriction on summary statistics and investigate
 the relationship between the partial posterior and the full posterior.
 As a side result, we give a lemma building a bridge between the recent
 prior free inferential model (
\begin_inset CommandInset citation
LatexCommand citet
key "martin2013inferential"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand citet
key "martin2015conditional"

\end_inset

) and traditional Bayesian inference.
 Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:abc-sdr"

\end_inset

 elicits a new ABC algorithm which automatically produces summary statistics
 through nonlinear SDR.
 A simulation result is provided in this section
\color red
 
\color black
as well
\color inherit
.
 Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Discussion"

\end_inset

 briefly discusses the results and points out some possible future generalizatio
ns.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:asymp-partial-post"

\end_inset

Asymptotic Properties of Partial Posterior
\end_layout

\begin_layout Standard
Suppose 
\begin_inset Formula $X_{1},\ldots,X_{n}\mid\theta$
\end_inset

 are i.i.d.
 with common PDF 
\begin_inset Formula $f\left(x\mid\theta\right)$
\end_inset

, and there exists a true but unknown value 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 Without loss of generality, we assume 
\begin_inset Formula $\theta\in\mathbb{R}$
\end_inset

, and all probability density functions are with respect to the Lebesgue
 measure.
 For illustration purpose, we define the following terminology.
\end_layout

\begin_layout Definition
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Partial Posterior
\end_layout

\end_inset

Let 
\begin_inset Formula $S=S\left(X_{1},\ldots,X_{n}\right)$
\end_inset

 be
\color red
 
\color black
statistics 
\color red
 
\color inherit
of the data.
 Given a prior 
\begin_inset Formula $\pi\left(\theta\right)$
\end_inset

, we call the distribution 
\begin_inset Formula 
\[
\pi\left(\theta\mid S\right)\propto\pi\left(\theta\right)g\left(S\mid\theta\right)
\]

\end_inset

the partial posterior, where 
\begin_inset Formula $g\left(S\mid\theta\right)$
\end_inset

 is the probability density function of statistic 
\begin_inset Formula $S\left(X_{1},\ldots,X_{n}\right)$
\end_inset

 derived from the data density, and correspondingly, 
\begin_inset Formula 
\[
\pi\left(\theta\mid X_{1},\ldots,X_{n}\right)\propto\pi\left(\theta\right)f\left(X_{1},\ldots,X_{n}\mid\theta\right)
\]

\end_inset

 is called the full posterior.
\end_layout

\begin_layout Standard
From equation 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:two-step-approx-abc"

\end_inset

, partial posterior significantly reduces the complexity of full posterior
 by replacing the dependence on full data by lower dimensional statistics
 
\begin_inset Formula $S$
\end_inset

.
 If the partial posterior deviates from 
\color black
the 
\color red
 
\color inherit
full posterior too much, then no matter how delicately we sample from 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\pi\left(\theta\mid S_{\mathrm{sim}}\in O\left(S_{\mathrm{obs}},\varepsilon\right)\right)$
\end_inset

 , how small 
\begin_inset Formula $\varepsilon$
\end_inset

 we choose, the resultant samples would not behave like ones drawn from
 the original full posterior, which
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color red
 
\color black
makes 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 the subsequent Bayesian analysis fragile and unreliable.
 Therefore, theoretical connection between some easily verifiable properties
 and asymptotic behaviour of partial posterior is of relevance.
 In particular, we want to study consistency and asymptotic normality of
 our Bayesian procedures.
 The following theorems try to demonstrate the connection between the asymptotic
 behaviour of summary statistics and that of partial posterior.
 We start from the most popular statistics, the maximum likelihood estimator
 (MLE) of 
\begin_inset Formula $\theta$
\end_inset

.
 
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:bernstein-von-mises-mle"

\end_inset

Let 
\begin_inset Formula $\hat{\theta}$
\end_inset

, the MLE of 
\begin_inset Formula $\theta$
\end_inset

, be a strongly consistent estimator, and let 
\begin_inset Formula $\hat{I}$
\end_inset

 be the observed Fisher information evaluated at 
\begin_inset Formula $\hat{\theta}$
\end_inset

, and the full posterior holds Bernstein--von Mises theorem.
 Then for any 
\begin_inset Formula $\varepsilon>0$
\end_inset

, and any 
\begin_inset Formula $t$
\end_inset

, the partial posterior after conditioned on 
\begin_inset Formula $\hat{\theta}$
\end_inset

 satisfies 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}P\left(\sqrt{n\hat{I}}\left(\theta-\hat{\theta}\right)\le t\mid\hat{\theta}\in O\left(\theta_{0},\varepsilon\right)\right)=\Phi\left(t\right),\ascv.
\]

\end_inset


\end_layout

\begin_layout Proof
See  
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:proof-thm-1"

\end_inset

.
\end_layout

\begin_layout Remark
There is a slight difference between 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}P\left(\sqrt{n\hat{I}}\left(\theta-\hat{\theta}\right)\le t\mid\hat{\theta}\in O\left(\theta_{0},\varepsilon\right)\right)=\Phi\left(t\right),\ascv.\left(P_{\theta_{0}}\right)
\]

\end_inset

and 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}P\left(\sqrt{n\hat{I}}\left(\theta-\hat{\theta}\right)\le t\mid\hat{\theta}\right)=\Phi\left(t\right),\ascv.
\]

\end_inset

By definition, 
\begin_inset Formula 
\begin{equation}
P\left(\sqrt{n\hat{I}}\left(\theta-\hat{\theta}\right)\le t\mid\hat{\theta}\right)=\lim_{\varepsilon\rightarrow0}\frac{P\left(\sqrt{n\hat{I}}\left(\theta-\hat{\theta}\right)\le t,\hat{\theta}\in O\left(s,\varepsilon\right)\right)}{P\left(\hat{\theta}\in O\left(s,\varepsilon\right)\right)}.\label{eq:def-loose-partial-post}
\end{equation}

\end_inset

The result of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:bernstein-von-mises-mle"

\end_inset

 can only be used to prove 
\begin_inset Formula 
\[
\lim_{\varepsilon\rightarrow0}\lim_{n\rightarrow\infty}P\left(\sqrt{n\hat{I}}\left(\theta-\hat{\theta}\right)\le t\mid\hat{\theta}\in O\left(\theta_{0},\varepsilon\right)\right)=\Phi\left(t\right),\ascv,
\]

\end_inset

switching order of limits in equation 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:def-loose-partial-post"

\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Remark
The definition of 
\begin_inset Formula $P\left(\theta\mid\hat{\theta}\in O\left(\theta_{0},\varepsilon\right)\right)$
\end_inset

 is different from the approximation 
\begin_inset Formula $P\left(\theta\mid\hat{\theta}\in O\left(\hat{\theta}_{\mathrm{obs}},\varepsilon\right)\right)$
\end_inset

.
 In former case, 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is evaluated at 
\begin_inset Formula $X_{1},\ldots,X_{n}\sim\pi\left(x\mid\theta_{0}\right)$
\end_inset

, the observed data, while the latter evaluates 
\begin_inset Formula $\hat{\theta}$
\end_inset

 at 
\begin_inset Formula $Z_{1},\ldots,Z_{m}\sim\pi\left(z\mid\theta\right)$
\end_inset

, the simulated data.
 
\end_layout

\end_deeper
\begin_layout Standard
By
\color red
 
\color black
assumptions 
\color inherit
, the asymptotic distribution of the full posterior is still normal, and
 we have 
\begin_inset Formula 
\begin{eqnarray*}
 &  & \sup_{t\in\mathbb{R}}\left|P\left(\sqrt{n\hat{I}}\left(\theta-\hat{\theta}\right)\le t\mid\hat{\theta}\in O\left(\theta_{0},\varepsilon\right)\right)-P\left(\sqrt{n\hat{I}}\left(\theta-\hat{\theta}\right)\le t\mid X_{1},\ldots,X_{n}\right)\right|\\
 & \le & \sup_{t\in\mathbb{R}}\left|P\left(\sqrt{n\hat{I}}\left(\theta-\hat{\theta}\right)\le t\mid\hat{\theta}\in O\left(\theta_{0},\varepsilon\right)\right)-\Phi\left(t\right)\right|+\sup_{s\in\mathbb{R}}\left|P\left(\sqrt{n\hat{I}}\left(\theta-\hat{\theta}\right)\le s\mid X_{1},\ldots,X_{n}\right)-\Phi\left(s\right)\right|\\
 & \le & 2\varepsilon\rightarrow0,\:\left(\mathrm{as}\: n\rightarrow\infty\right).
\end{eqnarray*}

\end_inset

Hence, we can informally say that two random variables 
\begin_inset Formula $\sqrt{n\hat{I}}\left(\theta-\hat{\theta}\right)\mid\hat{\theta}$
\end_inset

 and 
\begin_inset Formula $\sqrt{n\hat{I}}\left(\theta-\hat{\theta}\right)\mid X_{1},\ldots,X_{n}$
\end_inset

 are close in distribution.
 Note that both random variables asymptotically center at consistent MLE,
 and hence will eventually concentrate at 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 Meanwhile, the scale factors in both random variables are 
\begin_inset Formula $\sqrt{n\hat{I}}$
\end_inset

, which
\color red
 
\color black
ensures 
\color red
 
\color inherit
the same square root credible 
\color black
intervals
\color inherit
.
 In this sense, we feel that the partial posterior conditioned on MLE has
 the same efficiency as the full posterior.
 Later theorems will tell us that if the summary statistics are not efficient,
 the corresponding partial likelihood will have a different scale factor,
 and thus lose efficiency and result in a larger credible interval.
 
\end_layout

\begin_layout Standard
A slightly modified proof of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:bernstein-von-mises-mle"

\end_inset

 can be used to support the posterior mean as a summary statistic in 
\begin_inset CommandInset citation
LatexCommand citet
key "fearnhead2012constructing"

\end_inset

 and we still have a similar result, namely 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}P\left(\sqrt{n\hat{I}}\left(\theta-E\left(\theta\mid X_{1},\ldots,X_{n}\right)\right)\le t\mid E\left(\theta\mid X_{1},\ldots,X_{n}\right)\in O\left(\theta_{0},\varepsilon\right)\right)=\Phi\left(t\right),\ascv.
\]

\end_inset

The key fact to support the assert above comes from 
\begin_inset CommandInset citation
LatexCommand citet
key "ghosh2011moment"

\end_inset

, that is, the higher order closeness of the posterior mean and the MLE,
 
\begin_inset Formula 
\begin{equation}
\lim_{n\rightarrow\infty}\sqrt{n}\left(E\left(\theta\mid X_{1},\ldots,X_{n}\right)-\hat{\theta}\right)=0,\ascv.\label{eq:high-order-close-post-mean-mle}
\end{equation}

\end_inset

Indeed, any estimator who has the same or higher order of closeness to MLE
 will work as an efficient summary statistic.
\end_layout

\begin_layout Standard
Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:bernstein-von-mises-mle"

\end_inset

 can be generalized to more intricate models.
 The following example shows the same phenomenon in data generated from
 a Markov process.
\end_layout

\begin_layout Example
\begin_inset CommandInset label
LatexCommand label
name "exa:Immigrate-emigrate-process"

\end_inset

Immigration-emigration process is
\color red
 
\color black
a 
\color red
 
\color inherit
crucial model in survival analysis and can be viewed as a special case of
 mass-action stochastic kinetic
\color red
 
\color black
network 
\color inherit
 (
\begin_inset CommandInset citation
LatexCommand citet
key "wilkinson2011stochastic"

\end_inset

).
 The model is defined by a birth procedure and
\color red
 
\color black
a 
\color red
 
\color inherit
death procedure during an infinitesimal time interval, namely, 
\begin_inset Formula 
\[
P\left(X\left(t+\diff t\right)=x_{1}\mid X\left(t\right)=x_{0}\right)=\begin{cases}
\lambda\diff t+o\left(\diff t\right), & x_{1}=x_{0}+1,\\
\mu x_{0}\diff t+o\left(\diff t\right), & x_{1}=x_{0}-1,\\
1-\lambda\diff t-\mu x_{0}\diff t+o\left(\diff t\right), & x_{1}=x_{0}.
\end{cases}
\]

\end_inset

Assume that we observe full data in the time interval 
\begin_inset Formula $\left[0,T\right]$
\end_inset

.
 Let 
\begin_inset Formula $T_{i},i=1,\ldots,n$
\end_inset

 be the event times and 
\begin_inset Formula $X_{i}=X\left(T_{i}\right),i=1,\ldots,n$
\end_inset

.
 Let 
\begin_inset Formula $X_{0}$
\end_inset

 be initial population, 
\begin_inset Formula $T_{0}=0$
\end_inset

, 
\begin_inset Formula $T_{n+1}=T$
\end_inset

.
 Then by Gillespie's algorithm, the likelihood is proportional to 
\begin_inset Formula 
\[
\lambda^{r_{1}}\exp\left(-\lambda T\right)\mu^{r_{2}}\exp\left(-\mu A_{T}\right),
\]

\end_inset

where 
\begin_inset Formula $r_{1}$
\end_inset

 and 
\begin_inset Formula $r_{2}$
\end_inset

 are number of events corresponding to immigration and emigration, and 
\begin_inset Formula 
\[
A_{T}=\int_{0}^{T}X\left(t\right)\diff t.
\]

\end_inset

The MLEs are 
\begin_inset Formula 
\[
\hat{\lambda}=\frac{r_{1}}{T},\hat{\mu}=\frac{r_{2}}{A_{T}},
\]

\end_inset

and they are strongly consistent estimators of 
\begin_inset Formula $\lambda$
\end_inset


\color red
 
\color black
and 
\begin_inset Formula $\mu$
\end_inset


\color red
 
\color inherit
when 
\begin_inset Formula $T$
\end_inset

 goes to infinity.
 By the computation in  
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Derivation-of-Example-1"

\end_inset

, we have the partial posterior density function of 
\begin_inset Formula $\sqrt{T}\left(\mu-\hat{\mu}\right)$
\end_inset

 conditioned on 
\begin_inset Formula $\hat{\mu}$
\end_inset

, 
\begin_inset Formula $r_{1}$
\end_inset

 and 
\begin_inset Formula $T$
\end_inset

 given by 
\begin_inset Formula 
\[
\lim_{T\rightarrow\infty}\pi\left(\sqrt{T}\left(\mu-\hat{\mu}\right)=t\mid\hat{\mu},r_{1},T\right)=\frac{\hat{\mu}}{\sqrt{2\pi\hat{\lambda}}}\exp\left(-\frac{\hat{\lambda}}{\hat{\mu}^{2}}t^{2}\right),\ascv.
\]

\end_inset


\end_layout

\begin_layout Standard
The MLE seems to be a perfect surrogate for 
\color black
the 
\color red
 
\color inherit
full data.
 However, in many cases, use of MLE is prohibitive due to heavy computational
 burden, particularly when the likelihood function is intractable.
 This is when the ABC comes on stage.
 
\begin_inset Formula $M$
\end_inset

-estimator is a generalization of the MLE, which is also consistent and
 asymptotically normal under mild conditions.
 Many 
\begin_inset Formula $M$
\end_inset

-estimators can be easily calculated, especially some moment estimators.
 To give an idea of the nature of approximation, we consider the following
 examples.
\end_layout

\begin_layout Example
\begin_inset CommandInset label
LatexCommand label
name "exa:Gamma-distribution"

\end_inset

Gamma distribution can be used to model hazard functions in survival analysis.
 The shape parameter of gamma distribution determines the trend of hazard
 and hence is a vital parameter to estimate.
 Assume 
\begin_inset Formula $X_{1},\ldots,X_{n}\sim\mathrm{Gamma}\left(\alpha,\beta\right)$
\end_inset

, where we know the scale parameter 
\begin_inset Formula $\beta$
\end_inset

, but not the shape parameter 
\begin_inset Formula $\alpha$
\end_inset

.
 The MLE of 
\begin_inset Formula $\alpha$
\end_inset

 is the solution of 
\begin_inset Formula 
\[
-\log\Gamma\left(\alpha\right)-\alpha\log\beta+\left(\alpha-1\right)\sum_{i=1}^{n}\log X_{i}-\frac{\sum_{i=1}^{n}X_{i}}{\beta}=0,
\]

\end_inset

which involves repeated evaluation of the gamma function in search of the
 root.
 A simple 
\begin_inset Formula $M$
\end_inset

-estimator 
\begin_inset Formula $\tilde{\alpha}=\overline{X}/\beta$
\end_inset

 is derived from its mean equation, 
\begin_inset Formula 
\[
\sum_{i=1}^{n}\left(X_{i}-\alpha\beta\right)=0.
\]

\end_inset

 Now we consider the partial posterior 
\begin_inset Formula $\pi\left(\alpha\mid\tilde{\alpha}\right)$
\end_inset

, when the prior is 
\begin_inset Formula $\pi\left(\alpha\right)\propto\exp\left(-\lambda\alpha\right)$
\end_inset

.
 By the calculation in  
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Derivation-of-Example-2"

\end_inset

, we show that the limit of cumulative probability function of 
\begin_inset Formula $\sqrt{n}\tilde{\alpha}^{-1}\left(\alpha-\tilde{\alpha}\right)$
\end_inset

 given 
\begin_inset Formula $\tilde{\alpha}$
\end_inset

 is 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}P\left(\sqrt{n}\tilde{\alpha}^{-1}\left(\alpha-\tilde{\alpha}\right)\le t\mid\tilde{\alpha}\right)=\Phi\left(t\right),\ascv,
\]

\end_inset

which means that the Bernstein-von Mises theorem holds for the partial posterior
 conditioned on the 
\begin_inset Formula $M$
\end_inset

-estimator 
\begin_inset Formula $\tilde{\alpha}$
\end_inset

.
 The scale factor of 
\color black
 the
\color inherit
 partial posterior is 
\begin_inset Formula $\sqrt{n}\tilde{\alpha}^{-1}$
\end_inset

, which is smaller than that of
\color red
 
\color black
the 
\color red
 
\color inherit
full posterior, 
\begin_inset Formula $\sqrt{n\psi'\left(\alpha\right)}$
\end_inset

, where 
\begin_inset Formula $\psi\left(\alpha\right)$
\end_inset

 is digamma function.
 That results in a larger credible interval based on the partial posterior.
 
\end_layout

\begin_deeper
\begin_layout Example
\begin_inset CommandInset label
LatexCommand label
name "exa:laplace-example"

\end_inset

Another example is Laplace distribution with PDF 
\begin_inset Formula 
\[
f_{\mu,\lambda}\left(t\right)=\frac{1}{2\lambda}\exp\left(-\frac{\left|t-\mu\right|}{\lambda}\right).
\]

\end_inset

 Here we want inference the location parameter 
\begin_inset Formula $\mu$
\end_inset

 holding 
\begin_inset Formula $\lambda$
\end_inset

 fixed.
 The MLE is the
\color red
 
\color black
the 
\color inherit
 sample median and the moment estimator is
\color red
 
\color black
the 
\color inherit
sample mean.
 Here we calculate the partial posterior based on sample mean.
 By the calculation in  
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Derivation-of-Example-3"

\end_inset

, we find that the characteristic function of 
\begin_inset Formula $\sqrt{n}\left(\mu-\overline{X}\right)$
\end_inset

 converges to 
\begin_inset Formula $\exp\left(-\lambda^{2}t^{2}\right)$
\end_inset

, which is the characteristic function of normal distribution.
 
\end_layout

\end_deeper
\begin_layout Standard
Example 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:laplace-example"

\end_inset

 uses the following lemma which is of independent interest.
 
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:bayes-inferential-model"

\end_inset

Assume 
\begin_inset Formula $X$
\end_inset

 has the same distribution as 
\begin_inset Formula $h\left(Y,\theta\right)$
\end_inset

, where 
\begin_inset Formula $h$
\end_inset

 is a function one-to-one in 
\begin_inset Formula $\theta$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 is a random variable independent of 
\begin_inset Formula $\theta$
\end_inset

.
 Let 
\begin_inset Formula $\theta=g\left(y,x\right)$
\end_inset

 and 
\begin_inset Formula $y=u\left(x,\theta\right)$
\end_inset

 be the solutions of equation 
\begin_inset Formula $x=h\left(y,\theta\right)$
\end_inset

.
 Further assume 
\begin_inset Formula $\partial u\left(x,\theta\right)/\partial x$
\end_inset

 exists and is not equal to zero.
 Then the posterior distribution of 
\begin_inset Formula $\theta$
\end_inset

 conditioned on 
\begin_inset Formula $X$
\end_inset

 under the uniform prior has the same distribution as 
\begin_inset Formula $g\left(Y,x\right)$
\end_inset

, where 
\begin_inset Formula $x$
\end_inset

 is fixed.
 
\end_layout

\begin_layout Remark
Although not quite related to ABC, this lemma gives another interpretation
 of inferential model of 
\begin_inset CommandInset citation
LatexCommand citet
key "martin2013inferential"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand citet
key "martin2015conditional"

\end_inset

.
 In their settings, 
\begin_inset Formula $Y$
\end_inset

 is called unobserved ancillary variable, and 
\begin_inset Formula $g\left(y,x\right)$
\end_inset

 is 
\begin_inset Formula $\Theta_{x}\left(u\right)$
\end_inset

 in their notation.
 They claim that their procedure results in a distribution of 
\begin_inset Formula $\theta$
\end_inset

 without referring to a prior.
 However, by our lemma, this model is mathematically the same as a posterior
 given a uniform prior.
\end_layout

\begin_layout Standard
The following theorems are built upon the Theorem 2.1 in 
\begin_inset CommandInset citation
LatexCommand citet
key "rivoirard2012bernstein"

\end_inset

, which guarantees the asymptotic normality of linear functionals of nonparametr
ic posterior.
 So we need all the assumptions in that theorem.
 Additionally, we need the following assumptions.
 
\end_layout

\begin_layout Assumption
\begin_inset CommandInset label
LatexCommand label
name "assu:second-order-bounded-differential"

\end_inset

There is a neighbourhood 
\begin_inset Formula $\theta\in O\left(\theta_{0},\varepsilon\right)$
\end_inset

 such that 
\begin_inset Formula $\int_{\mathbb{R}}g\left(x,\theta_{0}\right)\pi\left(x\mid\theta\right)\diff x$
\end_inset

 is a continuous twice differentiable in 
\begin_inset Formula $\theta$
\end_inset

 and the second order derivative is bounded by some constant 
\begin_inset Formula $L$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Assumption
\begin_inset CommandInset label
LatexCommand label
name "assu:m-est-consistent-asymp-norml"

\end_inset


\begin_inset Formula $M$
\end_inset

-estimator 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 and MLE 
\begin_inset Formula $\hat{\theta}$
\end_inset

 are both strongly consistent and asymptotically normal.
\end_layout

\begin_deeper
\begin_layout Assumption
\begin_inset CommandInset label
LatexCommand label
name "assu:bernstein-von-mises-full-posterior"

\end_inset

 Bernstein--von Mises theorem and posterior consistency hold for the full
 posterior of 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Assumption
\begin_inset CommandInset label
LatexCommand label
name "assu:theo-mle"

\end_inset

For any 
\begin_inset Formula $\theta\in\Theta$
\end_inset

, 
\begin_inset Formula $E_{\theta_{0}}\log f\left(X\mid\theta\right)\le E_{\theta_{0}}\log f\left(X\mid\theta_{0}\right)$
\end_inset

.
 
\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Standard
Now we can articulate the theorem.
 
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:partial-post-m-est"

\end_inset

Under the Assumptions 
\begin_inset CommandInset ref
LatexCommand ref
reference "assu:second-order-bounded-differential"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "assu:m-est-consistent-asymp-norml"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "assu:bernstein-von-mises-full-posterior"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "assu:theo-mle"

\end_inset

, and conditions of Theorem 2.1 in 
\begin_inset CommandInset citation
LatexCommand citet
key "rivoirard2012bernstein"

\end_inset

, for any 
\begin_inset Formula $\varepsilon$
\end_inset

 and 
\begin_inset Formula $t$
\end_inset

, 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}P\left(\left.\frac{\sqrt{n}\left(\theta-\tilde{\theta}\right)}{\sqrt{\tilde{V}}}\le t\right|\tilde{\theta}\in O\left(\theta_{0},\varepsilon\right)\right)=\Phi\left(t\right),\ascv,
\]

\end_inset

where 
\begin_inset Formula $\tilde{V}=V_{0}/G_{1}\left(\tilde{\theta},\tilde{\theta}\right)^{2}$
\end_inset

 is Godambe information.
 
\end_layout

\begin_layout Proof
See  
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Proof-of-Theorem-2"

\end_inset

.
\end_layout

\begin_layout Standard
Using similar arguments as Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:bernstein-von-mises-mle"

\end_inset

, the partial posterior 
\begin_inset Formula $\sqrt{n\tilde{V}^{-1}}\left(\theta-\tilde{\theta}\right)\mid\tilde{\theta}$
\end_inset


\color red
 
\color black
is asymptotically 
\color inherit
 close in distribution to 
\color black
the 
\color inherit
full posterior 
\begin_inset Formula $\sqrt{n\hat{I}}\left(\theta-\hat{\theta}\right)\mid X_{1},\ldots,X_{n}$
\end_inset

.
 Since both the 
\begin_inset Formula $M$
\end_inset

-estimator and the MLE are strongly consistent, the partial posterior still
 concentrates around the right 
\begin_inset Formula $\theta_{0}$
\end_inset

, but now the asymptotic 
\begin_inset Formula $\alpha-$
\end_inset

level credible interval based on the partial posterior, namely 
\begin_inset Formula 
\[
\left(\tilde{\theta}-Z_{\alpha/2}\sqrt{\frac{\tilde{V}}{n}},\tilde{\theta}+Z_{\alpha/2}\sqrt{\frac{\tilde{V}}{n}}\right),
\]

\end_inset

will be larger than that based on the full posterior, 
\begin_inset Formula 
\[
\left(\hat{\theta}-Z_{\alpha/2}\sqrt{\frac{\hat{I}^{-1}}{n}},\hat{\theta}+Z_{\alpha/2}\sqrt{\frac{\hat{I}^{-1}}{n}}\right),
\]

\end_inset

where 
\begin_inset Formula $Z_{\alpha/2}$
\end_inset

 is 
\begin_inset Formula $\left(1-\alpha/2\right)$
\end_inset

 quantile of standard normal distribution, because the Godambe information
 
\begin_inset Formula $\tilde{V}^{-1}$
\end_inset

 is typically no larger than Fisher information 
\begin_inset Formula $\hat{I}$
\end_inset

.
 Hence, we lose efficiency if we condition the posterior on an inefficient
 estimator, which coincides our intuition.
\end_layout

\begin_layout Standard
For extreme tortuous models, even finding a consistent estimator can be
 quite hard.
 There are still some simple statistics which may be consistent to some
\color red
 
\color black
functions 
\color inherit
 of 
\begin_inset Formula $\theta$
\end_inset

.
 Unless they are ancillary statistics, they always contain some information
 about the
\color red
 
\color black
parameters 
\color inherit
 of interest.
 Moreover, in real case, we use several statistics, each of which gives
 independent information of the full posterior.
 In the remainder of this section, we will mathematically qualify what independe
nt information means and show that using more than one statistic will improve
 the efficiency.
 
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $S_{i}$
\end_inset

, 
\begin_inset Formula $i=1,\ldots,q$
\end_inset

 be statistics of the sample.
 We make the following trivial assumptions.
 
\end_layout

\begin_layout Assumption
\begin_inset CommandInset label
LatexCommand label
name "assu:joint-normal-inconsist-stat"

\end_inset

The joint distribution of 
\begin_inset Formula $S_{1},\ldots,S_{q}$
\end_inset

 converges in distribution to a multivariate normal distribution 
\begin_inset Formula $N\left(h\left(\theta_{0}\right),n^{-1/2}\Sigma\left(\theta_{0}\right)\right)$
\end_inset

.
 And each 
\begin_inset Formula $S_{i}$
\end_inset

 converges to 
\begin_inset Formula $h_{i}\left(\theta_{0}\right)$
\end_inset

 almost surely.
 Further, assume 
\begin_inset Formula $\Sigma\left(\theta_{0}\right)$
\end_inset


\color red
 
\color black
is 
\color inherit
 positive definite.
 
\end_layout

\begin_layout Standard
Assumption 
\begin_inset CommandInset ref
LatexCommand ref
reference "assu:joint-normal-inconsist-stat"

\end_inset

 characterizes the statement independent information.
 Because if 
\begin_inset Formula $\Sigma\left(\theta_{0}\right)$
\end_inset

 has a lower rank, then some of 
\begin_inset Formula $S_{i}$
\end_inset

 can be expressed as 
\color black
a 
\color inherit
linear combination of
\color red
 
\color black
other 
\color inherit
 
\begin_inset Formula $S_{j}$
\end_inset

 asymptotically.
 Then the partial posterior can be reduced to partial posterior based solely
 on 
\begin_inset Formula $S_{j}$
\end_inset

.
\end_layout

\begin_layout Standard
In order to prove the theorem, we need some more technical assumptions.
\end_layout

\begin_layout Assumption
\begin_inset CommandInset label
LatexCommand label
name "assu:consist-linear-functional"

\end_inset

Let 
\begin_inset Formula $h\left(\theta_{0}\right)$
\end_inset

 be a linear functional of the distribution function, that is 
\begin_inset Formula 
\[
h\left(\theta_{0}\right)=\int_{\mathbb{R}}g\left(x\right)f\left(x\mid\theta_{0}\right)\diff x,
\]

\end_inset

where 
\begin_inset Formula $g\left(x\right)\in\mathbb{R}^{q}$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Assumption
\begin_inset CommandInset label
LatexCommand label
name "assu:super-strong-consistent"

\end_inset

Let 
\begin_inset Formula $S=\left(S_{1},\ldots,S_{q}\right)$
\end_inset

, assume 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\sqrt{n}\left(\frac{1}{n}\sum_{i=1}^{n}g\left(X_{i}\right)-S\right)=0,\ascv.
\]

\end_inset

and there exists a strongly consistent estimator 
\begin_inset Formula $\tilde{\Sigma}$
\end_inset

 of 
\begin_inset Formula $\Sigma\left(\theta_{0}\right)$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
Assumption 
\begin_inset CommandInset ref
LatexCommand ref
reference "assu:consist-linear-functional"

\end_inset

 is a natural consequence when we apply some version of strong law of large
 numbers to prove convergence of statistics.
 Only Assumption 
\begin_inset CommandInset ref
LatexCommand ref
reference "assu:super-strong-consistent"

\end_inset

 seems quite restrictive.
 Based on all these assumptions, the theorem describing the partial posterior
 conditioned on less informative statistics can be found as following.
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:bernsten-von-mise-inconsist-multv"

\end_inset

Under the Assumptions 
\begin_inset CommandInset ref
LatexCommand ref
reference "assu:joint-normal-inconsist-stat"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "assu:consist-linear-functional"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "assu:super-strong-consistent"

\end_inset

 and conditions of Theorem 2.1 in 
\begin_inset CommandInset citation
LatexCommand citet
key "rivoirard2012bernstein"

\end_inset

, for any vector 
\begin_inset Formula $a\in\mathbb{R}^{q}$
\end_inset

, 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\sup_{t\in\mathbb{R}}\left|P\left(\left.\frac{\sqrt{n}a^{T}\left(h\left(\theta\right)-S\right)}{\sqrt{a^{T}\tilde{\Sigma}a}}\le t\right|S\right)-\Phi\left(t\right)\right|=0,\ascv.
\]

\end_inset


\end_layout

\begin_layout Proof
See 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Proof-of-Theorem-3"

\end_inset

.
\end_layout

\begin_layout Standard
Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:bernsten-von-mise-inconsist-multv"

\end_inset

 extends the asymptotic results about 
\begin_inset Formula $M$
\end_inset

-estimators to more general statistics, particularly the intrinsically inconsist
ent statistics defined as follows.
\end_layout

\begin_layout Definition
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Intrinsic Consistency
\end_layout

\end_inset

Let 
\begin_inset Formula $S$
\end_inset

 be an non-ancillary statistic and converges to 
\begin_inset Formula $h\left(\theta_{0}\right)$
\end_inset

 almost surely.
 If 
\begin_inset Formula $h\left(\cdot\right)$
\end_inset

 is an one-to-one function and has an inverse function, then we say 
\begin_inset Formula $S$
\end_inset

 is intrinsically consistent.
 Otherwise, we say 
\begin_inset Formula $S$
\end_inset

 is intrinsically inconsistent.
 
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $S$
\end_inset

 is a one dimensional intrinsic inconsistent statistic, then Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:bernsten-von-mise-inconsist-multv"

\end_inset

 asserts the 
\begin_inset Formula $\left(1-\alpha\right)$
\end_inset

 asymptotic credible set based on
\color red
 
\color black
the 
\color inherit
 partial posterior is 
\begin_inset Formula 
\[
\left\{ \theta:S-Z_{\alpha/2}\sqrt{\frac{\tilde{\Sigma}}{n}}\le h\left(\theta\right)\le S+Z_{\alpha/2}\sqrt{\frac{\tilde{\Sigma}}{n}}\right\} .
\]

\end_inset

In an extreme case, when sample size 
\begin_inset Formula $n$
\end_inset

 is large enough, such that 
\begin_inset Formula $Z_{\alpha/2}/\sqrt{n}\approx0$
\end_inset

, the asymptotic credible interval by
\color red
 
\color black
the 
\color inherit
 full posterior would be close to the singleton 
\begin_inset Formula $\left\{ \hat{\theta}\right\} $
\end_inset

.
 However, the credible set based on
\color red
 
\color black
the 
\color inherit
 partial posterior would be 
\begin_inset Formula $\left\{ \theta:h\left(\theta\right)=S\right\} $
\end_inset

.
 By the definition of intrinsic inconsistency, 
\begin_inset Formula $h$
\end_inset

 is not a one-to-one function.
 Then the set 
\begin_inset Formula $\left\{ \theta:h\left(\theta\right)=S\right\} $
\end_inset

 would possibly hold multiple elements, hence larger than that from
\color red
 
\color black
the 
\color inherit
 full posterior.
 Again, in this sense, we perceive loss of efficiency due to conditioning
 the posterior on arbitrary statistics.
 
\end_layout

\begin_layout Standard
Another interesting use of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:bernsten-von-mise-inconsist-multv"

\end_inset

 is a more pragmatic asymptotic assessment of effectiveness of including
\color red
 
\color black
many
\color inherit
 statistics than that in 
\begin_inset CommandInset citation
LatexCommand citet
key "joyce2008approximately"

\end_inset

.
 In their settings, the effectiveness of summary statistics is measured
 by the difference between log-likelihoods, thus not operable when likelihood
 
\color black
 functions are
\color inherit
 intractable.
 On the other hand, our approach only
\color red
 
\color black
requires 
\color inherit
the asymptotic behaviour of statistics, and the corresponding credible set
 with 
\begin_inset Formula $q$
\end_inset

 statistics can be developed by Cremer
\begin_inset Note Comment
status open

\begin_layout Plain Layout
ask Ghosh
\end_layout

\end_inset

 device as 
\begin_inset Formula 
\[
\left\{ \theta:n\left(h\left(\theta\right)-S\right)^{T}\tilde{\Sigma}\left(h\left(\theta\right)-S\right)\le\chi_{1-\alpha,q}^{2}\right\} ,
\]

\end_inset

where 
\begin_inset Formula $\chi_{1-\alpha,q}^{2}$
\end_inset

 is 
\begin_inset Formula $\left(1-\alpha\right)$
\end_inset

 quantile of chi-square distribution with degree of freedom 
\begin_inset Formula $q$
\end_inset

.
 To select summary statistics, we can compare the asymptotic credible sets
 with and without the current statistic.
 If the difference is small, then we can safely throw the current statistic
 away.
 
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:abc-sdr"

\end_inset

Approximate Bayesian Computation
\color red
 
\color black
via 
\color inherit
 Nonlinear Sufficient Dimension Reduction
\end_layout

\begin_layout Standard
In principle, almost all the existing dimension reduction methods are valid
 in estimating the summary statistics.
 However, there is a slight difference between the setting of SDR and ABC.
 In the theory of SDR, the independent assumption 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:sdr"

\end_inset

 must hold rigorously, which implies 
\begin_inset Formula $Y\mid X$
\end_inset

 has exactly the same distribution as 
\begin_inset Formula $Y\mid S\left(X\right)$
\end_inset

.
 However, by our Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:bernstein-von-mises-mle"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:partial-post-m-est"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:bernsten-von-mise-inconsist-multv"

\end_inset

, the two distributions are only close in large but finite sample.
 
\end_layout

\begin_layout Standard
In our paper, we choose principal support vector machine in 
\begin_inset CommandInset citation
LatexCommand citet
key "li2011principal"

\end_inset

.
 Suppose we have a regression problem 
\begin_inset Formula $\left(Y_{i},X_{i}\right)$
\end_inset

, and search a nonlinear transformation 
\begin_inset Formula $\varphi:\mathbb{R}^{p}\rightarrow\mathbb{R}^{d}$
\end_inset

, such that 
\begin_inset Formula $Y\independent X\mid\varphi\left(X\right)$
\end_inset

.
 Then the main steps in principal support vector machine are given in Algorithm
 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:Principal-Support-Vector"

\end_inset

 .
 
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Enumerate
(Optional) Marginally standardize data 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

.
 The purpose of this step is so that the kernel 
\begin_inset Formula $\kappa$
\end_inset

 treats different components of 
\begin_inset Formula $X_{i}$
\end_inset

 more or less equally.
 
\end_layout

\begin_layout Enumerate
Choose kernel 
\begin_inset Formula $\kappa$
\end_inset

 and the number of basis
\color red
 
\color black
functions 
\color inherit

\begin_inset Formula $k$
\end_inset

 (usually around 
\begin_inset Formula $n/3\sim2n/3$
\end_inset

).
 Compute 
\begin_inset Formula $K=\left(\kappa\left(X_{i},X_{j}\right)\right)_{n\times n}$
\end_inset

.
 Let 
\begin_inset Formula $Q=I_{n}-J_{n}/n$
\end_inset

, where 
\begin_inset Formula $J_{n}$
\end_inset

 is the 
\begin_inset Formula $n\times n$
\end_inset

 matrix whose entries are 1.
 Compute largest 
\begin_inset Formula $k$
\end_inset

 eigenvalues 
\begin_inset Formula $\lambda_{1},\ldots,\lambda_{k}$
\end_inset

 and corresponding eigenvectors 
\begin_inset Formula $w_{1},\ldots,w_{k}$
\end_inset

 of matrix 
\begin_inset Formula $QKQ$
\end_inset

.
 Let 
\begin_inset Formula $\Psi=\left(w_{1},\ldots,w_{k}\right)$
\end_inset

 and 
\begin_inset Formula $P_{\Psi}=\Psi\left(\Psi^{T}\Psi\right)^{-1}\Psi^{T}$
\end_inset

 be the projection matrix onto 
\begin_inset Formula $\Psi$
\end_inset

.
\end_layout

\begin_layout Enumerate
\begin_inset CommandInset label
LatexCommand label
name "enu:svm"

\end_inset

Partition the response variable space 
\begin_inset Formula $Y$
\end_inset

 into 
\begin_inset Formula $h$
\end_inset

 slices defined by 
\begin_inset Formula $y_{1},\ldots,y_{h-1}$
\end_inset

.
 For each 
\begin_inset Formula $y_{s},\: s=1,\ldots,h-1$
\end_inset

, define a new response variable 
\begin_inset Formula $\tilde{Y}_{si}=I_{\left[Y_{i}\le y_{s}\right]}-I_{\left[Y_{i}>y_{s}\right]}$
\end_inset

.
 Then solve the modified support vector machine problem as a standard quadratic
 programming 
\begin_inset Formula 
\[
\min_{\alpha}-1^{T}\alpha+\frac{1}{4}\alpha^{T}\mathrm{diag}\left(\tilde{Y}_{s}\right)P_{\Psi}\mathrm{diag}\left(\tilde{Y}_{s}\right)\alpha,
\]

\end_inset

subject to constraints
\begin_inset Formula 
\[
\begin{cases}
0\le\alpha\le\lambda,\\
\tilde{Y}_{s}^{T}\alpha=0,
\end{cases}
\]

\end_inset

where 
\begin_inset Formula $\mathrm{diag}\left(\tilde{Y}_{s}\right)$
\end_inset

 is a diagonal matrix using 
\begin_inset Formula $\tilde{Y}_{s}$
\end_inset

 as diagonal, 
\begin_inset Formula $\lambda$
\end_inset

 is a hyper-parameter in ordinary support vector machine.
 The coefficients of support vectors in RKHS
\color red
 
\color black
are
\color inherit
 
\begin_inset Formula 
\[
c_{s}^{*}=\frac{1}{2}\left(\Psi^{T}\Psi\right)^{-1}\Psi^{T}\mathrm{diag}\left(\tilde{Y}_{s}\right)\alpha_{s}.
\]

\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset CommandInset label
LatexCommand label
name "enu:pca"

\end_inset

Let 
\begin_inset Formula $d$
\end_inset

 be the target dimension.
 Compute the eigenvectors 
\begin_inset Formula $v_{1},\ldots,v_{d}$
\end_inset

 of first largest 
\begin_inset Formula $d$
\end_inset

 eigenvalues of the matrix 
\begin_inset Formula $\sum_{s=1}^{h-1}c_{s}^{*}c_{s}^{*T}$
\end_inset

.
 Let 
\begin_inset Formula $V=\left(v_{1},\ldots,v_{d}\right).$
\end_inset


\end_layout

\begin_layout Enumerate
Let 
\begin_inset Formula $K\left(x,X\right)=\left(\kappa\left(x,X_{i}\right)-n^{-1}\sum_{j=1}^{n}\kappa\left(x,X_{j}\right)\right)$
\end_inset

 be a 
\begin_inset Formula $n$
\end_inset

 dimensional vector.
 Then the estimated transformation 
\begin_inset Formula $\hat{\varphi}\left(x\right)=V^{T}\left(\mathrm{diag}\left(\lambda_{1},\ldots\lambda_{k}\right)\right)^{-1}\Psi^{T}K\left(x,X\right).$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Principal Support Vector Machine
\begin_inset CommandInset label
LatexCommand label
name "alg:Principal-Support-Vector"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset

By slicing the response variable space, we discretize variation of 
\begin_inset Formula $Y$
\end_inset

.
 The support vector machine in Step 
\begin_inset CommandInset ref
LatexCommand ref
reference "enu:svm"

\end_inset

 recognizes the robust separate hyperplanes.
 We will expect the variation of 
\begin_inset Formula $Y$
\end_inset

 along the directions within hyperplanes to be negligible and that along
 the directions perpendicular to the hyperplanes the most part of covariation
 between 
\begin_inset Formula $Y$
\end_inset

 and 
\begin_inset Formula $X$
\end_inset

 is explained.
 The principal component analysis on the support vectors in Step 
\begin_inset CommandInset ref
LatexCommand ref
reference "enu:pca"

\end_inset

 estimates the
\color red
 
\color black
principal 
\color red
 
\color inherit
perpendicular directions and hence creates the sufficient directions in
 RHKS.
 
\end_layout

\begin_layout Standard
Based on Algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:Principal-Support-Vector"

\end_inset

, we formulate our 
\color black
two-step 
\color inherit
approximate Bayesian computation algorithm in Algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:ABC-via-PSVM"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Enumerate
\begin_inset CommandInset label
LatexCommand label
name "enu:direct-sampling"

\end_inset

Sample 
\begin_inset Formula $\theta_{i}$
\end_inset

 from the prior 
\begin_inset Formula $\pi\left(\theta\right)$
\end_inset

 and sample 
\begin_inset Formula $X_{i1},\ldots,X_{in}$
\end_inset

 from the model 
\begin_inset Formula $f\left(x\mid\theta_{i}\right)$
\end_inset

.
 
\end_layout

\begin_layout Enumerate
View 
\begin_inset Formula $\left(\theta_{i},X_{i1},\ldots X_{in}\right)$
\end_inset

 as a multivariate regression problem and reduce the dimension from 
\begin_inset Formula $n$
\end_inset

 to 
\begin_inset Formula $d$
\end_inset

 via principal support vector machine.
 Denote the estimated transformation as 
\begin_inset Formula $\hat{S}\left(X_{1},\ldots,X_{n}\right)$
\end_inset

.
 
\end_layout

\begin_layout Enumerate
Either use existent samples in Step 
\begin_inset CommandInset ref
LatexCommand ref
reference "enu:direct-sampling"

\end_inset

 or repeat it and get new sample.
 Calculate the estimated summary statistics 
\begin_inset Formula $\hat{S}_{i}=\hat{S}\left(X_{i1},\ldots,X_{in}\right)$
\end_inset

 on each set 
\begin_inset Formula $X_{i1},\ldots,X_{in}$
\end_inset

 corresponding to prior samples 
\begin_inset Formula $\theta_{i}$
\end_inset

.
 Also calculate 
\begin_inset Formula $\hat{S}_{\mathrm{obs}}=\hat{S}\left(X_{1},\ldots,X_{n}\right)$
\end_inset

 on the observed data set.
\end_layout

\begin_layout Enumerate
Based on the metric 
\begin_inset Formula $\rho\left(\hat{S}_{i},\hat{S}_{\mathrm{obs}}\right)$
\end_inset

, make the decision of accept or reject of 
\begin_inset Formula $\theta_{i}$
\end_inset

.
 
\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:ABC-via-PSVM"

\end_inset

ABC via PSVM
\end_layout

\end_inset


\end_layout

\end_inset

Algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:ABC-via-PSVM"

\end_inset

 directly generalizes the semi-automatic ABC in 
\begin_inset CommandInset citation
LatexCommand citet
key "fearnhead2012constructing"

\end_inset

.
 In their algorithm, the summary statistics are fixed as posterior means
 and recommended the estimation method is polynomial regression.
 Our algorithm relaxes the restriction on summary statistics and
\color red
 
\color black
lets 
\color inherit
the data and nonparametric algorithm together find them adaptively.
 One significant difference between our algorithm and the conventional ABC
 is in Step 
\begin_inset CommandInset ref
LatexCommand ref
reference "enu:direct-sampling"

\end_inset

, where each prior sample 
\begin_inset Formula $\theta_{i}$
\end_inset

 produces exact 
\begin_inset Formula $n$
\end_inset

 simulated data, because the nonparametric estimator of statistics should
 take 
\begin_inset Formula $n$
\end_inset

 arguments so that it can be evaluated at both observed data and simulated
 data.
\end_layout

\begin_layout Standard
Next, we will show a simple simulation example to illustrate our algorithm.
\end_layout

\begin_layout Example
Autoregressive model with lag one, AR(1).
 
\begin_inset Formula 
\[
Y_{i}=\beta Y_{i-1}+\varepsilon.
\]

\end_inset

Set 
\begin_inset Formula $Y_{1}=1$
\end_inset

 and number of observation is 100.
 Assume 
\begin_inset Formula $\varepsilon\sim N\left(0,0.5^{2}\right)$
\end_inset

, true regression coefficient 0.6.
 We put uniform prior in 
\begin_inset Formula $\left(-1,1\right)$
\end_inset

 on 
\begin_inset Formula $\beta$
\end_inset

, then the true posterior distribution is 
\begin_inset Formula $N\left(\sum_{i=1}^{99}Y_{i}Y_{i+1}/\left(1+\sum_{i=1}^{99}Y_{i}^{2}\right),\left(1+\sum_{i=1}^{99}Y_{i}^{2}\right)^{-1}\right)$
\end_inset

.
 Now we apply our algorithm with the target dimension 
\begin_inset Formula $d=1$
\end_inset

 and slicing pieces 
\begin_inset Formula $h=4$
\end_inset

 with the slicing parameters 
\begin_inset Formula $y_{k}$
\end_inset

 as quartiles.
 The sample size from the prior is 1000, with 
\begin_inset Formula $k=100/2=500$
\end_inset

.
 Kernel 
\begin_inset Formula $\kappa$
\end_inset

 is chosen as Gaussian kernel 
\begin_inset Formula $\kappa\left(x_{i},x_{j}\right)=\exp\left(-10^{-5}\times\|x_{i}-x_{j}\|^{2}\right)$
\end_inset

.
 Then the posterior density estimated from ABC samples are plotted in Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:ABC-vs-True"

\end_inset

.
 
\begin_inset Note Comment
status open

\begin_layout Plain Layout
try increasing number of slice and k
\end_layout

\end_inset


\end_layout

\begin_layout Example
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename 程序/ABCvsFull _ 100 _ 1000 _ 4 .pdf
	scale 70

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
ABC Density vs True Posterior Density
\begin_inset CommandInset label
LatexCommand label
name "fig:ABC-vs-True"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset

The slight skewness in Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:ABC-vs-True"

\end_inset

 possibly due the the small sample size of the observed data.
 Another interesting result of this simulation is shown in Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:ss-vs-mle"

\end_inset

.
 
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename 程序/ESSvsMLE _ 100 _ 1000 _ 4 .pdf
	scale 70

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Estimated Summary Statistic vs MLE
\begin_inset CommandInset label
LatexCommand label
name "fig:ss-vs-mle"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset

There is a strong linear relationship between the estimated summary statistic
 and MLE 
\begin_inset Formula 
\[
\hat{\beta}=\frac{\sum_{i=1}^{99}Y_{i}Y_{i+1}}{\sum_{i=1}^{99}Y_{i}^{2}},
\]

\end_inset

which is one of the most efficient summary
\color red
 
\color black
statistics 
\color inherit
 based on Theorems 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:bernstein-von-mises-mle"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:partial-post-m-est"

\end_inset

.
 Hence, our algorithm will automatically approach the most efficient summary
 statistics in a nonparametric way.
 
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Discussion"

\end_inset

Discussion
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
not know epsilon sufficient, not know sufficient lead to intrinsic consistency,r
egression had to generate to multivariate theta,dim must be same
\end_layout

\end_inset

In this paper, we explore ABC both from a theoretical and computational
 points of view.
 The theory part architects the foundation of ABC by linking asymptotic
 properties of statistics to that of the partial posterior.
 The application part innovates the algorithm by virtue of bridging selection
 of summary statistics and SDR.
 However, although the theory in 
\begin_inset CommandInset citation
LatexCommand citet
key "li1992principal"

\end_inset

 is very powerful and may be used as a theoretical guard of our algorithm,
 it heavily depends on the relation 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:sdr"

\end_inset

 holding rigorously.
 We do not know whether the result from principal support vector machine
 would be defunct if 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:sdr"

\end_inset

 is only valid in 
\begin_inset Formula $\varepsilon$
\end_inset

-sufficient way.
 Moreover, bringing in dimension reduction regression settings perhaps moderates
 the usage when there are multiple parameters of interest, and may need
 advance techniques such as envelope models of 
\begin_inset CommandInset citation
LatexCommand citet
key "su2011partial,su2012inner"

\end_inset

.
 
\end_layout

\begin_layout Section*
Acknowledgment
\end_layout

\begin_layout Standard
Ghosh's research was partially supported by an NSF Grant.
\end_layout

\begin_layout Chapter
\start_of_appendix
Proof for Higher-Order Properties of Bayesian Empirical Likelihood: Multivariate
 Case
\end_layout

\begin_layout Standard
In the appendix, we give the detail proof of the theorem 
\begin_inset Note Comment
status open

\begin_layout Plain Layout
asymptotic expansion theorem 
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Section
differentiability with respect to 
\begin_inset Formula $\theta$
\end_inset

.
 
\end_layout

\begin_layout Standard
In this section, we prove the fact that the empirical likelihood is a smooth
 function of the mean parameter 
\begin_inset Formula $\theta$
\end_inset

.
 Thus we can take arbitrary order of derivative with respect to 
\begin_inset Formula $\theta$
\end_inset

.
 We begin the proof by the fact that the Lagrange multipliers are smooth
 functions of 
\begin_inset Formula $\theta$
\end_inset

.
 
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:mul-el-smooth-lagrange-multp"

\end_inset

Under the assumption 
\begin_inset CommandInset ref
LatexCommand formatted
reference "enu:multv-diff-full-rank"

\end_inset

, 
\begin_inset Formula $\nu\left(\theta\right)\in C^{\infty}\left(H_{n}\right)$
\end_inset

 with probability 1 in 
\begin_inset Formula $P_{X}^{n}$
\end_inset

.
 
\end_layout

\begin_layout Proof
\begin_inset Formula $\nu\left(\theta\right)$
\end_inset

 is a implicit function defined by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:lambda-eq"

\end_inset

 .
 Let 
\begin_inset Formula 
\begin{eqnarray*}
G_{1}\left(\nu,\theta\right) & = & \sum_{i=1}^{n}\frac{X_{i}-\theta}{1+\nu^{T}\left(X_{i}-\theta\right)},
\end{eqnarray*}

\end_inset

By implicit function theorem, we only need to show that 
\begin_inset Formula $\det\left(\frac{\partial G_{1}}{\partial\nu}\right)\neq0$
\end_inset

.
 For any 
\begin_inset Formula $1\le j,l\le p$
\end_inset

, 
\begin_inset Formula 
\[
\frac{\partial G_{1j}}{\partial\nu_{l}}=\frac{\partial}{\partial\nu_{l}}\sum_{i=1}^{n}\frac{X_{ij}-\theta_{j}}{1+\sum_{k=1}^{p}\nu_{k}\left(X_{ik}-\theta_{k}\right)}=-\sum_{i=1}^{n}\frac{X_{ij}-\theta_{j}}{1+\nu^{T}\left(X_{i}-\theta\right)}\frac{X_{il}-\theta_{l}}{1+\nu^{T}\left(X_{i}-\theta\right)}.
\]

\end_inset

Let 
\begin_inset Formula 
\[
\Delta=\left(\frac{X_{ij}-\theta_{j}}{1+\nu^{T}\left(X_{i}-\theta\right)}\right)_{n\times p}.
\]

\end_inset

Then 
\begin_inset Formula $\frac{\partial G_{1}}{\partial\nu}=\Delta^{T}\Delta$
\end_inset

.
 If we want 
\begin_inset Formula $\det\left(\frac{\partial G_{1}}{\partial\nu}\right)\neq0$
\end_inset

 , we need 
\begin_inset Formula $\Delta$
\end_inset

 has a full column rank.
 Suppose 
\begin_inset Formula $\mathrm{rank}\left(\Delta\right)<p$
\end_inset

, then there is a vector 
\begin_inset Formula $\alpha\neq0$
\end_inset

, such that 
\begin_inset Formula $\Delta\alpha=0$
\end_inset

, that is 
\begin_inset Formula 
\begin{eqnarray*}
\sum_{j=1}^{p}\frac{X_{ij}-\theta_{j}}{1+\nu^{T}\left(X_{i}-\theta\right)}\alpha_{j} & = & 0,\\
\frac{1}{1+\nu^{T}\left(X_{i}-\theta\right)}\sum_{j=1}^{p}\left(\alpha_{j}X_{ij}-\alpha_{j}\theta_{j}\right) & = & 0.
\end{eqnarray*}

\end_inset

Note that 
\begin_inset Formula $\hat{w}_{i}=\frac{1}{n\left[1+\nu^{T}\left(X_{i}-\theta\right)\right]}>0$
\end_inset

, so we have 
\begin_inset Formula 
\[
\sum_{j=1}^{p}\alpha_{j}X_{ij}=\sum_{j=1}^{p}\alpha_{j}\theta_{j}.
\]

\end_inset

For any 
\begin_inset Formula $1\le i\neq l\le p$
\end_inset

, 
\begin_inset Formula 
\[
\sum_{j=1}^{p}\alpha_{j}X_{ij}=\sum_{j=1}^{p}\alpha_{j}\theta_{j}=\sum_{j=1}^{p}\alpha_{j}X_{lj},
\]

\end_inset

that is 
\begin_inset Formula 
\begin{eqnarray*}
\sum_{j=1}^{p}\left(X_{ij}-X_{lj}\right)\alpha_{j} & = & 0,\\
Z\alpha & = & 0.
\end{eqnarray*}

\end_inset

Contradictory with the assumption 
\begin_inset CommandInset ref
LatexCommand formatted
reference "enu:multv-diff-full-rank"

\end_inset

.
 Hence, we have 
\begin_inset Formula $\det\left(\frac{\partial G_{1}}{\partial\nu}\right)\neq0$
\end_inset

, and therefore, by implicit function theorem, 
\begin_inset Formula $\nu$
\end_inset

 is differentiable with respect to 
\begin_inset Formula $\theta$
\end_inset

.
 Now we prove the existence of higher order derivatives by induction.
 For 
\begin_inset Formula $k=1$
\end_inset

, take the derivative with respect to 
\begin_inset Formula $\theta_{l}$
\end_inset

 at the both sides of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:lambda-eq"

\end_inset

, 
\begin_inset Formula 
\[
\sum_{i=1}^{n}\sum_{s=1}^{p}\frac{\partial\nu_{s}}{\partial\theta_{l}}\frac{X_{is}-\theta_{s}}{1+\nu^{T}\left(X_{i}-\theta\right)}\frac{X_{ij}-\theta_{j}}{1+\nu^{T}\left(X_{i}-\theta\right)}=-\delta_{jl}\sum_{i=1}^{n}\frac{1}{1+\nu^{T}\left(X_{i}-\theta\right)}+\nu_{l}\sum_{i=1}^{n}\frac{X_{ij}-\theta_{j}}{\left[1+\nu^{T}\left(X_{i}-\theta\right)\right]^{2}},
\]

\end_inset

where 
\begin_inset Formula $\delta_{jl}$
\end_inset

 is Kronecker's delta, or in matrix form 
\begin_inset Formula 
\[
\left(\Delta^{T}\Delta\right)\frac{\partial\nu}{\partial\theta}=-nI_{p}+n\Delta^{T}W\nu,
\]

\end_inset

where 
\begin_inset Formula $W$
\end_inset

 is the vector of weights 
\begin_inset Formula $\hat{w}_{i}\left(\theta\right)$
\end_inset

.
 Hence 
\begin_inset Formula 
\[
\frac{\partial\nu}{\partial\theta}=-n\left(\Delta^{T}\Delta\right)^{-1}+n\left(\Delta^{T}\Delta\right)^{-1}\Delta^{T}W\nu^{T}.
\]

\end_inset

We know both 
\begin_inset Formula $\Delta$
\end_inset

, 
\begin_inset Formula $W$
\end_inset

 and 
\begin_inset Formula $\nu$
\end_inset

 are differentiable with respect to 
\begin_inset Formula $\theta$
\end_inset

.
 Suppose 
\begin_inset Formula $k\le K$
\end_inset

, we have 
\begin_inset Formula 
\[
\nabla^{k}\nu=f_{k}\left(\nu\left(\theta\right)\right),
\]

\end_inset

where 
\begin_inset Formula $f_{k}\in C^{\infty}$
\end_inset

 are a bunch of smooth functions of 
\begin_inset Formula $\nu$
\end_inset

, then for 
\begin_inset Formula $k=K+1$
\end_inset

, 
\begin_inset Formula 
\[
\nabla^{K+1}\nu=\nabla\left(\nabla^{K}\nu\right).
\]

\end_inset

We know that 
\begin_inset Formula $\nu$
\end_inset

 is differentiable, and 
\begin_inset Formula $f_{K}$
\end_inset

 are smooth functions, so the gradient above is well defined, and 
\begin_inset Formula 
\[
\nabla^{K+1}\nu=\nabla f_{K}\left(\nu\right)\nabla\nu,
\]

\end_inset

are also smooth functions.
 By mathematical induction, we have 
\begin_inset Formula $\nu\in C^{\infty}\left(H_{n}\right)$
\end_inset

.
 
\end_layout

\begin_layout Standard
The smoothness of Lagrange multipliers leads to the smoothness of empirical
 likelihood weights, and thus logarithm of empirical likelihood.
 These results justify the Taylor expansion of logarithm of empirical likelihood
 around the sample mean.
 
\end_layout

\begin_layout Section
the behavior of the logarithm of empirical likelihood and higher order derivativ
es around sample mean
\end_layout

\begin_layout Standard
We expand the posterior around the sample mean which is indeed the maximum
 of log empirical likelihood.
 
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:mean-max-el"

\end_inset


\begin_inset Formula $\overline{X}$
\end_inset

 maximizes the log empirical likelihood 
\begin_inset Formula $\hat{l}\left(\theta\right)$
\end_inset

, and 
\begin_inset Formula $\nabla\hat{l}\left(\overline{X}\right)=0$
\end_inset

.
 
\end_layout

\begin_layout Proof
By the inequality of arithmetic mean and geometric mean, we have 
\begin_inset Formula 
\begin{equation}
\hat{l}\left(\theta\right)=\frac{1}{n}\sum_{i=1}^{n}\log\hat{w}_{i}\left(\theta\right)=\log\left(\prod_{i=1}^{n}\hat{w}_{i}\left(\theta\right)\right)^{\frac{1}{n}}\le\log\frac{\sum_{i=1}^{n}\hat{w}_{i}\left(\theta\right)}{n}=\log\frac{1}{n},\label{eq:bound-log-el}
\end{equation}

\end_inset

where the equality holds if and only if all the 
\begin_inset Formula $\hat{w}_{i}\left(\theta\right)$
\end_inset

 are equal, i.e.
 , 
\begin_inset Formula $\hat{w}_{i}\left(\theta\right)=n^{-1}.$
\end_inset

 So 
\begin_inset Formula $\theta=\sum_{i=1}^{n}\hat{w}_{i}\left(\theta\right)X_{i}=\sum_{i=1}^{n}n^{-1}X_{i}=\overline{X}$
\end_inset

.
 So 
\begin_inset Formula $\hat{w}_{i}\left(\overline{X}\right)=n^{-1}$
\end_inset

 and 
\begin_inset Formula $\nu\left(\overline{X}\right)=0$
\end_inset

.
 By 
\begin_inset CommandInset ref
LatexCommand formatted
reference "lem:mul-el-smooth-lagrange-multp"

\end_inset

, 
\begin_inset Formula $\hat{l}\left(\theta\right)$
\end_inset

 is a smooth function of 
\begin_inset Formula $\theta$
\end_inset

, so at the maximal 
\begin_inset Formula $\overline{X}$
\end_inset

, 
\begin_inset Formula $\nabla\hat{l}\left(\theta\right)=0$
\end_inset

.
 
\end_layout

\begin_layout Standard
Next, we prove that the higher order derivatives of log empirical likelihood
 evaluated at 
\begin_inset Formula $\overline{X}$
\end_inset

 are solely smooth functions of the sample central moments.
 Then the remainder of Taylor expansion can be bounded by the power of 
\begin_inset Formula $n^{-1}$
\end_inset

.
 
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:control-higher-order-derivative-l"

\end_inset

For any 
\begin_inset Formula $k=2,3,\ldots$
\end_inset

 , 
\begin_inset Formula 
\[
\nabla^{k}\hat{l}\left(\theta\right)=f_{k}\left(\frac{1}{n}\sum_{i=1}^{n}\frac{\prod_{s=1}^{l_{1}}\left(X_{ij_{s}}-\theta_{j_{s}}\right)}{\left[1+\nu^{T}\left(X_{i}-\theta\right)\right]^{t_{l_{1},1}}},\frac{1}{n}\sum_{i=1}^{n}\frac{\prod_{s=1}^{l_{1}}\left(X_{ij_{s}}-\theta_{j_{s}}\right)}{\left[1+\nu^{T}\left(X_{i}-\theta\right)\right]^{t_{l_{1},2}}},\ldots,\frac{1}{n}\sum_{i=1}^{n}\frac{\prod_{s=1}^{l_{m}}\left(X_{ij_{s}}-\theta_{j_{s}}\right)}{\left[1+\nu^{T}\left(X_{i}-\theta\right)\right]^{t_{l_{m},q}}},\nu\right),
\]

\end_inset

where 
\begin_inset Formula $f_{k}$
\end_inset

 are rational function and the denominator are only the power of 
\begin_inset Formula $\Delta^{T}\Delta$
\end_inset

, 
\begin_inset Formula $1\le l_{i}\le k$
\end_inset

, and 
\begin_inset Formula $l_{i}\le t_{l_{i},j}\le C_{k}<\infty$
\end_inset

.
 
\end_layout

\begin_layout Proof
We prove this lemma use mathematical induction.
 When 
\begin_inset Formula $u=2$
\end_inset

, by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:expression-fisher-inf-mat"

\end_inset

 below, we have 
\begin_inset Formula 
\[
\nabla^{2}\hat{l}\left(\theta\right)=f_{2}\left(\frac{1}{n}\sum_{i=1}^{n}\frac{\left(X_{ij}-\theta_{j}\right)\left(X_{is}-\theta_{s}\right)}{\left[1+\nu^{T}\left(X_{i}-\theta\right)\right]^{2}},\nu\right).
\]

\end_inset

Suppose for 
\begin_inset Formula $u=k$
\end_inset

, we have 
\begin_inset Formula 
\[
\nabla^{k}\hat{l}\left(\theta\right)=f_{k}\left(\frac{1}{n}\sum_{i=1}^{n}\frac{\prod_{s=1}^{l_{1}}\left(X_{ij_{s}}-\theta_{j_{s}}\right)}{\left[1+\nu^{T}\left(X_{i}-\theta\right)\right]^{t_{l_{1},1}}},\frac{1}{n}\sum_{i=1}^{n}\frac{\prod_{s=1}^{l_{1}}\left(X_{ij_{s}}-\theta_{j_{s}}\right)}{\left[1+\nu^{T}\left(X_{i}-\theta\right)\right]^{t_{l_{1},2}}},\ldots,\frac{1}{n}\sum_{i=1}^{n}\frac{\prod_{s=1}^{l_{m}}\left(X_{ij_{s}}-\theta_{j_{s}}\right)}{\left[1+\nu^{T}\left(X_{i}-\theta\right)\right]^{t_{l_{m},q}}},\nu\right).
\]

\end_inset

When 
\begin_inset Formula $u=k+1$
\end_inset

, for any 
\begin_inset Formula $v=1,2,\ldots,p$
\end_inset

, 
\begin_inset Formula 
\[
\frac{\partial\nabla^{k}\hat{l}\left(\theta\right)}{\partial\theta_{v}}=\nabla f_{k}\frac{\partial}{\partial\theta_{v}}\left(\frac{1}{n}\sum_{i=1}^{n}\frac{\prod_{s=1}^{l_{1}}\left(X_{ij_{s}}-\theta_{j_{s}}\right)}{\left[1+\nu^{T}\left(X_{i}-\theta\right)\right]^{t_{l_{1},1}}},\ldots,\frac{1}{n}\sum_{i=1}^{n}\frac{\prod_{s=1}^{l_{m}}\left(X_{ij_{s}}-\theta_{j_{s}}\right)}{\left[1+\nu^{T}\left(X_{i}-\theta\right)\right]^{t_{l_{m},q}}},\nu\right).
\]

\end_inset

For any 
\begin_inset Formula $l$
\end_inset

 and 
\begin_inset Formula $t$
\end_inset

 , 
\begin_inset Formula 
\begin{eqnarray*}
 &  & \frac{\partial}{\partial\theta_{v}}\frac{1}{n}\sum_{i=1}^{n}\frac{\prod_{s=1}^{l}\left(X_{ij_{s}}-\theta_{j_{s}}\right)}{\left[1+\nu^{T}\left(X_{i}-\theta\right)\right]^{t}}\\
 & = & \frac{1}{n}\sum_{i=1}^{n}\frac{1}{\left[1+\nu^{T}\left(X_{i}-\theta\right)\right]^{2t}}\Bigg\{-\sum_{s=1}^{l}\delta_{j_{s}v}\prod_{r\neq s}\left(X_{ij_{r}}-\theta_{j_{r}}\right)\left[1+\nu^{T}\left(X_{i}-\theta\right)\right]^{t}-\prod_{s=1}^{l}\left(X_{ij_{s}}-\theta_{j_{s}}\right)\\
 &  & \times t\left[1+\nu^{T}\left(X_{i}-\theta\right)\right]^{t-1}\left[\sum_{h=1}^{p}\frac{\partial\nu_{h}}{\partial\theta_{t}}\left(X_{ih}-\theta_{h}\right)-\nu_{v}\right]\Bigg\}\\
 & = & -\sum_{s=1}^{l}\delta_{j_{s}v}\left\{ \frac{1}{n}\sum_{i=1}^{n}\frac{\prod_{r\neq s}\left(X_{ij_{r}}-\theta_{j_{r}}\right)}{\left[1+\nu^{T}\left(X_{i}-\theta\right)\right]^{t}}\right\} -t\sum_{h=1}^{p}\frac{\partial\nu_{h}}{\partial\theta_{t}}\left\{ \frac{1}{n}\sum_{i=1}^{n}\frac{\left(X_{ih}-\theta_{h}\right)\prod_{s=1}^{l}\left(X_{ij_{s}}-\theta_{j_{s}}\right)}{\left[1+\nu^{T}\left(X_{i}-\theta\right)\right]^{t+1}}\right\} \\
 &  & +t\nu_{v}\left\{ \frac{1}{n}\sum_{i=1}^{n}\frac{\prod_{s=1}^{l}\left(X_{ij_{s}}-\theta_{j_{s}}\right)}{\left[1+\nu^{T}\left(X_{i}-\theta\right)\right]^{t+1}}\right\} ,
\end{eqnarray*}

\end_inset

which is still a smooth function of 
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}\frac{\prod_{s=1}^{l}\left(X_{ij_{s}}-\theta_{j_{s}}\right)}{\left[1+\nu^{T}\left(X_{i}-\theta\right)\right]^{t}}$
\end_inset

, and the denominator is only introduced by term 
\begin_inset Formula $\frac{\partial\nu}{\partial\theta}$
\end_inset

, which contain only 
\begin_inset Formula $\Delta^{T}\Delta$
\end_inset

.
 Note that the gradient of a rational function 
\begin_inset Formula $f_{k}$
\end_inset

 is still a rational function.
 Let 
\begin_inset Formula $C_{k+1}=C_{k}+1$
\end_inset

.
 Then the statement holds when 
\begin_inset Formula $u=k+1$
\end_inset

, by mathematical induction, it also holds for any 
\begin_inset Formula $k$
\end_inset

.
 
\end_layout

\begin_layout Standard
By 
\begin_inset CommandInset ref
LatexCommand formatted
reference "lem:control-higher-order-derivative-l"

\end_inset

, when evaluated at 
\begin_inset Formula $\theta=\overline{X}$
\end_inset

, since 
\begin_inset Formula $\nu\left(\overline{X}\right)=0$
\end_inset

, then 
\begin_inset Formula 
\[
\nabla^{k}\hat{l}\left(\overline{X}\right)=f_{k}\left(\frac{1}{n}\sum_{i=1}^{n}\prod_{s=1}^{l_{1}}\left(X_{ij_{s}}-\theta_{j_{s}}\right),\ldots,\frac{1}{n}\sum_{i=1}^{n}\prod_{s=1}^{l_{m}}\left(X_{ij_{s}}-\theta_{j_{s}}\right)\right).
\]

\end_inset


\begin_inset Note Comment
status open

\begin_layout Plain Layout
cut paste to other place
\end_layout

\end_inset

By assumption 
\begin_inset CommandInset ref
LatexCommand formatted
reference "enu:finite-moment-sample"

\end_inset

 and SLLN, when 
\begin_inset Formula $n\rightarrow\infty$
\end_inset

, 
\begin_inset Formula 
\[
\frac{1}{n}\sum_{i=1}^{n}\prod_{s=1}^{l}\left(X_{ij_{s}}-\overline{X}_{j_{s}}\right)\rightarrow E\prod_{s=1}^{l}\left(X_{ij_{s}}-\theta_{j_{s},0}\right)<\infty,\ascv.
\]

\end_inset


\begin_inset Note Comment
status open

\begin_layout Plain Layout
not enough for higher order derivative to be finite, need some properties
 for f_k.
\end_layout

\end_inset

By the smoothness of 
\begin_inset Formula $f_{k}$
\end_inset

 , for any 
\begin_inset Formula $\varepsilon$
\end_inset

, there exist a sufficient large 
\begin_inset Formula $N_{\left(k\right)}$
\end_inset

, such that for any 
\begin_inset Formula $n>N_{\left(k\right)}$
\end_inset

, 
\begin_inset Formula 
\[
\left|\nabla^{k}\hat{l}\left(\overline{X}\right)-f_{k}\left(E\prod_{s=1}^{l}\left(X_{ij_{s}}-\theta_{j_{s},0}\right)\right)\right|<\varepsilon,\ascv.
\]

\end_inset

By smoothness of 
\begin_inset Formula $\nabla^{k}\hat{l}\left(\theta\right)$
\end_inset

, there exist a 
\begin_inset Formula $\delta>0$
\end_inset

, such that for any 
\begin_inset Formula $\left|B\left(\theta-\overline{X}\right)\right|<\delta$
\end_inset

, 
\begin_inset Formula 
\[
\left|\nabla^{k}\hat{l}\left(\theta\right)-\nabla^{k}\hat{l}\left(\overline{X}\right)\right|<\varepsilon.
\]

\end_inset

Hence, 
\begin_inset Formula 
\[
\left|\nabla^{k}\hat{l}\left(\theta\right)-f_{k}\left(E\prod_{s=1}^{l}\left(X_{ij_{s}}-\theta_{j_{s},0}\right)\right)\right|\le\left|\nabla^{k}\hat{l}\left(\overline{X}\right)-f_{k}\left(E\prod_{s=1}^{l}\left(X_{ij_{s}}-\theta_{j_{s},0}\right)\right)\right|+\left|\nabla^{k}\hat{l}\left(\theta\right)-\nabla^{k}\hat{l}\left(\overline{X}\right)\right|<2\varepsilon.
\]

\end_inset

Note that 
\begin_inset Formula $f_{k}$
\end_inset

 is a rational function and the denominator contains only the power of 
\begin_inset Formula $\Delta^{T}\Delta$
\end_inset

, then there exists a constant 
\begin_inset Formula $M_{\left(k\right)}$
\end_inset

, such that 
\begin_inset Formula $\left|f_{k}\left(E\prod_{s=1}^{l}\left(X_{ij_{s}}-\theta_{j_{s},0}\right)\right)\right|<M_{\left(k\right)}.$
\end_inset

 Therefore, 
\begin_inset Formula 
\[
M_{\left(k\right)}-2\varepsilon<\nabla^{k}\hat{l}\left(\theta\right)<M_{\left(k\right)}+2\varepsilon.
\]

\end_inset

Particularly, we can compute 
\begin_inset Formula $\nabla^{2}\hat{l}\left(\overline{X}\right)=n\left[\sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2}\right]^{-1}$
\end_inset

, which is a positive definite matrix with probability 
\begin_inset Formula $1$
\end_inset

 in 
\begin_inset Formula $P_{X}^{n}$
\end_inset

 by assumption 
\begin_inset CommandInset ref
LatexCommand formatted
reference "enu:pd-sample-var"

\end_inset

 .
 
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:near-mean-2nd-order-bound"

\end_inset

Under the assumption 
\begin_inset CommandInset ref
LatexCommand formatted
reference "enu:pd-sample-var"

\end_inset

,there exists a 
\begin_inset Formula $\delta_{1}>0$
\end_inset

, such that 
\begin_inset Formula 
\[
\sum_{i=1}^{n}\ln\hat{w}_{i}\left(\theta\right)-\sum_{i=1}^{n}\ln\hat{w}_{i}\left(\overline{X}\right)\le-\frac{1}{4}Y^{T}Y,
\]

\end_inset

for any 
\begin_inset Formula $\theta\in\left\{ \left|B\left(\theta-\overline{X}\right)\right|\le\delta_{1}\right\} \cap H_{n}$
\end_inset

 .
 
\end_layout

\begin_layout Proof
By Taylor expansion, 
\begin_inset Formula 
\begin{eqnarray*}
\frac{1}{n}\left(\sum_{i=1}^{n}\ln\hat{w}_{i}\left(\theta\right)-\sum_{i=1}^{n}\ln\hat{w}_{i}\left(\overline{X}\right)\right) & = & \left(\nabla\hat{l}\left(\overline{X}\right)\right)^{T}\left(\theta-\overline{X}\right)+\frac{1}{2}\left(\theta-\overline{X}\right)^{T}\nabla^{2}\hat{l}\left(\theta^{*}\right)\left(\theta-\overline{X}\right),
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $\left|\theta^{*}-\overline{X}\right|\le\left|\theta-\overline{X}\right|$
\end_inset

.
 By 
\begin_inset CommandInset ref
LatexCommand formatted
reference "lem:mean-max-el"

\end_inset

, the first term in above equation is zero.
 By 
\begin_inset CommandInset ref
LatexCommand formatted
reference "lem:control-higher-order-derivative-l"

\end_inset

, we know that 
\begin_inset Formula $\nabla^{2}\hat{l}\left(\theta\right)$
\end_inset

 is a continuous function in 
\begin_inset Formula $\theta$
\end_inset

.
 There exists a 
\begin_inset Formula $\delta_{1}$
\end_inset

, such that for any 
\begin_inset Formula $\left|B\left(\theta^{*}-\overline{X}\right)\right|\le\left|B\left(\theta-\overline{X}\right)\right|<\delta_{1}$
\end_inset

, 
\begin_inset Formula 
\[
\left|\left(\theta-\overline{X}\right)^{T}\nabla^{2}\hat{l}\left(\theta^{*}\right)\left(\theta-\overline{X}\right)+\left|B\left(\theta-\overline{X}\right)\right|^{2}\right|<\frac{1}{2}\left|B\left(\theta-\overline{X}\right)\right|^{2},
\]

\end_inset

hence 
\begin_inset Formula $\left(\theta-\overline{X}\right)^{T}\nabla^{2}\hat{l}\left(\theta^{*}\right)\left(\theta-\overline{X}\right)<-2^{-1}\left|B\left(\theta-\overline{X}\right)\right|^{2}$
\end_inset

.
 Therefore, 
\begin_inset Formula 
\[
\sum_{i=1}^{n}\ln\hat{w}_{i}\left(\theta\right)-\sum_{i=1}^{n}\ln\hat{w}_{i}\left(\overline{X}\right)<\frac{1}{2}\times\frac{1}{2}\left|\sqrt{n}B\left(\theta-\overline{X}\right)\right|^{2}=\frac{1}{4}Y^{T}Y.
\]

\end_inset


\end_layout

\begin_layout Section
bell shape of the logarithm of empirical likelihood
\end_layout

\begin_layout Standard
If we want the expansion of posterior is uniform on the parameter space,
 we do not only need the Taylor expansion around the sample mean, but also
 require controlling the value of posterior in the region far from sample
 mean.
 This can be achieved by showing empirical likelihood have 
\begin_inset Quotes eld
\end_inset

bell
\begin_inset Quotes erd
\end_inset

 shape around the sample mean.
 
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:second-order-der-neg-def"

\end_inset

Under the assumption 
\begin_inset CommandInset ref
LatexCommand formatted
reference "enu:multv-diff-full-rank"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "enu:finite-moment-sample"

\end_inset

, 
\begin_inset Formula $\frac{\partial^{2}\hat{l}\left(\theta\right)}{\partial\theta\partial\theta^{T}}$
\end_inset

 is negative definite almost surely in 
\begin_inset Formula $P_{X}^{n}$
\end_inset

.
 
\end_layout

\begin_layout Proof

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
For any 
\begin_inset Formula $j=1,2,\ldots,p$
\end_inset

, take the first derivative of logarithm of empirical likelihood,
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial\hat{l}_{n}\left(\theta\right)}{\partial\theta_{j}} & = & \frac{1}{n}\frac{\partial}{\partial\theta_{j}}\sum_{i=1}^{n}\left[-\ln\left(1+\sum_{k=1}^{p}\nu_{k}\left(\theta\right)\left(X_{ik}-\theta_{k}\right)\right)\right]\\
 & = & \sum_{i=1}^{n}\sum_{k=1}^{p}\frac{\partial\nu_{k}}{\partial\theta_{j}}\frac{X_{ik}-\theta_{k}}{n\left[1+\nu^{T}\left(X_{i}-\theta\right)\right]}+\sum_{i=1}^{n}\frac{1}{n\left[1+\nu^{T}\left(X_{i}-\theta\right)\right]}\nu_{j}=\nu_{j}.
\end{eqnarray*}

\end_inset

Then the Hessian of logarithm of empirical likelihood is 
\begin_inset Formula 
\[
\frac{\partial^{2}\hat{l}\left(\theta\right)}{\partial\theta^{T}\partial\theta}=\frac{\partial\nu}{\partial\theta}=-n\left(\Delta^{T}\Delta\right)^{-1}+n\left(\Delta^{T}\Delta\right)^{-1}\Delta^{T}W\nu^{T}.
\]

\end_inset

Note that 
\begin_inset Formula 
\[
\Delta\nu=\left(\frac{\nu^{T}\left(X_{i}-\theta\right)}{1+\nu^{T}\left(X_{i}-\theta\right)}\right)_{i=1}^{n}=\left(1-\frac{1}{1+\lambda^{T}\left(X_{i}-\theta\right)}\right)_{i=1}^{n}=1_{n\times1}-nW,
\]

\end_inset

where 
\begin_inset Formula $1_{n\times1}$
\end_inset

 is a column vector with all entries are equal to 1, so 
\begin_inset Formula 
\[
W=\frac{1}{n}\left(1_{n\times1}-\Delta\nu\right).
\]

\end_inset

 Replace 
\begin_inset Formula $W$
\end_inset

 in Hessian, 
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial^{2}\hat{l}\left(\theta\right)}{\partial\theta^{T}\partial\theta} & = & n\left[-\left(\Delta^{T}\Delta\right)^{-1}+\left(\Delta^{T}\Delta\right)^{-1}\Delta^{T}\frac{1}{n}\left(1_{n\times1}-\Delta\nu\right)\nu^{T}\right]\\
 & = & -n\left(\Delta^{T}\Delta\right)^{-1}+\left(\Delta^{T}\Delta\right)^{-1}\left(\Delta^{T}1_{n\times1}\right)-\left(\Delta^{T}\Delta\right)^{-1}\Delta^{T}\Delta\nu\nu^{T}.
\end{eqnarray*}

\end_inset

Also note that 
\begin_inset Formula 
\[
\Delta^{T}1_{n\times1}=\left(\sum_{i=1}^{n}\frac{X_{ij}-\theta_{j}}{1+\nu^{T}\left(X_{i}-\theta\right)}\right)_{j=1}^{n}=0.
\]

\end_inset

Therefore,
\begin_inset Formula 
\begin{equation}
\frac{\partial^{2}\hat{l}\left(\theta\right)}{\partial\theta^{T}\partial\theta}=-n\left(\Delta^{T}\Delta\right)^{-1}-\nu\nu^{T},\label{eq:expression-fisher-inf-mat}
\end{equation}

\end_inset

is obviously negative definite since in proof of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "lem:mul-el-smooth-lagrange-multp"

\end_inset

, we know that 
\begin_inset Formula $\Delta$
\end_inset

 has a full column rank.
 Particularly when 
\begin_inset Formula $\theta=\overline{X}$
\end_inset

, 
\begin_inset Formula $\frac{\partial^{2}\hat{l}\left(\overline{X}\right)}{\partial\theta^{T}\partial\theta}=-\left[\frac{1}{n}\sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2}\right]^{-1}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Now we can control the tail part by the following lemma.
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:exponential-decay-tail"

\end_inset

Under the assumption 
\begin_inset CommandInset ref
LatexCommand formatted
reference "enu:multv-diff-full-rank"

\end_inset

, for any 
\begin_inset Formula $\delta>0$
\end_inset

, there exists an 
\begin_inset Formula $\varepsilon>0$
\end_inset

, 
\begin_inset Formula $N_{2}$
\end_inset

, and 
\begin_inset Formula $D_{1}\subset\mathbb{R}^{p}$
\end_inset

, 
\begin_inset Formula $P_{X}^{n}\left(D_{1}\right)=1$
\end_inset

, such that 
\begin_inset Formula 
\[
\hat{l}\left(\theta\right)-\hat{l}\left(\overline{X}\right)\le-\varepsilon,\ascv,
\]

\end_inset

for any 
\begin_inset Formula $\left|B\left(\theta-\overline{X}\right)\right|\ge\delta$
\end_inset

 and 
\begin_inset Formula $\theta\in H_{n}$
\end_inset

.
 
\end_layout

\begin_layout Proof
In the proof of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "lem:second-order-der-neg-def"

\end_inset

, we know that 
\begin_inset Formula $\nu\left(\overline{X}\right)=0$
\end_inset

, thus 
\begin_inset Formula $\hat{l}\left(\overline{X}\right)=-\log n$
\end_inset

 and score function 
\begin_inset Formula $\hat{l}'\left(\overline{X}\right)=\nu\left(\overline{X}\right)=0$
\end_inset

.
 By strictly convexity of log empirical likelihood from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "lem:second-order-der-neg-def"

\end_inset

, we know that 
\begin_inset Formula $\overline{X}$
\end_inset

 is the unique maximal.
 Therefore, for any 
\begin_inset Formula $\theta\neq\overline{X}$
\end_inset

, 
\begin_inset Formula $\hat{l}\left(\theta\right)<\hat{l}\left(\overline{X}\right)$
\end_inset

.
 Note that the set 
\begin_inset Formula 
\[
\left\{ \theta|\left|B\left(\theta-\overline{X}\right)\right|\ge\delta\right\} \cap H_{n}
\]

\end_inset

is a compact set, and 
\begin_inset Formula $\hat{l}\left(\theta\right)$
\end_inset

 is a continuous function, then there exists a 
\begin_inset Formula $\theta^{*}\in\left\{ \theta|\left|B\left(\theta-\overline{X}\right)\right|\ge\delta\right\} \cap H_{n}$
\end_inset

 , such that for any 
\begin_inset Formula $\theta\in\left\{ \theta|\left|B\left(\theta-\overline{X}\right)\right|\ge\delta\right\} \cap H_{n}$
\end_inset

, 
\begin_inset Formula 
\[
\hat{l}\left(\theta\right)\le\hat{l}\left(\theta^{*}\right).
\]

\end_inset

Therefore, 
\begin_inset Formula $\hat{l}\left(\theta\right)\le\hat{l}\left(\theta^{*}\right)<\hat{l}\left(\overline{X}\right)$
\end_inset

, 
\begin_inset Formula 
\[
\hat{l}\left(\theta\right)-\hat{l}\left(\overline{X}\right)\le\hat{l}\left(\theta^{*}\right)-\hat{l}\left(\overline{X}\right)<0.
\]

\end_inset

Let 
\begin_inset Formula $\varepsilon=\frac{1}{2}\left(\hat{l}\left(\overline{X}\right)-\hat{l}\left(\theta^{*}\right)\right)$
\end_inset

, then we have 
\begin_inset Formula 
\[
\hat{l}\left(\theta\right)-\hat{l}\left(\overline{X}\right)\le\hat{l}\left(\theta^{*}\right)-\hat{l}\left(\overline{X}\right)<\varepsilon.
\]

\end_inset


\end_layout

\begin_layout Section
expansion near the sample mean
\end_layout

\begin_layout Standard
After we control the tail of the posterior, the remaining work are solely
 based on smoothness of the log empirical likelihood and prior.
 Particularly the form of expansion polynomials are determined by the product
 of Taylor expansion of empirical likelihood and prior at sample mean.
 
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:central-expansion-llik"

\end_inset


\begin_inset Note Comment
status open

\begin_layout Plain Layout
need condition
\end_layout

\end_inset

There exist a 
\begin_inset Formula $\delta_{2}$
\end_inset

, a constant 
\begin_inset Formula $M_{1}$
\end_inset

 and 
\begin_inset Formula $N_{1}$
\end_inset

, such that 
\begin_inset Formula 
\[
\left|\int_{\left\{ B\left(\theta-\overline{X}\right)\le\delta_{2}\right\} \cap H_{n}}\exp\left(-\frac{1}{2}Y^{T}Y+\sum_{k=3}^{K+3}\delta_{k}\hat{l}n^{-\frac{k-2}{2}}\right)-\prod_{i=1}^{n}\frac{\hat{w}_{i}\left(\theta\right)}{\hat{w}_{i}\left(\overline{X}\right)}\diff n^{-\frac{1}{2}}Y\right|\le M_{1}n^{-\frac{1}{2}\left(K+3\right)},\:\ascv.
\]

\end_inset


\end_layout

\begin_layout Proof
First, we can choose 
\begin_inset Formula $\delta_{2}$
\end_inset

 small enough so that 
\begin_inset Formula $\left\{ B\left(\theta-\overline{X}\right)\le\delta_{2}\right\} \subset H_{n}$
\end_inset

.
 
\begin_inset Formula 
\begin{eqnarray*}
 &  & \int_{\left\{ B\left(\theta-\overline{X}\right)\le\delta_{2}\right\} \cap H_{n}}\exp\left(-\frac{1}{2}Y^{T}Y+\sum_{k=3}^{K+3}\delta_{k}\hat{l}n^{-\frac{k-2}{2}}\right)-\prod_{i=1}^{n}\frac{\hat{w}_{i}\left(\theta\right)}{\hat{w}_{i}\left(\overline{X}\right)}\diff n^{-\frac{1}{2}}Y\\
 & = & \int_{\left\{ B\left(\theta-\overline{X}\right)\le\delta_{2}\right\} }\exp\left(\sum_{i=1}^{n}\ln\hat{w}_{i}\left(\theta\right)-\sum_{i=1}^{n}\ln\hat{w}_{i}\left(\overline{X}\right)\right)\\
 &  & \left[\exp\left(\left(-\frac{1}{2}Y^{T}Y+\sum_{k=3}^{K+3}\delta_{k}\hat{l}n^{-\frac{k-2}{2}}\right)-\sum_{i=1}^{n}\left(\ln\hat{w}_{i}\left(\theta\right)-\ln\hat{w}_{i}\left(\overline{X}\right)\right)\right)-1\right]\diff n^{-\frac{1}{2}}Y.
\end{eqnarray*}

\end_inset

By 
\begin_inset CommandInset ref
LatexCommand formatted
reference "lem:near-mean-2nd-order-bound"

\end_inset

, and Taylor expansion, the above equation can be bounded by 
\begin_inset Formula 
\begin{eqnarray*}
 &  & \int_{\left\{ \left|B\left(\theta-\overline{X}\right)\right|\le\delta_{2}\right\} }\exp\left(-\frac{Y^{T}Y}{4}\right)\left|\exp\left(-\frac{n^{-\frac{K+2}{2}}}{\left(K+4\right)!}\left[\left(\theta-\overline{X}\right)\nabla\right]^{K+4}\hat{l}\left(\theta^{*}\right)\right)-1\right|\diff n^{-\frac{1}{2}}Y\\
 & = & n^{-\frac{1}{2}}\int_{\left\{ \sqrt{Y^{T}Y}\le\delta_{2}\sqrt{n}\right\} }\exp\left(-\frac{Y^{T}Y}{4}\right)\left|\exp\left(-\frac{n^{-\frac{K+2}{2}}}{\left(K+4\right)!}\left(Y^{T}B^{-T}\nabla\right)^{K+4}\hat{l}\left(\theta^{*}\right)\right)-1\right|\diff Y.
\end{eqnarray*}

\end_inset

By 
\begin_inset CommandInset ref
LatexCommand formatted
reference "lem:control-higher-order-derivative-l"

\end_inset

, we know that 
\begin_inset Formula $\nabla^{K+4}\hat{l}\left(\theta^{*}\right)$
\end_inset

 are bounded by some constants.
 Note that for any 
\begin_inset Formula $Y$
\end_inset

, and any vector 
\begin_inset Formula $a$
\end_inset

, there exists a constant 
\begin_inset Formula $C_{1}$
\end_inset

, such that 
\begin_inset Formula 
\begin{equation}
\left(Y^{T}a\right)^{2}\le C_{1}\left(Y^{T}Y\right),\label{eq:average-inequality}
\end{equation}

\end_inset

so we can bounded the above equation by 
\begin_inset Formula 
\[
n^{-\frac{1}{2}}\int_{\left\{ \sqrt{Y^{T}Y}\le\delta_{2}\sqrt{n}\right\} }\exp\left(-\frac{Y^{T}Y}{4}\right)\left(\exp\left(-n^{-\frac{K+2}{2}}C_{3}\left(Y^{T}Y\right)^{\frac{K+4}{2}}\right)-1\right)\diff Y,
\]

\end_inset

where 
\begin_inset Formula $C_{3}=C_{1}^{\frac{K+4}{2}}$
\end_inset

.
 Denote the above integrand to be 
\begin_inset Formula $F\left(Y,\sqrt{n}\right)$
\end_inset

.
 We can the inequality in this lemma if we can prove the following 
\begin_inset Formula 
\begin{equation}
\lim_{n\rightarrow\infty}\frac{n^{-\frac{1}{2}}\int_{\left\{ \sqrt{Y^{T}Y}\le\delta_{2}\sqrt{n}\right\} }F\left(Y,\sqrt{n}\right)\diff Y}{n^{-\frac{1}{2}\left(K+3\right)}}=C_{2},\label{eq:lim-rhs-vs-lfs-inequality}
\end{equation}

\end_inset

for some constant 
\begin_inset Formula $C_{2}<\infty$
\end_inset

.
 Let 
\begin_inset Formula $t=\sqrt{n}$
\end_inset

, and relax 
\begin_inset Formula $t\in\mathbb{R}^{+}$
\end_inset

.
 The the above formula can be written as 
\begin_inset Formula 
\[
\frac{\int_{\left\{ \sqrt{Y^{T}Y}\le\delta_{2}t\right\} }F\left(Y,t\right)\diff Y}{t^{-K-2}}.
\]

\end_inset

Take the derivatives of both numerator and denominator with respect to 
\begin_inset Formula $t$
\end_inset

.
 For the denominator 
\begin_inset Formula $\left(t^{-K-2}\right)'=-\left(K+2\right)t^{-K-3}.$
\end_inset

 For the numerator, we change to 
\begin_inset Formula $n$
\end_inset

 dimensional spherical coordinate system, 
\begin_inset Formula 
\begin{eqnarray*}
Y_{1} & = & r\cos\left(\varphi_{1}\right),\\
Y_{2} & = & r\sin\left(\varphi_{1}\right)\cos\left(\varphi_{2}\right),\\
 & \vdots\\
Y_{p} & = & r\sin\left(\varphi_{1}\right)\sin\left(\varphi_{2}\right)\cdots\sin\left(\varphi_{p-1}\right).
\end{eqnarray*}

\end_inset

So the numerator can be written as 
\begin_inset Formula 
\begin{eqnarray*}
 &  & \int_{\left\{ r\le\delta_{2}t\right\} }\exp\left(-\frac{r^{2}}{4}\right)\left(\exp\left(-C_{3}t^{-K-2}r^{K+4}\right)-1\right)\\
 &  & \times r^{p-1}\sin^{p-2}\left(\varphi_{1}\right)\sin^{p-3}\left(\varphi_{2}\right)\cdots\sin\left(\varphi_{p-2}\right)\diff r\diff\varphi_{1}\cdots\diff\varphi_{p-1}\\
 & = & \int_{0}^{2\pi}\diff\varphi_{p-1}\int_{0}^{\pi}\sin^{p-2}\left(\varphi_{1}\right)\diff\varphi_{1}\cdots\int_{0}^{\pi}\sin\left(\varphi_{p-2}\right)\diff\varphi_{p-2}\\
 &  & \times\int_{0}^{\delta_{2}t}\exp\left(-\frac{r^{2}}{4}\right)\left(\exp\left(-C_{3}t^{-K-4}r^{K+4}\right)-1\right)r^{p-1}\diff r\\
 & \le & 2\pi\left(\pi\right)^{p-2}\int_{0}^{\delta_{2}t}\exp\left(-\frac{r^{2}}{4}\right)\left(\exp\left(-C_{3}t^{-K-2}r^{K+4}\right)-1\right)r^{p-1}\diff r\\
 & = & 2\pi^{p-1}\int_{0}^{\delta_{2}t}\exp\left(-\frac{r^{2}}{4}\right)\left(\exp\left(-C_{3}t^{-K-2}r^{K+4}\right)-1\right)r^{p-1}\diff r.
\end{eqnarray*}

\end_inset

 
\begin_inset Formula 
\begin{eqnarray*}
 &  & \frac{\diff}{\diff t}\int_{0}^{\delta_{2}t}\exp\left(-\frac{r^{2}}{4}\right)\left(\exp\left(-C_{3}t^{-K-2}r^{K+4}\right)-1\right)r^{p-1}\diff r\\
 & = & \int_{0}^{\delta_{2}t}-\left(K+2\right)t^{-K-3}\left(-C_{3}r^{K+4}\right)\exp\left(-\frac{r^{2}}{4}\right)\exp\left(-C_{3}t^{-K-2}r^{K+4}\right)r^{p-1}\diff r\\
 &  & +\exp\left(-\frac{\delta_{2}^{2}t^{2}}{4}\right)\left(\exp\left(-C_{3}t^{-K-2}\left(\delta_{2}t\right)^{K+4}\right)-1\right)\\
 & \le & C_{3}\left(K+2\right)t^{-K-3}\int_{0}^{\delta_{2}t}r^{K+p+3}\exp\int_{A\cap H_{n}}\exp\left(-\frac{Y^{T}Y}{4}\right)\left|\alpha_{h}\left(Y,n\right)\right|\diff Y\\
 &  & \left(-\left(\frac{1}{4}-C_{3}t^{-K-2}r^{K+2}\right)r^{2}\right)\diff r+\exp\left(-\left(\frac{1}{4}-C_{3}\delta_{2}^{K+2}\right)\delta_{2}^{2}t^{2}\right)-\exp\left(-\frac{\delta_{2}^{2}t^{2}}{4}\right).
\end{eqnarray*}

\end_inset

If 
\begin_inset Formula 
\[
\delta_{2}<\sqrt[K+2]{\frac{1}{4C_{3}}},
\]

\end_inset

then 
\begin_inset Formula 
\[
\frac{1}{4}-C_{3}\delta_{2}^{K+2}>0,
\]

\end_inset

hence 
\begin_inset Formula 
\[
\lim_{t\rightarrow+\infty}\frac{\exp\left(-\left(\frac{1}{4}-C_{3}\delta_{2}^{K+2}\right)\delta_{2}^{2}t^{2}\right)-\exp\left(-\frac{\delta_{2}^{2}t^{2}}{4}\right)}{-\left(K+2\right)t^{-K-3}}=0.
\]

\end_inset

For the first term in derivative of numerator, we have for any 
\begin_inset Formula $t>0$
\end_inset

.
 
\begin_inset Formula 
\begin{eqnarray*}
 &  & \int_{0}^{\delta_{2}t}r^{K+p+3}\exp\left(-\left(\frac{1}{4}-C_{3}t^{-K-2}r^{K+2}\right)r^{2}\right)\diff r\\
 & \le & \int_{0}^{\delta_{2}t}r^{K+p+3}\exp\left(-\left(\frac{1}{4}-C_{3}\delta_{2}^{K+2}\right)r^{2}\right)\diff r\\
 & \le & \int_{\mathbb{R}}\left|r\right|^{K+p+3}\exp\left(-\left(-\frac{1}{4}-C_{3}\delta_{2}^{K+2}\right)r^{2}\right)\diff r<+\infty.
\end{eqnarray*}

\end_inset

Therefore, there exists a constant 
\begin_inset Formula $C_{2}$
\end_inset

, such that 
\begin_inset Formula 
\begin{eqnarray*}
 &  & \lim_{t\rightarrow+\infty}\frac{C_{3}\left(K+2\right)t^{-K-3}\int_{0}^{\delta_{2}t}r^{K+p+3}\exp\left(-\left(\frac{1}{4}-C_{3}t^{-K-2}r^{K+2}\right)r^{2}\right)\diff r}{-\left(K+2\right)t^{-K-3}}\\
 & = & -C_{3}\lim_{t\rightarrow+\infty}\int_{0}^{\delta_{2}t}r^{K+p+3}\exp\left(-\left(\frac{1}{4}-C_{3}t^{-K-2}r^{K+2}\right)r^{2}\right)\diff r=C_{2}.
\end{eqnarray*}

\end_inset

By L'Hostiple's rule, we have 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:lim-rhs-vs-lfs-inequality"

\end_inset

 holds, and therefore, the lemma holds.
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:central-expansion-post-prod"

\end_inset


\begin_inset Note Comment
status open

\begin_layout Plain Layout
need change the subscript of N, M, C
\end_layout

\end_inset

Under the assumption 
\begin_inset CommandInset ref
LatexCommand formatted
reference "enu:multv-diff-full-rank"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "enu:finite-moment-sample"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "enu:smooth-piror"

\end_inset

, there exist a 
\begin_inset Formula $\delta_{2}>0$
\end_inset

, a constant 
\begin_inset Formula $M_{2}$
\end_inset

 and 
\begin_inset Formula $N_{2}$
\end_inset

, such that 
\begin_inset Formula 
\begin{equation}
\left|\int_{\left\{ \left|B\left(\theta-\overline{X}\right)\right|\le\delta\right\} \cap H_{n}}\exp\left(-\frac{1}{2}Y^{T}Y+\sum_{k=3}^{K+3}\delta_{k}\hat{l}n^{-\frac{k-2}{2}}\right)\rho_{K}\left(\theta\right)-\prod_{i=1}^{n}\frac{\hat{w}_{i}\left(\theta\right)}{\hat{w}_{i}\left(\overline{X}\right)}\rho\left(\theta\right)\diff n^{-\frac{1}{2}}Y\right|\le M_{2}n^{-\frac{1}{2}\left(K+2\right)},\:\ascv.\label{eq:central-exp-post}
\end{equation}

\end_inset


\end_layout

\begin_layout Proof
Apply Taylor expansion to 
\begin_inset Formula $\hat{l}\left(\theta\right)$
\end_inset

 around 
\begin_inset Formula $\overline{X}$
\end_inset

, for any 
\begin_inset Formula $\theta\in H_{n}$
\end_inset

, there exists a 
\begin_inset Formula $\theta^{*}$
\end_inset

satisfies 
\begin_inset Formula $\left|B\left(\theta^{*}-\overline{X}\right)\right|\le\left|B\left(\theta-\overline{X}\right)\right|$
\end_inset

, such that, 
\begin_inset Formula 
\begin{eqnarray*}
\hat{l}\left(\theta\right) & = & \hat{l}\left(\overline{X}\right)+\nabla\hat{l}\left(\overline{X}\right)\left(\theta-\overline{X}\right)+\frac{1}{2}\left(\theta-\overline{X}\right)^{T}\frac{\partial^{2}\hat{l}\left(\overline{X}\right)}{\partial\theta^{T}\partial\theta}\left(\theta-\overline{X}\right)+\sum_{k=3}^{K+3}\frac{1}{k!}\left[\left(\theta-\overline{X}\right)^{T}\nabla\right]^{k}\hat{l}\left(\overline{X}\right)\\
 &  & +\frac{1}{\left(K+4\right)!}\left[\left(\theta-\overline{X}\right)\nabla\right]^{K+4}\hat{l}\left(\theta^{*}\right)\\
 & = & \hat{l}\left(\overline{X}\right)-\frac{1}{2}Y^{T}Yn^{-1}+\sum_{k=3}^{K+3}\delta_{k}\hat{l}n^{-\frac{k}{2}}++\frac{1}{\left(K+4\right)!}\left[\left(\theta-\overline{X}\right)\nabla\right]^{K+4}\hat{l}\left(\theta^{*}\right).
\end{eqnarray*}

\end_inset

Now we have 
\begin_inset Formula 
\begin{eqnarray*}
 &  & \left|\exp\left(-\frac{1}{2}Y^{T}Y+\sum_{k=3}^{K+3}\delta_{k}\hat{l}n^{-\frac{k-2}{2}}\right)\rho_{K}\left(\theta\right)-\prod_{i=1}^{n}\frac{\hat{w}_{i}\left(\theta\right)}{\hat{w}_{i}\left(\overline{X}\right)}\rho\left(\theta\right)\right|\\
 &  & \le\left|\exp\left(-\frac{1}{2}Y^{T}Y+\sum_{k=3}^{K+3}\delta_{k}\hat{l}n^{-\frac{k-2}{2}}\right)\rho_{K}\left(\theta\right)-\prod_{i=1}^{n}\frac{\hat{w}_{i}\left(\theta\right)}{\hat{w}_{i}\left(\overline{X}\right)}\rho_{K}\left(\theta\right)\right|\\
 &  & +\left|\prod_{i=1}^{n}\frac{\hat{w}_{i}\left(\theta\right)}{\hat{w}_{i}\left(\overline{X}\right)}\rho_{K}\left(\theta\right)-\prod_{i=1}^{n}\frac{\hat{w}_{i}\left(\theta\right)}{\hat{w}_{i}\left(\overline{X}\right)}\rho\left(\theta\right)\right|\\
 &  & \le\left|\rho_{K}\left(\theta\right)\right|\exp\left(n\left(\hat{l}\left(\theta\right)-\hat{l}\left(\overline{X}\right)\right)\right)\left|\exp\left(n\left(\hat{l}\left(\overline{X}\right)-\frac{1}{2}Y^{T}Yn^{-1}+\sum_{k=3}^{K+3}\delta_{k}\hat{l}n^{-\frac{k-2}{2}}-\hat{l}\left(\theta\right)\right)\right)-1\right|\\
 &  & +\exp\left(n\left(\hat{l}\left(\theta\right)-\hat{l}\left(\overline{X}\right)\right)\right)\left|\rho_{K}\left(\theta\right)-\rho\left(\theta\right)\right|.
\end{eqnarray*}

\end_inset

For the first part of the above formula, by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "lem:central-expansion-llik"

\end_inset

, we have it to be bounded by 
\begin_inset Formula 
\[
\max_{\theta\in\left\{ \left\{ B\left(\theta-\overline{X}\right)\le\delta_{2}\right\} \cap H_{n}\right\} }\rho_{K}\left(\theta\right)M_{1}n^{-\frac{1}{2}\left(K+3\right)}.
\]

\end_inset

For the second part, by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "lem:near-mean-2nd-order-bound"

\end_inset

, Taylor expansion in 
\begin_inset Formula $\rho\left(\theta\right)$
\end_inset

, and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:average-inequality"

\end_inset

, we have the upper bound to be 
\begin_inset Formula 
\begin{eqnarray*}
 &  & \int_{\left\{ Y^{T}Y\le\delta_{2}^{2}n\right\} }\exp\left(-\frac{Y^{T}Y}{4}\right)\frac{n^{-\frac{K+1}{2}}}{\left(K+1\right)!}\left(Y^{T}B^{-T}\nabla\right)^{K+1}\rho\left(\theta^{*}\right)\diff n^{-\frac{1}{2}}Y\\
 & \le & \frac{1}{\left(K+1\right)!}\max_{\theta\in\left\{ \left\{ B\left(\theta-\overline{X}\right)\le\delta_{2}\right\} \cap H_{n}\right\} }\left|\nabla^{K+1}\rho\left(\theta^{*}\right)\right|\int_{\left\{ Y^{T}Y\le\delta_{2}^{2}n\right\} }\exp\left(-\frac{Y^{T}Y}{4}\right)C_{1}^{\frac{K+1}{2}}\left(Y^{T}Y\right)^{\frac{K+1}{2}}\diff Yn^{-\frac{K+2}{2}}\\
 & \le & \frac{C_{1}^{\frac{K+1}{2}}}{\left(K+1\right)!}\max_{\theta\in\left\{ \left\{ B\left(\theta-\overline{X}\right)\le\delta_{2}\right\} \cap H_{n}\right\} }\left|\nabla^{K+1}\rho\left(\theta\right)\right|\int_{\mathbb{R}^{p}}\exp\left(-\frac{Y^{T}Y}{4}\right)\left(Y^{T}Y\right)^{\frac{K+1}{2}}\diff Yn^{-\frac{K+2}{2}}\\
 & = & \frac{C_{1}^{\frac{K+1}{2}}}{\left(K+1\right)!}\max_{\theta\in\left\{ \left\{ B\left(\theta-\overline{X}\right)\le\delta_{2}\right\} \cap H_{n}\right\} }\left|\nabla^{K+1}\rho\left(\theta\right)\right|2\pi^{p-1}\int_{0}^{\infty}\exp\left(-\frac{r^{2}}{4}\right)r^{K+1}r^{p-1}\diff rn^{-\frac{K+2}{2}}\\
 & = & \left[\frac{2\pi^{p-1}C_{1}^{\frac{K+1}{2}}}{\left(K+1\right)!}\max_{\theta\in\left\{ B\left(\theta-\overline{X}\right)\le\delta_{2}\right\} \cap H_{n}}\left|\nabla^{K+1}\rho\left(\theta\right)\right|\int_{0}^{\infty}r^{p+K}\exp\left(-\frac{r^{2}}{4}\right)\diff r\right]n^{-\frac{K+2}{2}}
\end{eqnarray*}

\end_inset

Since 
\begin_inset Formula $p\ge1$
\end_inset

, we have 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:central-exp-post"

\end_inset

 holds.
\end_layout

\begin_layout Section
proof of the main theorem
\end_layout

\begin_layout Standard
We first intuitively derive 
\begin_inset Note Comment
status open

\begin_layout Plain Layout
add expansion polynomial
\end_layout

\end_inset

.
 First, we expand 
\begin_inset Formula 
\begin{eqnarray*}
 &  & \exp\left(\sum_{k=3}^{K+3}\delta_{k}\hat{l}n^{-\frac{k-2}{2}}\right)\\
 & = & \sum_{i=0}^{K+1}\frac{1}{i!}\left(\sum_{k=3}^{K+3}\delta_{k}\hat{l}n^{-\frac{k-2}{2}}\right)^{i}\\
 & = & 1+\sum_{i=1}^{K+1}\frac{1}{i!}\sum_{\sum_{u=3}^{K+3}m_{u,i}=i}\binom{i}{m_{3,i},m_{4,i},\ldots,m_{K+3,i}}\prod_{u=3}^{K+3}\left(\delta_{u}\hat{l}\right)^{m_{u,i}}n^{-\frac{1}{2}\sum_{u=3}^{K+3}m_{u,i}\left(u-2\right)}.
\end{eqnarray*}

\end_inset

Then we product the above expansion by 
\begin_inset Formula $\rho_{K}$
\end_inset

, 
\begin_inset Formula 
\begin{eqnarray*}
 &  & \exp\left(\sum_{k=3}^{K+3}\frac{1}{k!}\delta_{k}\hat{l}n^{-\frac{k-2}{2}}\right)\rho_{K}\left(\theta\right)\\
 & = & \left[1+\sum_{i=1}^{K+1}\frac{1}{i!}\sum_{\sum_{u=3}^{K+3}m_{u,i}=i}\binom{i}{m_{3,i},m_{4,i},\ldots,m_{K+3,i}}\prod_{u=3}^{K+3}\left(\delta_{u}\hat{l}\right)^{m_{u,i}}n^{-\frac{1}{2}\sum_{u=3}^{K+3}m_{u,i}\left(u-2\right)}\right]\\
 &  & \left(\rho\left(\overline{X}\right)+\sum_{j=1}^{K}\delta_{j}\rho n^{-\frac{j}{2}}\right)\\
 & = & \rho\left(\overline{X}\right)+\sum_{j=1}^{K}\delta_{j}\rho n^{-\frac{j}{2}}+\rho\left(\overline{X}\right)\sum_{i=1}^{K+1}\frac{1}{i!}\\
 &  & \sum_{\sum_{u=3}^{K+3}m_{u,i}=i}\binom{i}{m_{3,i},m_{4,i},\ldots,m_{K+3,i}}\prod_{u=3}^{K+3}\left(\delta_{u}\hat{l}\right)^{m_{u,i}}n^{-\frac{1}{2}\sum_{u=3}^{K+3}m_{u,i}\left(u-2\right)}\\
 &  & +\left[\sum_{i=1}^{K+1}\frac{1}{i!}\sum_{\sum_{u=3}^{K+3}m_{u,i}=i}\binom{i}{m_{3,i},m_{4,i},\ldots,m_{K+3,i}}\prod_{u=3}^{K+3}\left(\delta_{u}\hat{l}\right)^{m_{u,i}}n^{-\frac{1}{2}\sum_{u=3}^{K+3}m_{u,i}\left(u-2\right)}\right]\sum_{j=1}^{K}\delta_{j}\rho n^{-\frac{j}{2}}.
\end{eqnarray*}

\end_inset

For the third term in above equation, we change the summation index.
 Let 
\begin_inset Formula $\sum_{u=3}^{K+3}m_{u,i}\left(u-2\right)=h$
\end_inset

.
 Note that for any 
\begin_inset Formula $\sum_{u=3}^{K+3}m_{u,i}=i$
\end_inset

, 
\begin_inset Formula $i\le h\le i\left(K+1\right)$
\end_inset

, 
\begin_inset Formula $h/\left(K+1\right)\le i\le h$
\end_inset

.
 Thus the third term summation can be rearranged as 
\begin_inset Formula 
\[
\sum_{h=1}^{\left(K+1\right)^{2}}\left[\rho\left(\overline{X}\right)\sum_{\frac{h}{K+1}\le i\le h}\frac{1}{i!}\sum_{I_{i,h}}\binom{i}{m_{3,i},m_{4,i},\ldots,m_{K+3,i}}\prod_{u=3}^{K+3}\left(\delta_{u}\hat{l}\right)^{m_{u,i}}\right]n^{-\frac{h}{2}}.
\]

\end_inset

Similarly for the fourth term, let 
\begin_inset Formula $\sum_{u=3}^{K+3}m_{u,i}\left(u-2\right)+j=h$
\end_inset

, then the summation can be rearranged as 
\begin_inset Formula 
\[
\sum_{h=2}^{\left(K+1\right)^{2}+K}\left[\sum_{j=1}^{h-1}\delta_{j}\rho\sum_{\frac{h-j}{K+1}\le i\le h-j}\frac{1}{i!}\sum_{I_{i,h-j}}\binom{i}{m_{3,i},m_{4,i},\ldots,m_{K+3,i}}\prod_{u=3}^{K+3}\left(\delta_{u}\hat{l}\right)^{m_{u,i}}\right]n^{-\frac{h}{2}}.
\]

\end_inset

We collect the same order term of 
\begin_inset Formula $n$
\end_inset

, and denote the summation of all the terms with order higher than 
\begin_inset Formula $K$
\end_inset

 to be 
\begin_inset Formula $R_{K}\left(Y\right)$
\end_inset

, then we get the product as 
\begin_inset Formula 
\begin{eqnarray*}
 &  & \rho\left(\overline{X}\right)+\left(\delta_{1}\rho+\rho\left(\overline{X}\right)\delta_{3}\hat{l}\right)n^{-\frac{1}{2}}\\
 &  & +\sum_{h=2}^{K}\left[\delta_{h}\rho+\sum_{j=0}^{h-1}\delta_{j}\rho\sum_{\frac{h-j}{K+1}\le i\le h-j}\frac{1}{i!}\sum_{I_{i,h-j}}\binom{i}{m_{3,i},m_{4,i},\ldots,m_{K+3,i}}\prod_{u=3}^{K+3}\left(\delta_{u}\hat{l}\right)^{m_{u,i}}\right]n^{-\frac{h}{2}}+R_{K}\left(Y\right).
\end{eqnarray*}

\end_inset

 Integral over any Borel set 
\begin_inset Formula $A\cap H_{n}$
\end_inset

, we can get the polynomial 
\begin_inset Formula $P_{K}\left(A,n\right)$
\end_inset

.
 Now we can prove the main theorem 
\begin_inset Note Comment
status open

\begin_layout Plain Layout
add ref to main theorem
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $A_{1}=\left\{ \left|Y\right|_{2}\ge\delta_{2}\sqrt{n}\right\} $
\end_inset

 and 
\begin_inset Formula $A_{2}=\left\{ \left|Y\right|_{2}<\delta_{2}\sqrt{n}\right\} $
\end_inset

.
 Then 
\begin_inset Formula 
\begin{eqnarray*}
 &  & \left|\int_{A\cap H_{n}}\prod_{i=1}^{n}\frac{\hat{w}_{i}\left(\theta\right)}{\hat{w}_{i}\left(\overline{X}\right)}\rho\left(\theta\right)\diff n^{-\frac{1}{2}}Y-P_{K}\left(A,n\right)\right|\\
 & = & \left|\int_{A\cap H_{n}}\prod_{i=1}^{n}\frac{\hat{w}_{i}\left(\theta\right)}{\hat{w}_{i}\left(\overline{X}\right)}\rho\left(\theta\right)-\exp\left(-\frac{Y^{T}Y}{2}\right)\sum_{h=0}^{K}\alpha_{h}\left(Y,n\right)n^{-\frac{h}{2}}\diff n^{-\frac{1}{2}}Y\right|\\
 & \le & \left|\int_{A\cap H_{n}\cap A_{1}}\prod_{i=1}^{n}\frac{\hat{w}_{i}\left(\theta\right)}{\hat{w}_{i}\left(\overline{X}\right)}\rho\left(\theta\right)-\exp\left(-\frac{Y^{T}Y}{2}\right)\sum_{h=0}^{K}\alpha_{h}\left(Y,n\right)n^{-\frac{h}{2}}\diff n^{-\frac{1}{2}}Y\right|\\
 &  & +\left|\int_{A\cap H_{n}\cap A_{2}}\prod_{i=1}^{n}\frac{\hat{w}_{i}\left(\theta\right)}{\hat{w}_{i}\left(\overline{X}\right)}\rho\left(\theta\right)-\exp\left(-\frac{Y^{T}Y}{2}\right)\sum_{h=0}^{K}\alpha_{h}\left(Y,n\right)n^{-\frac{h}{2}}\diff n^{-\frac{1}{2}}Y\right|.
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Proof
For the first term
\begin_inset Note Comment
status open

\begin_layout Plain Layout
change the first second term ref into ref eq
\end_layout

\end_inset

, by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "lem:exponential-decay-tail"

\end_inset

, we have
\begin_inset Formula 
\begin{eqnarray*}
 &  & \left|\int_{A\cap H_{n}\cap A_{1}}\prod_{i=1}^{n}\frac{\hat{w}_{i}\left(\theta\right)}{\hat{w}_{i}\left(\overline{X}\right)}\rho\left(\theta\right)-\exp\left(-\frac{Y^{T}Y}{2}\right)\sum_{h=0}^{K}\alpha_{h}\left(Y,n\right)n^{-\frac{h}{2}}\diff n^{-\frac{1}{2}}Y\right|\\
 & \le & \int_{A\cap H_{n}\cap A_{1}}\exp\left(n\left(\hat{l}\left(\theta\right)-\hat{l}\left(\overline{X}\right)\right)\right)\rho\left(\theta\right)\diff n^{-\frac{1}{2}}Y\\
 &  & +\left|\int_{A\cap H_{n}\cap A_{1}}\exp\left(-\frac{Y^{T}Y}{4}-\frac{Y^{T}Y}{4}\right)\sum_{h=0}^{K}\alpha_{h}\left(Y,n\right)n^{-\frac{h}{2}}\diff n^{-\frac{1}{2}}Y\right|\\
 & \le & \exp\left(-n\varepsilon\right)\int_{A\cap H_{n}\cap A_{1}}\rho\left(\theta\right)\diff B\left(\theta-\overline{X}\right)\\
 &  & +\exp\left(-\frac{\delta_{2}^{2}n}{4}\right)\left|\int_{A\cap H_{n}\cap A_{1}}\exp\left(-\frac{Y^{T}Y}{4}\right)\sum_{h=0}^{K}\alpha_{h}\left(Y,n\right)n^{-\frac{h}{2}}\diff n^{-\frac{1}{2}}Y\right|\\
 & \le & \exp\left(-n\varepsilon\right)\int_{\mathbb{R}}\rho\left(\theta\right)\diff B\left(\theta-\overline{X}\right)+\exp\left(-n\frac{\delta_{2}^{2}}{4}\right)\sum_{h=0}^{K}\left(\int_{A\cap H_{n}}\exp\left(-\frac{Y^{T}Y}{4}\right)\left|\alpha_{h}\left(Y,n\right)\right|\diff Y\right)n^{-\frac{h+1}{2}}.
\end{eqnarray*}

\end_inset

Note that the above terms are exponentially decreasing with respect to 
\begin_inset Formula $n$
\end_inset

, so there exists an 
\begin_inset Formula $N_{3}$
\end_inset

, and 
\begin_inset Formula $M_{3}$
\end_inset

, such that for any 
\begin_inset Formula $n\ge N_{3}$
\end_inset

, 
\begin_inset Formula 
\[
\left|\int_{A\cap H_{n}\cap A_{1}}\prod_{i=1}^{n}\frac{\hat{w}_{i}\left(\theta\right)}{\hat{w}_{i}\left(\overline{X}\right)}\rho\left(\theta\right)-\exp\left(-\frac{Y^{T}Y}{2}\right)\sum_{h=0}^{K}\alpha_{h}\left(Y,n\right)n^{-\frac{h}{2}}\diff n^{-\frac{1}{2}}Y\right|\le M_{3}n^{-\frac{K+2}{2}}.
\]

\end_inset


\end_layout

\begin_layout Proof
For the second term, by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "lem:central-expansion-post-prod"

\end_inset

, we have 
\begin_inset Formula 
\begin{eqnarray*}
 &  & \left|\int_{A\cap H_{n}\cap A_{2}}\prod_{i=1}^{n}\frac{\hat{w}_{i}\left(\theta\right)}{\hat{w}_{i}\left(\overline{X}\right)}\rho\left(\theta\right)-\exp\left(-\frac{Y^{T}Y}{2}\right)\sum_{h=0}^{K}\alpha_{h}\left(Y,n\right)n^{-\frac{h}{2}}\diff n^{-\frac{1}{2}}Y\right|\\
 & \le & \left|\int_{A\cap H_{n}\cap A_{2}}\prod_{i=1}^{n}\frac{\hat{w}_{i}\left(\theta\right)}{\hat{w}_{i}\left(\overline{X}\right)}\rho\left(\theta\right)-\exp\left(-\frac{1}{2}Y^{T}Y+\sum_{k=3}^{K+3}\delta_{k}\hat{l}n^{-\frac{k-2}{2}}\right)\rho_{K}\left(\theta\right)\diff n^{-\frac{1}{2}}Y\right|\\
 &  & +\left|\int_{A\cap H_{n}\cap A_{2}}\exp\left(-\frac{1}{2}Y^{T}Y+\sum_{k=3}^{K+3}\delta_{k}\hat{l}n^{-\frac{k-2}{2}}\right)\rho_{K}\left(\theta\right)-\exp\left(-\frac{Y^{T}Y}{2}\right)\sum_{h=0}^{K}\alpha_{h}\left(Y,n\right)n^{-\frac{h}{2}}\diff n^{-\frac{1}{2}}Y\right|\\
 & \le & M_{2}n^{-\frac{K+2}{2}}\\
 &  & +\left|\int_{A\cap H_{n}\cap A_{2}}\exp\left(-\frac{1}{2}Y^{T}Y+\sum_{k=3}^{K+3}\delta_{k}\hat{l}n^{-\frac{k-2}{2}}\right)\rho_{K}\left(\theta\right)-\exp\left(-\frac{Y^{T}Y}{2}\right)\sum_{h=0}^{K}\alpha_{h}\left(Y,n\right)n^{-\frac{h}{2}}\diff n^{-\frac{1}{2}}Y\right|.
\end{eqnarray*}

\end_inset

For the second term of above, we add and subtract 
\begin_inset Formula $R_{K}\left(Y\right)$
\end_inset

 in integrand, and by Taylor expansion, 
\begin_inset Formula 
\begin{eqnarray*}
 &  & \left|\int_{A\cap H_{n}\cap A_{2}}\exp\left(-\frac{1}{2}Y^{T}Y+\sum_{k=3}^{K+3}\delta_{k}\hat{l}n^{-\frac{k-2}{2}}\right)\rho_{K}\left(\theta\right)-\exp\left(-\frac{Y^{T}Y}{2}\right)\sum_{h=0}^{K}\alpha_{h}\left(Y,n\right)n^{-\frac{h}{2}}\diff n^{-\frac{1}{2}}Y\right|\\
 & \le & \left|\int_{A\cap H_{n}\cap A_{2}}\exp\left(-\frac{Y^{T}Y}{2}\right)\rho_{K}\left(\theta\right)\left[\exp\left(\sum_{k=3}^{K+3}\delta_{k}\hat{l}n^{-\frac{k-2}{2}}\right)-\sum_{i=0}^{K+1}\frac{1}{i!}\left(\sum_{k=3}^{K+3}\delta_{k}\hat{l}n^{-\frac{k-2}{2}}\right)^{i}\right]\diff n^{-\frac{1}{2}}Y\right|\\
 &  & +\left|\int_{A\cap H_{n}\cap A_{2}}\exp\left(-\frac{Y^{T}Y}{2}\right)R_{K}\left(Y\right)\diff n^{-\frac{1}{2}}Y\right|\\
 & = & \left|\int_{A\cap H_{n}\cap A_{2}}\exp\left(-\frac{Y^{T}Y}{2}\right)\rho_{K}\left(\theta\right)\frac{1}{\left(K+2\right)!}\exp\left(L\right)\left(\sum_{k=3}^{K+3}\delta_{k}\hat{l}n^{-\frac{k-2}{2}}\right)^{K+2}\diff n^{-\frac{1}{2}}Y\right|\\
 &  & +\left|\int_{A\cap H_{n}\cap A_{2}}\exp\left(-\frac{Y^{T}Y}{2}\right)R_{K}\left(Y\right)\diff n^{-\frac{1}{2}}Y\right|,
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $\left|L\right|\le\left|\sum_{k=3}^{K+3}\delta_{k}\hat{l}n^{-\frac{k-2}{2}}\right|$
\end_inset

.
 We know that 
\begin_inset Formula $R_{K}\left(Y\right)$
\end_inset

 is a polynomial with order 
\begin_inset Formula $n^{-\frac{1}{2}\left(K+1\right)}$
\end_inset

, so there exists an 
\begin_inset Formula $M_{3}$
\end_inset

, such that 
\begin_inset Formula 
\[
\left|\int_{A\cap H_{n}\cap A_{2}}\exp\left(-\frac{Y^{T}Y}{2}\right)R_{K}\left(Y\right)\diff n^{-\frac{1}{2}}Y\right|\le M_{3}n^{-\frac{1}{2}\left(K+2\right)}.
\]

\end_inset

For the first term, 
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{eqnarray}
 &  & \left|\int_{A\cap H_{n}\cap A_{2}}\exp\left(-\frac{Y^{T}Y}{2}\right)\rho_{K}\left(\theta\right)\frac{1}{\left(K+2\right)!}\exp\left(L\right)\left(\sum_{k=3}^{K+3}\delta_{k}\hat{l}n^{-\frac{k-2}{2}}\right)^{K+2}\diff n^{-\frac{1}{2}}Y\right|\label{eq:1}\\
 & \le & \frac{1}{\left(K+2\right)!}\left|\int_{A\cap H_{n}\cap A_{2}}\exp\left(-\frac{Y^{T}Y}{2}\right)\rho_{K}\left(\theta\right)\exp\left(\left|\sum_{k=3}^{K+3}\delta_{k}\hat{l}n^{-\frac{k-2}{2}}\right|\right)\left(\sum_{k=3}^{K+3}\delta_{k}\hat{l}n^{-\frac{k-2}{2}}\right)^{K+2}\diff n^{-\frac{1}{2}}Y\right|\nonumber \\
 & = & \frac{1}{\left(K+2\right)!}\Bigg|\int_{A\cap H_{n}\cap A_{2}}\rho_{K}\left(\theta\right)\exp\left(-n\left\{ \frac{\left(\theta-\overline{X}\right)^{T}B^{2}\left(\theta-\overline{X}\right)}{2}-\left|\sum_{k=3}^{K+3}\frac{\left[\left(\theta-\overline{X}\right)^{T}\nabla\right]^{k}\hat{l}}{k!}\right|\right\} \right)\nonumber \\
 &  & \left\{ n\sum_{k=3}^{K+3}\frac{\left[\left(\theta-\overline{X}\right)^{T}\nabla\right]^{k}\hat{l}}{k!}\right\} ^{K+2}\diff B\left(\theta-\overline{X}\right)\Bigg|.\nonumber 
\end{eqnarray}

\end_inset

We need 
\begin_inset Formula $\delta_{2}$
\end_inset

sufficiently small, so that there exist an 
\begin_inset Formula $C_{4}$
\end_inset

 and 
\begin_inset Formula $C_{5}$
\end_inset

, such that 
\begin_inset Formula 
\begin{eqnarray*}
\frac{\left(\theta-\overline{X}\right)^{T}B^{2}\left(\theta-\overline{X}\right)}{2}-\left|\sum_{k=3}^{K+3}\frac{\left[\left(\theta-\overline{X}\right)^{T}\nabla\right]^{k}\hat{l}}{k!}\right| & \ge & C_{4}\left(\theta-\overline{X}\right)^{T}B^{2}\left(\theta-\overline{X}\right),\\
\sum_{k=3}^{K+3}\frac{\left[\left(\theta-\overline{X}\right)^{T}\nabla\right]^{k}\hat{l}}{k!} & \le & C_{5}\left[\left(\theta-\overline{X}\right)^{T}\nabla\right]^{3}\hat{l}.
\end{eqnarray*}

\end_inset

Hence, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:1"

\end_inset

 can be bounded by 
\begin_inset Formula 
\begin{eqnarray*}
 &  & \frac{n^{K+2}}{\left(K+2\right)!}\left|\int_{A\cap H_{n}\cap A_{n}}\exp\left(-nC_{4}\left(\theta-\overline{X}\right)^{T}B^{2}\left(\theta-\overline{X}\right)\right)\left\{ C_{5}\left[\left(\theta-\overline{X}\right)^{T}\nabla\right]^{3}\hat{l}\right\} ^{K+2}\diff B\left(\theta-\overline{X}\right)\right|\\
 & \le & \frac{C_{5}^{K+2}n^{K+2}}{\left(K+2\right)!}\left|\int_{A\cap H_{n}}\exp\left(-C_{4}Y^{T}Y\right)\left(\delta_{3}\hat{l}\right)^{K+2}n^{-\frac{3\left(K+2\right)}{2}}\diff n^{-\frac{1}{2}}Y\right|\\
 & \le & \frac{C_{5}^{K+2}}{\left(K+2\right)!}\left|\int_{A\cap H_{n}}\exp\left(-C_{4}Y^{T}Y\right)\left(\delta_{3}\hat{l}\right)^{K+2}\diff Y\right|n^{-\frac{K+3}{2}}.
\end{eqnarray*}

\end_inset

Add all the parts together, we get the inequality in 
\begin_inset Note Comment
status open

\begin_layout Plain Layout
add ref to main theorem
\end_layout

\end_inset

.
\end_layout

\begin_layout Chapter
Proof of Approximate Bayesian Computation via Sufficient Dimension Reduction
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:proof-thm-1"

\end_inset

Proof of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:bernstein-von-mises-mle"

\end_inset


\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{eqnarray*}
 &  & P\left(\sqrt{n\hat{I}}\left(\theta-\hat{\theta}\right)\le t\mid\hat{\theta}\in O\left(\theta_{0},\varepsilon\right)\right)\\
 & = & \frac{P\left(\sqrt{n\hat{I}}\left(\theta-\hat{\theta}\right)\le t,\hat{\theta}\in O\left(\theta_{0},\varepsilon\right)\right)}{P\left(\hat{\theta}\in O\left(\theta_{0},\varepsilon\right)\right)}\\
 & = & \frac{\int I_{\left[\hat{\theta}\in O\left(\theta_{0},\varepsilon\right)\right]}I_{\left[\sqrt{n\hat{I}}\left(\theta-\hat{\theta}\right)\le t\right]}\prod_{i=1}^{n}\pi\left(X_{i}\mid\theta\right)\pi\left(\theta\right)\diff X_{i}\diff\theta}{\int I_{\left[\hat{\theta}\in O\left(\theta_{0},\varepsilon\right)\right]}\prod_{i=1}^{n}\pi\left(X_{i}\mid\theta\right)\pi\left(\theta\right)\diff X_{i}\diff\theta}.
\end{eqnarray*}

\end_inset

Let 
\begin_inset Formula $P^{\infty}\left(\theta\right)$
\end_inset

 be the probability measure on infinite independent and identically distributed
 sequence 
\begin_inset Formula $X_{1},\ldots,X_{n},\ldots,$
\end_inset

.
 Then 
\begin_inset Formula 
\begin{eqnarray}
 &  & P\left(\sqrt{n\hat{I}}\left(\theta-\hat{\theta}\right)\le t\mid\hat{\theta}\in O\left(\theta_{0},\varepsilon\right)\right)\nonumber \\
 & = & \frac{E_{\pi\left(\theta\right)}E_{P^{\infty}\left(\theta_{0}\right)}I_{\left[\hat{\theta}\in O\left(\theta_{0},\varepsilon\right)\right]}I_{\left[\sqrt{n\hat{I}}\left(\theta-\hat{\theta}\right)\le t\right]}\exp\left(\sum_{i=1}^{n}\log\pi\left(X_{i}\mid\theta\right)-\log\pi\left(X_{i}\mid\theta_{0}\right)\right)}{E_{\pi\left(\theta\right)}E_{P^{\infty}\left(\theta_{0}\right)}I_{\left[\hat{\theta}\in O\left(\theta_{0},\varepsilon\right)\right]}\exp\left(\sum_{i=1}^{n}\log\pi\left(X_{i}\mid\theta\right)-\log\pi\left(X_{i}\mid\theta_{0}\right)\right)}.\label{eq:post-change-measure}
\end{eqnarray}

\end_inset

By strong consistency of the maximum likelihood estimator, for any 
\begin_inset Formula $\varepsilon>0$
\end_inset

, there is an 
\begin_inset Formula $N>0$
\end_inset

, such that for any 
\begin_inset Formula $n>N$
\end_inset

, 
\begin_inset Formula $P\left(\hat{\theta}\in O\left(\theta_{0},\varepsilon\right)\mid\theta_{0}\right)=1$
\end_inset

.
 Thus, we can drop the indicator 
\begin_inset Formula $I_{\left[\hat{\theta}\in O\left(\theta_{0},\varepsilon\right)\right]}$
\end_inset

 without changing the value in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:post-change-measure"

\end_inset

.
 We can change the order of integration.
 So the numerator of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:post-change-measure"

\end_inset

 is 
\begin_inset Formula 
\begin{eqnarray*}
 &  & E_{P^{\infty}\left(\theta_{0}\right)}\left(\prod_{i=1}^{n}\pi\left(X_{i}\mid\theta_{0}\right)\right)^{-1}E_{\pi\left(\theta\right)}I_{\left[\sqrt{n\hat{I}}\left(\theta-\hat{\theta}\right)\le t\right]}\prod_{i=1}^{n}\pi\left(X_{i}\mid\theta\right)\\
 & = & E_{P^{\infty}\left(\theta_{0}\right)}\left(\prod_{i=1}^{n}\pi\left(X_{i}\mid\theta_{0}\right)\right)^{-1}\int I_{\left[\sqrt{n\hat{I}}\left(\theta-\hat{\theta}\right)\le t\right]}\prod_{i=1}^{n}\pi\left(X_{i}\mid\theta\right)\pi\left(\theta\right)\diff\theta\\
 & = & E_{P^{\infty}\left(\theta_{0}\right)}\left(\prod_{i=1}^{n}\pi\left(X_{i}\mid\theta_{0}\right)\right)^{-1}P\left(\sqrt{n\hat{I}}\left(\theta-\hat{\theta}\right)\le t\mid X_{1},\ldots,X_{n}\right)E_{\pi\left(\theta\right)}\prod_{i=1}^{n}\pi\left(X_{i}\mid\theta\right).
\end{eqnarray*}

\end_inset

By Bernstein--von Mises theorem, 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}P\left(\sqrt{n\hat{I}}\left(\theta-\hat{\theta}\right)\le t\mid X_{1},\ldots,X_{n}\right)=\Phi\left(t\right),\ascv P^{\infty}\left(\theta_{0}\right).
\]

\end_inset

Hence, the result holds.
\end_layout

\begin_layout Standard
To prove a similar result about conditioning on posterior mean, we go through
 similar steps as in the proof of Theorem 
\begin_inset CommandInset ref
LatexCommand eqref
reference "thm:bernstein-von-mises-mle"

\end_inset

.
 The only needed change is to prove 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}P\left(\sqrt{n\hat{I}}\left(\theta-E\left(\theta\mid X_{1},\ldots,X_{n}\right)\right)\le t\mid X_{1},\ldots,X_{n}\right)=\Phi\left(t\right),\ascv P^{\infty}\left(\theta_{0}\right).
\]

\end_inset

We know from 
\begin_inset CommandInset citation
LatexCommand citet
key "ghosh2011moment"

\end_inset

 that with probability 1, 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:high-order-close-post-mean-mle"

\end_inset

 holds.
 By conditioning on 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

, both posterior mean and maximum likelihood estimator are fixed numbers
 and 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\sqrt{n}\left(E\left(\theta\mid X_{1},\ldots,X_{n}\right)-\hat{\theta}\right)=0.\ascv.
\]

\end_inset

 Hence,
\color red
 
\color black
if we assume the CDF of the full posterior is continuous and asymptotically
 normal, then 
\begin_inset Formula 
\begin{eqnarray*}
 &  & \left|P\left(\sqrt{n\hat{I}}\left(\theta-E\left(\theta\mid X_{1},\ldots,X_{n}\right)\right)\le t\mid X_{1},\ldots,X_{n}\right)-\Phi\left(t\right)\right|\\
 & \le & \Bigg|P\left(\sqrt{n\hat{I}}\left(\theta-\hat{\theta}\right)+\hat{I}^{1/2}\sqrt{n}\left(E\left(\theta\mid X_{1},\ldots,X_{n}\right)-\hat{\theta}\right)\le t\mid X_{1},\ldots,X_{n}\right)\\
 &  & -P\left(\sqrt{n\hat{I}}\left(\theta-\hat{\theta}\right)\le t\mid X_{1},\ldots,X_{n}\right)\Bigg|+\left|P\left(\sqrt{n\hat{I}}\left(\theta-\hat{\theta}\right)\le t\mid X_{1},\ldots,X_{n}\right)-\Phi\left(t\right)\right|\\
 & \rightarrow & 0,\mathrm{\: as\:}\left(n\rightarrow\infty\right).
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Section
Derivation of Examples
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "sub:Derivation-of-Example-1"

\end_inset

Derivation of Example 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:Immigrate-emigrate-process"

\end_inset


\end_layout

\begin_layout Standard
First, we get the distribution of 
\begin_inset Formula $\mu$
\end_inset

.
 
\begin_inset Formula 
\[
\mu^{r_{2}}\exp\left(-\mu\frac{r_{2}}{\hat{\mu}}\right)\left|-\frac{r_{2}}{\left(\hat{\mu}\right)^{2}}\right|\propto r_{2}\mu^{r_{2}}\exp\left(-\frac{\mu}{\hat{\mu}}r_{2}\right).
\]

\end_inset

Now we sum out 
\begin_inset Formula $r_{2}$
\end_inset

.
 By definition 
\begin_inset Formula $X_{0}+r_{1}-r_{2}\ge0$
\end_inset

.
 Hence the distribution of 
\begin_inset Formula $\hat{\mu}$
\end_inset

 is proportional to 
\begin_inset Formula 
\[
\sum_{r_{2}=0}^{X_{0}+r_{1}}r_{2}\left(\mu\exp\left(-\frac{\mu}{\hat{\mu}}\right)\right)^{r_{2}}.
\]

\end_inset

Let 
\begin_inset Formula $U=\mu\exp\left(-\mu/\hat{\mu}\right)$
\end_inset

 and 
\begin_inset Formula $R=X_{0}+r_{1}=X_{0}+\hat{\lambda}T$
\end_inset

.
 Let 
\begin_inset Formula 
\begin{eqnarray*}
L & = & \sum_{r_{2}=0}^{R}r_{2}U^{r_{2}}=U\frac{\diff}{\diff U}\sum_{r_{2}=0}^{R}U^{r_{2}}=U\frac{\diff}{\diff U}\left(\frac{1-U^{R+1}}{1-U}\right)\\
 & = & U\left(1-U\right)^{-2}\left[1-\left(R+1\right)U^{R}+RU^{R+1}\right].
\end{eqnarray*}

\end_inset

For fixed 
\begin_inset Formula $t$
\end_inset

, consider 
\begin_inset Formula $\mu=\hat{\mu}+t/\sqrt{T}$
\end_inset

.
 
\begin_inset Formula 
\begin{eqnarray*}
\log U & = & -\frac{\mu}{\hat{\mu}}+\log\mu=-1-\frac{t}{\sqrt{T}\hat{\mu}}+\log\hat{\mu}+\log\left(1+\frac{t}{\sqrt{T}\hat{\mu}}\right)\\
 & = & \log\hat{\mu}-1-\frac{t^{2}}{2\left(\hat{\mu}\right)^{2}T}+o\left(T^{-1}\right).
\end{eqnarray*}

\end_inset


\color black
Hence 
\begin_inset Formula 
\[
\lim_{T\rightarrow\infty}U=\frac{\mu_{0}}{e},\ascv.
\]

\end_inset

the limit is a constant.
 
\begin_inset Formula 
\[
R\log U=\left(X_{0}+\hat{\lambda}T\right)\left(\log\hat{\mu}-1\right)-\frac{\hat{\lambda}t^{2}}{2\left(\hat{\mu}\right)^{2}}+o\left(1\right).
\]

\end_inset


\begin_inset Formula 
\[
L=\frac{UR}{\left(1-U\right)^{2}}\left(\frac{1}{R}-\frac{R+1}{R}\exp\left(R\log U\right)+U\exp\left(R\log U\right)\right)
\]

\end_inset

Note that the density of 
\begin_inset Formula $t=\sqrt{T}\left(\mu-\hat{\mu}\right)$
\end_inset

 is only proportional to 
\begin_inset Formula $L$
\end_inset

, hence only the terms containing 
\begin_inset Formula $t$
\end_inset

 will affect the limit distribution, other terms can be omitted.
 Also recall that, 
\begin_inset Formula 
\[
\lim_{T\rightarrow\infty}\frac{1}{R}=\lim_{T\rightarrow\infty}\frac{1}{X_{0}+\hat{\lambda}T}=0,
\]

\end_inset

we have 
\begin_inset Formula 
\begin{eqnarray*}
\lim_{T\rightarrow\infty}L & \propto & \lim_{T\rightarrow\infty}\left(\frac{1}{R}-\frac{R+1}{R}\exp\left(R\log U\right)+U\exp\left(R\log U\right)\right)\\
 & = & \lim_{T\rightarrow\infty}\left(U-1-\frac{1}{R}\right)\exp\left(\left(X_{0}+\hat{\lambda}T\right)\left(\log\hat{\mu}-1\right)-\frac{\hat{\lambda}t^{2}}{2\left(\hat{\mu}\right)^{2}}+o\left(1\right)\right)\\
 & = & \lim_{T\rightarrow\infty}\left(\frac{\mu_{0}}{e}-1\right)\exp\left(\left(X_{0}+\hat{\lambda}T\right)\left(\log\hat{\mu}-1\right)\right)\exp\left(-\frac{\hat{\lambda}t^{2}}{2\left(\hat{\mu}\right)^{2}}+o\left(1\right)\right)\\
 & \propto & \exp\left(-\frac{\hat{\lambda}t^{2}}{2\left(\hat{\mu}\right)^{2}}\right).
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "sub:Derivation-of-Example-2"

\end_inset

Derivation of Example 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:Gamma-distribution"

\end_inset


\end_layout

\begin_layout Standard
We know 
\begin_inset Formula $\overline{X}\sim\mathrm{Gamma}\left(n\alpha,\beta/n\right),$
\end_inset

 so 
\begin_inset Formula $\tilde{\alpha}\sim\mathrm{Gamma}\left(n\alpha,n^{-1}\right).$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
\pi\left(\alpha\mid\tilde{\alpha}\right) & \propto & \pi\left(\tilde{\alpha}\mid\alpha\right)\pi\left(\alpha\right)=\frac{1}{\Gamma\left(n\alpha\right)n^{-n\alpha}}\left(\tilde{\alpha}\right)^{n\alpha-1}\exp\left(-n\tilde{\alpha}\right)\exp\left(-\lambda\alpha\right)\\
 & \propto & \frac{\left(n\tilde{\alpha}\exp\left(-\lambda/n\right)\right)^{n\alpha}}{\Gamma\left(n\alpha\right)}.
\end{eqnarray*}

\end_inset

Next we will show 
\begin_inset Formula $\pi\left(\sqrt{n}\left(\alpha-\tilde{\alpha}\right)/b\mid\tilde{\alpha}\right)\rightarrow N\left(0,1\right)\ascv,$
\end_inset

 for some suitable 
\begin_inset Formula $b$
\end_inset

.
 The PDF of 
\begin_inset Formula $t$
\end_inset

 is proportional to 
\begin_inset Formula 
\[
\frac{\left(n\tilde{\alpha}\exp\left(-\lambda/n\right)\right)^{n\left(bt/\sqrt{n}+\tilde{\alpha}\right)}}{\Gamma\left(n\left(bt/\sqrt{n}+\tilde{\alpha}\right)\right)}\frac{b}{\sqrt{n}}.
\]

\end_inset

Take logarithm, and drop all the terms not related to 
\begin_inset Formula $t$
\end_inset

, since those terms can be divided from both numerator and denominator,
 
\begin_inset Formula 
\begin{equation}
\sqrt{n}bt\log\left(n\tilde{\alpha}\right)-\lambda b\frac{t}{\sqrt{n}}-\log\Gamma\left(n\tilde{\alpha}\left(1+\frac{bt}{\sqrt{n}\tilde{\alpha}}\right)\right).\label{eq:log-density-drop-not-have-t}
\end{equation}

\end_inset

Using Stirling formula to approximate gamma function, 
\begin_inset Formula 
\begin{eqnarray*}
 &  & \log\Gamma\left(n\tilde{\alpha}\left(1+\frac{bt}{\sqrt{n}\tilde{\alpha}}\right)\right)\\
 & \approx & \left[n\tilde{\alpha}\left(1+\frac{bt}{\sqrt{n}\tilde{\alpha}}\right)-\frac{1}{2}\right]\log\left(n\tilde{\alpha}\left(1+\frac{bt}{\sqrt{n}\tilde{\alpha}}\right)\right)-n\tilde{\alpha}\left(1+\frac{bt}{\sqrt{n}\tilde{\alpha}}\right)+\frac{1}{2}\log2\pi.
\end{eqnarray*}

\end_inset

So 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:log-density-drop-not-have-t"

\end_inset

 can be written as 
\begin_inset Formula 
\begin{eqnarray}
 &  & \sqrt{n}bt\log\left(n\tilde{\alpha}\right)-\lambda b\frac{t}{\sqrt{n}}-\sqrt{n}bt\log\left(1+\frac{bt}{\sqrt{n}\tilde{\alpha}}\right)\nonumber \\
 &  & -\sqrt{n}bt\log\left(n\tilde{\alpha}\right)-\left(n\tilde{\alpha}-\frac{1}{2}\right)\log\left(1+\frac{bt}{\sqrt{n}\tilde{\alpha}}\right)+\sqrt{n}bt\nonumber \\
 & = & -\sqrt{n}bt\log\left(1+\frac{bt}{\sqrt{n}\tilde{\alpha}}\right)-\left(n\tilde{\alpha}-\frac{1}{2}\right)\log\left(1+\frac{bt}{\sqrt{n}\tilde{\alpha}}\right)+\sqrt{n}bt-\lambda b\frac{t}{\sqrt{n}}.\label{eq:log-density-after-stirling}
\end{eqnarray}

\end_inset

Now we can apply Taylor expansion for term 
\begin_inset Formula $\log\left(1+bt/\left(\sqrt{n}\tilde{\alpha}\right)\right)$
\end_inset

,
\begin_inset Formula 
\[
\log\left(1+\frac{bt}{\sqrt{n}\tilde{\alpha}}\right)=\frac{bt}{\sqrt{n}\tilde{\alpha}}-\frac{b^{2}t^{2}}{2n\tilde{\alpha}^{2}}+\frac{b^{3}t^{3}}{3n^{3/2}\tilde{\alpha}^{3}}+o\left(\frac{t^{3}}{n^{3/2}}\right).
\]

\end_inset

Substituting the expansion into 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:log-density-after-stirling"

\end_inset

,
\begin_inset Formula 
\begin{eqnarray*}
 &  & -\sqrt{n}bt\log\left(1+\frac{bt}{\sqrt{n}\tilde{\alpha}}\right)-\left(n\tilde{\alpha}-\frac{1}{2}\right)\log\left(1+\frac{bt}{\sqrt{n}\tilde{\alpha}}\right)+\sqrt{n}bt-\lambda b\frac{t}{\sqrt{n}}\\
 & = & -\sqrt{n}bt\left(\frac{bt}{\sqrt{n}\tilde{\alpha}}-\frac{b^{2}t^{2}}{2n\tilde{\alpha}^{2}}+\frac{b^{3}t^{3}}{3n^{3/2}\tilde{\alpha}^{3}}+o\left(\frac{t^{3}}{n^{3/2}}\right)\right)\\
 &  & -\left(n\tilde{\alpha}-\frac{1}{2}\right)\left(\frac{bt}{\sqrt{n}\tilde{\alpha}}-\frac{b^{2}t^{2}}{2n\tilde{\alpha}^{2}}+\frac{b^{3}t^{3}}{3n^{3/2}\tilde{\alpha}^{3}}+o\left(\frac{t^{3}}{n^{3/2}}\right)\right)+\sqrt{n}bt-\lambda b\frac{t}{\sqrt{n}}\\
 & = & -\frac{b^{2}t^{2}}{\tilde{\alpha}}+\frac{b^{3}t^{3}}{2\sqrt{n}\tilde{\alpha}^{2}}-o\left(\frac{t^{3}}{\sqrt{n}}\right)-\sqrt{n}bt+\frac{b^{2}t^{2}}{2\tilde{\alpha}}-\frac{b^{3}t^{3}}{2\sqrt{n}\alpha^{2}}-o\left(\frac{t^{3}}{\sqrt{n}}\right)\\
 &  & +\frac{bt}{2\sqrt{n}\tilde{\alpha}}-\frac{b^{2}t^{2}}{4n\tilde{\alpha}^{2}}+o\left(\frac{t^{2}}{n}\right)+\sqrt{n}bt-\lambda b\frac{t}{\sqrt{n}}\\
 & \approx & -\frac{b^{2}t^{2}}{2\tilde{\alpha}}.
\end{eqnarray*}

\end_inset

If we set 
\begin_inset Formula $b=\sqrt{\tilde{\alpha}}$
\end_inset

, then the rescaled partial posterior convergence to standard normal.
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "sub:Derivation-of-Example-3"

\end_inset

Derivation of Example 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:laplace-example"

\end_inset


\end_layout

\begin_layout Standard

\color black
First we prove lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:bayes-inferential-model"

\end_inset

.
\end_layout

\begin_layout Proof

\color black
Assume 
\begin_inset Formula $Y$
\end_inset

 has a probability density function 
\begin_inset Formula $f\left(y\right)$
\end_inset

.
 Let 
\begin_inset Formula $y=u\left(x,\theta\right)$
\end_inset

 be the solution of equation 
\begin_inset Formula $x=h\left(y,\theta\right)$
\end_inset

.
 Then 
\begin_inset Formula $h\left(Y,\theta\right)$
\end_inset

 has a probability density function
\begin_inset Formula 
\[
f\left(u\left(x,\theta\right)\right)\left|\frac{\partial u\left(x,\theta\right)}{\partial x}\right|.
\]

\end_inset

Then the posterior distribution under the uniform prior is proportional
 to 
\begin_inset Formula 
\[
f\left(u\left(X,\theta\right)\right)\left|\frac{\partial u\left(X,\theta\right)}{\partial x}\right|.
\]

\end_inset

Now we find the probability density function of 
\begin_inset Formula $g\left(Y,X\right)$
\end_inset

.
 By assumptions, we know 
\begin_inset Formula $y=u\left(x,\theta\right)$
\end_inset

 is also the solution of 
\begin_inset Formula $\theta=g\left(y,x\right)$
\end_inset

.
 Hence the probability density function is also 
\begin_inset Formula 
\[
f\left(u\left(X,\theta\right)\right)\left|\frac{\partial u\left(X,\theta\right)}{\partial x}\right|.
\]

\end_inset


\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
We know if 
\begin_inset Formula $X,Y$
\end_inset

 are independent exponential random variables with mean 
\begin_inset Formula $\lambda$
\end_inset

, then 
\begin_inset Formula $X-Y$
\end_inset

 is Laplace distributed with 
\begin_inset Formula $\mu=0$
\end_inset

 and the same 
\begin_inset Formula $\lambda$
\end_inset

.
 So we know our sample 
\begin_inset Formula $Z$
\end_inset

 has the same distribution as 
\begin_inset Formula $X-Y+\mu$
\end_inset

.
 So the sample mean 
\begin_inset Formula $\overline{Z}$
\end_inset

 has the same distribution as 
\begin_inset Formula $\overline{X}-\overline{Y}+\mu$
\end_inset

.
 It is easy to check 
\begin_inset Formula $\overline{X}$
\end_inset

 and 
\begin_inset Formula $\overline{Y}$
\end_inset

 have gamma distribution with location parameter 
\begin_inset Formula $n$
\end_inset

 and scale parameter 
\begin_inset Formula $n^{-1}\lambda$
\end_inset

.
 Hence the posterior distribution of 
\begin_inset Formula $\mu$
\end_inset

on 
\begin_inset Formula $\overline{Z}$
\end_inset

 under the uniform prior has the same distribution as 
\begin_inset Formula $\overline{Z}-\left(\overline{X}-\overline{Y}\right)$
\end_inset

.
 Hence the posterior distribution 
\begin_inset Formula $\sqrt{n}\left(\mu-\overline{Z}\right)$
\end_inset

 has the same distribution as 
\begin_inset Formula $-\sqrt{n}\left(\overline{X}-\overline{Y}\right)$
\end_inset

 We know the characteristic function of 
\begin_inset Formula $\sqrt{n}$
\end_inset


\begin_inset Formula $\overline{X}$
\end_inset

 is 
\begin_inset Formula 
\[
\left[1-\frac{\lambda}{n}i\left(\sqrt{n}t\right)\right]^{-n},
\]

\end_inset

So the characteristic function of 
\begin_inset Formula $-\sqrt{n}\left(\overline{X}-\overline{Y}\right)$
\end_inset

 is 
\begin_inset Formula 
\[
\left[1-\frac{\lambda}{n}i\left(\sqrt{n}t\right)\right]^{-n}\left[1-\frac{\lambda}{n}i\left(-\sqrt{n}t\right)\right]^{-n}=\left(1+\frac{\lambda^{2}t^{2}}{n}\right)^{-n}\rightarrow\exp\left(-\lambda^{2}t^{2}\right).
\]

\end_inset

Hence 
\begin_inset Formula $\sqrt{n}\left(\mu-\overline{Z}\right)$
\end_inset

 has an asymptotic normal distribution with zero mean and variance 
\begin_inset Formula $2\lambda^{2}$
\end_inset

.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Proof-of-Theorem-2"

\end_inset

Proof of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:partial-post-m-est"

\end_inset


\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:taylor-expansion-in-dist"

\end_inset

Under Assumptions 
\begin_inset CommandInset ref
LatexCommand ref
reference "assu:second-order-bounded-differential"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "assu:m-est-consistent-asymp-norml"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "assu:bernstein-von-mises-full-posterior"

\end_inset

, for any 
\begin_inset Formula $\varepsilon$
\end_inset

, 
\begin_inset Formula $\delta_{1}$
\end_inset

 and 
\begin_inset Formula $\delta_{2}$
\end_inset

, there exists an 
\begin_inset Formula $N$
\end_inset

, such that for any 
\begin_inset Formula $n\ge N$
\end_inset

, 
\begin_inset Formula 
\[
P_{\theta_{0}}^{\infty}\left(\omega:P_{\omega}^{n}\left(\sqrt{n}\left|G\left(\theta,\tilde{\theta}\right)-G_{1}\left(\hat{\theta},\tilde{\theta}\right)\left(\theta-\tilde{\theta}\right)\right|\le2\varepsilon\right)\ge1-\delta_{1}\right)\ge1-\delta_{2}.
\]

\end_inset


\end_layout

\begin_layout Proof
By Taylor expansion and Assumption 
\begin_inset CommandInset ref
LatexCommand ref
reference "assu:second-order-bounded-differential"

\end_inset

, 
\begin_inset Formula 
\begin{equation}
\left|G\left(\theta,\tilde{\theta}\right)-G\left(\hat{\theta},\tilde{\theta}\right)-G_{1}\left(\hat{\theta},\tilde{\theta}\right)\left(\theta-\hat{\theta}\right)\right|\le L\left(\theta-\hat{\theta}\right)^{2},\label{eq:second-lip-1}
\end{equation}

\end_inset

and 
\begin_inset Formula 
\begin{equation}
\left|G\left(\tilde{\theta},\tilde{\theta}\right)-G\left(\hat{\theta},\tilde{\theta}\right)-G_{1}\left(\hat{\theta},\tilde{\theta}\right)\left(\tilde{\theta}-\hat{\theta}\right)\right|\le L\left(\tilde{\theta}-\hat{\theta}\right)^{2}.\label{eq:second-lip-2}
\end{equation}

\end_inset

By posterior consistency, there exists a 
\begin_inset Formula $\Omega_{1}\subset\Omega$
\end_inset

, 
\begin_inset Formula $P_{\theta_{0}}^{\infty}\left(\Omega_{1}\right)=1$
\end_inset

, such that for any 
\begin_inset Formula $\omega\in\Omega_{1}$
\end_inset

, random variable 
\begin_inset Formula $\left(\theta\mid X_{1}\left(\omega\right),\ldots,X_{n}\left(\omega\right)\right)$
\end_inset

 converges in probability to 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 Hence 
\begin_inset Formula $\left(\theta-\theta_{0}\right)\mid X_{1}\left(\omega\right),\ldots,X_{n}\left(\omega\right)=o_{P_{\omega}^{n}}\left(1\right)$
\end_inset

.
 By Bernstein--von Mises, there exists a 
\begin_inset Formula $\Omega_{2}\subset\Omega$
\end_inset

, 
\begin_inset Formula $P_{\theta_{0}}^{\infty}\left(\Omega_{2}\right)=1$
\end_inset

, such that for any 
\begin_inset Formula $\omega\in\Omega_{2}$
\end_inset

, random variable 
\begin_inset Formula $\left(\sqrt{n\hat{I}}\left(\theta-\hat{\theta}\right)\mid X_{1}\left(\omega\right),\ldots,X_{n}\left(\omega\right)\right)$
\end_inset

 converges in distribution to a standard normal random variable.
 Hence 
\begin_inset Formula $\left(\sqrt{n\hat{I}}\left(\theta-\hat{\theta}\right)\mid X_{1}\left(\omega\right),\ldots,X_{n}\left(\omega\right)\right)=O_{P_{\omega}^{n}}\left(1\right).$
\end_inset

 For any 
\begin_inset Formula $\omega\in\Omega_{1}\cap\Omega_{2}$
\end_inset

, 
\begin_inset Formula 
\begin{eqnarray*}
\left(\sqrt{n}L\left(\theta-\hat{\theta}\right)^{2}\mid X_{1}\left(\omega\right),\ldots,X_{n}\left(\omega\right)\right) & = & L\left(\sqrt{n}\left(\theta-\hat{\theta}\right)\times\left(\theta-\hat{\theta}\right)\mid X_{1}\left(\omega\right),\ldots,X_{n}\left(\omega\right)\right)\\
 & = & LO_{P_{\omega}^{n}}\left(1\right)\times o_{P_{\omega}^{n}}\left(1\right)=o_{P_{\omega}^{n}}\left(1\right),
\end{eqnarray*}

\end_inset

which means for any 
\begin_inset Formula $\varepsilon$
\end_inset

 and 
\begin_inset Formula $\delta_{1}$
\end_inset

, there exists an 
\begin_inset Formula $N_{1}$
\end_inset

 such that 
\begin_inset Formula 
\[
P_{\omega}^{n}\left(\sqrt{n}L\left(\theta-\hat{\theta}\left(\omega\right)\right)^{2}\le\varepsilon\mid X_{1}\left(\omega\right),\ldots,X_{n}\left(\omega\right)\right)\ge1-\delta_{1}.
\]

\end_inset

By Assumptions 
\begin_inset CommandInset ref
LatexCommand ref
reference "assu:m-est-consistent-asymp-norml"

\end_inset

, 
\begin_inset Formula $\tilde{\theta}-\theta_{0}=o_{P_{\theta_{0}}^{\infty}}\left(1\right)$
\end_inset

, 
\begin_inset Formula $\sqrt{n}\left(\tilde{\theta}-\theta_{0}\right)=O_{P_{\theta_{0}}^{\infty}}\left(1\right)$
\end_inset

, 
\begin_inset Formula $\hat{\theta}-\theta_{0}=o_{P_{\theta_{0}}^{\infty}}\left(1\right)$
\end_inset

 and 
\begin_inset Formula $\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)=O_{P_{\theta_{0}}^{\infty}}\left(1\right).$
\end_inset

Then 
\begin_inset Formula 
\[
\sqrt{n}L\left(\tilde{\theta}-\hat{\theta}\right)^{2}\le2L\left[\sqrt{n}\left(\tilde{\theta}-\theta_{0}\right)^{2}+\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)^{2}\right]=o_{P_{\theta_{0}}^{\infty}}\left(1\right),
\]

\end_inset

which means for any 
\begin_inset Formula $\varepsilon$
\end_inset

 and 
\begin_inset Formula $\delta_{2}$
\end_inset

, there exists an 
\begin_inset Formula $N_{2}$
\end_inset

, such that for any 
\begin_inset Formula $n\ge N_{2}$
\end_inset

, 
\begin_inset Formula 
\[
P_{\theta_{0}}^{\infty}\left(\omega:\sqrt{n}L\left(\tilde{\theta}\left(\omega\right)-\hat{\theta}\left(\omega\right)\right)^{2}\le\varepsilon\right)\ge1-\delta_{2}.
\]

\end_inset

Let 
\begin_inset Formula $\Omega_{\varepsilon}=\left\{ \omega:\sqrt{n}L\left(\tilde{\theta}\left(\omega\right)-\hat{\theta}\left(\omega\right)\right)^{2}\le\varepsilon\right\} $
\end_inset

.
 For any 
\begin_inset Formula $\omega\in\Omega_{1}\cap\Omega_{2}\cap\Omega_{\varepsilon}$
\end_inset

, 
\begin_inset Formula 
\begin{eqnarray*}
 &  & \sqrt{n}\left|G\left(\theta,\tilde{\theta}\right)-G_{1}\left(\hat{\theta},\tilde{\theta}\right)\left(\theta-\tilde{\theta}\right)\right|\\
 & \le & \sqrt{n}\left|G\left(\theta,\tilde{\theta}\right)-G\left(\hat{\theta},\tilde{\theta}\right)-G_{1}\left(\hat{\theta},\tilde{\theta}\right)\left(\theta-\hat{\theta}\right)\right|\\
 &  & +\sqrt{n}\left|G\left(\tilde{\theta},\tilde{\theta}\right)-G\left(\hat{\theta},\tilde{\theta}\right)-G_{1}\left(\hat{\theta},\tilde{\theta}\right)\left(\tilde{\theta}-\hat{\theta}\right)\right|\\
 & \le & \sqrt{n}L\left(\theta-\hat{\theta}\right)^{2}+\sqrt{n}L\left(\tilde{\theta}-\hat{\theta}\right)^{2}\le2\varepsilon,
\end{eqnarray*}

\end_inset

with probability 
\begin_inset Formula $1-\delta_{1}$
\end_inset

 
\begin_inset Formula $\left(P_{\omega}^{n}\right)$
\end_inset

.
 Also recall 
\begin_inset Formula $P_{\theta_{0}}^{\infty}\left(\Omega_{1}\cap\Omega_{2}\cap\Omega_{\varepsilon}\right)\ge1-\delta_{2}$
\end_inset

.
 Hence for and 
\begin_inset Formula $n\ge\max\left\{ N_{1},N_{2}\right\} $
\end_inset

, 
\begin_inset Formula 
\[
P_{\theta_{0}}^{\infty}\left(\omega:P_{\omega}^{n}\left(\sqrt{n}\left|G\left(\theta,\tilde{\theta}\right)-G_{1}\left(\hat{\theta},\tilde{\theta}\right)\left(\theta-\tilde{\theta}\right)\right|\le2\varepsilon\right)\ge1-\delta_{1}\right)\ge1-\delta_{2}.
\]

\end_inset


\end_layout

\begin_layout Remark
This result is weaker than the settings in posterior consistency and Bernstein--
von Mises theorem.
 In posterior consistency, the posterior random variable 
\begin_inset Formula $\left(\theta\mid X_{1},\ldots,X_{n}\right)$
\end_inset

 is convergence in probability in 
\begin_inset Formula $P_{\omega}^{n}$
\end_inset

 almost surely in 
\begin_inset Formula $P_{\theta_{0}}^{\infty}$
\end_inset

.
 Similar statement can be expressed in Bernstein--von Mises.
 However, in this lemma, the rescaled posterior random variable is convergence
 in probability in 
\begin_inset Formula $P_{\omega}^{n}$
\end_inset

 in probability in 
\begin_inset Formula $P_{\theta_{0}}^{\infty}$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Remark
There is slight difference between the rescaled posterior random variable
 in this lemma and in 
\begin_inset Formula 
\begin{equation}
\sqrt{n}\left[\int_{\mathbb{R}}g\left(x,\tilde{\theta}\right)\pi\left(x\mid\theta\right)\diff x-\int_{\mathbb{R}}g\left(x,\tilde{\theta}\right)\pi\left(x\mid\tilde{\theta}\right)\diff x-\left(\left.\frac{\diff}{\diff\theta}\int_{\mathbb{R}}g\left(x,\tilde{\theta}\right)\pi\left(x\mid\theta\right)\diff x\right|_{\theta=\tilde{\theta}}\right)\left(\theta-\tilde{\theta}\right)\right]\rightarrow0,\ascv,\label{eq:strong-vanish-error}
\end{equation}

\end_inset

.
 The first order differential term is 
\begin_inset Formula $G_{1}\left(\tilde{\theta},\tilde{\theta}\right)$
\end_inset

.
 However, since 
\begin_inset Formula $G_{1}\left(\tilde{\theta},\tilde{\theta}\right)\rightarrow G_{1}\left(\theta_{0},\theta_{0}\right)$
\end_inset

 and 
\begin_inset Formula $G_{1}\left(\hat{\theta},\tilde{\theta}\right)\rightarrow G_{1}\left(\theta_{0},\theta_{0}\right)$
\end_inset

 almost surely in 
\begin_inset Formula $P_{\theta_{0}}^{\infty}$
\end_inset

and 
\begin_inset Formula $\sqrt{n}$
\end_inset

 term is absorbed by 
\begin_inset Formula $\left(\theta-\tilde{\theta}\right)$
\end_inset

, we have proved a weak version of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:strong-vanish-error"

\end_inset

.
 
\end_layout

\end_deeper
\begin_layout Proof
Under the Assumptions 
\begin_inset CommandInset ref
LatexCommand ref
reference "assu:second-order-bounded-differential"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "assu:m-est-consistent-asymp-norml"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "assu:bernstein-von-mises-full-posterior"

\end_inset

, we have the result from Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:taylor-expansion-in-dist"

\end_inset

, 
\begin_inset Formula 
\begin{equation}
P_{\theta_{0}}^{\infty}\left(\omega:P_{\omega}^{n}\left(\sqrt{n}\left|G\left(\theta,\tilde{\theta}\right)-G_{1}\left(\hat{\theta},\tilde{\theta}\right)\left(\theta-\tilde{\theta}\right)\right|\le2\varepsilon\right)\ge1-\delta_{1}\right)\ge1-\delta_{2}.\label{eq:conv-in-dist-reminder}
\end{equation}

\end_inset

Let 
\begin_inset Formula $\Omega_{1}=\left\{ \omega:P_{\omega}^{n}\left(\sqrt{n}\left|G\left(\theta,\tilde{\theta}\right)-G_{1}\left(\hat{\theta},\tilde{\theta}\right)\left(\theta-\tilde{\theta}\right)\right|\le2\varepsilon\right)\ge1-\delta_{1}\right\} $
\end_inset

, 
\begin_inset Formula $C_{n}=E_{\pi\left(\theta\right)}E_{P^{\infty}\left(\theta_{0}\right)}I_{\left[\tilde{\theta}\in O\left(\theta_{0},\varepsilon\right)\right]}\exp\left(\sum_{i=1}^{n}\log f\left(X_{i}\mid\theta\right)-\log f\left(X_{i}\mid\theta_{0}\right)\right)$
\end_inset

.
 Now by the same technique used in the proof of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:bernstein-von-mises-mle"

\end_inset

, we have for sufficiently large 
\begin_inset Formula $N$
\end_inset

, 
\begin_inset Formula 
\begin{eqnarray*}
 &  & \left|P\left(\left.\frac{\sqrt{n}\left(\theta-\tilde{\theta}\right)}{\sqrt{\tilde{V}}}\le t\right|\tilde{\theta}\in O\left(\theta_{0},\varepsilon\right)\right)-\Phi\left(t\right)\right|\\
 & = & \left|\frac{E_{\pi\left(\theta\right)}E_{P^{\infty}\left(\theta_{0}\right)}I_{\left[\tilde{\theta}\in O\left(\theta_{0},\varepsilon\right)\right]}I_{\left[\sqrt{n\tilde{V}^{-1}}\left(\theta-\tilde{\theta}\right)\le t\right]}\prod_{i=1}^{n}f\left(X_{i}\mid\theta\right)/f\left(X_{i}\mid\theta_{0}\right)}{E_{\pi\left(\theta\right)}E_{P^{\infty}\left(\theta_{0}\right)}I_{\left[\tilde{\theta}\in O\left(\theta_{0},\varepsilon\right)\right]}\prod_{i=1}^{n}f\left(X_{i}\mid\theta\right)/f\left(X_{i}\mid\theta_{0}\right)}-\Phi\left(t\right)\right|\\
 & \le & C_{n}^{-1}E_{\pi\left(\theta\right)}E_{P^{\infty}\left(\theta_{0}\right)}I_{\Omega_{1}}\left|P\left(\sqrt{n\tilde{V}^{-1}}\left(\theta-\tilde{\theta}\right)\le t\mid X_{1},\ldots,X_{n}\right)-\Phi\left(t\right)\right|\prod_{i=1}^{n}f\left(X_{i}\mid\theta\right)/f\left(X_{i}\mid\theta_{0}\right)\\
 &  & +C_{n}^{-1}E_{\pi\left(\theta\right)}E_{P^{\infty}\left(\theta_{0}\right)}I_{\Omega_{1}^{c}}\left|P\left(\sqrt{n\tilde{V}^{-1}}\left(\theta-\tilde{\theta}\right)\le t\mid X_{1},\ldots,X_{n}\right)-\Phi\left(t\right)\right|\prod_{i=1}^{n}f\left(X_{i}\mid\theta\right)/f\left(X_{i}\mid\theta_{0}\right).
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Proof
First considering the samples within 
\begin_inset Formula $\Omega_{1}$
\end_inset

, by 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:conv-in-dist-reminder"

\end_inset

, 
\begin_inset Formula $\sqrt{V_{0}^{-1}}\left\{ \sqrt{n}\left[G_{1}\left(\tilde{\theta},\tilde{\theta}\right)\left(\theta-\tilde{\theta}\right)-G\left(\theta,\tilde{\theta}\right)\right]\right\} $
\end_inset

 converges in probability to 0.
 By Theorem 2.1 in 
\begin_inset CommandInset citation
LatexCommand citet
key "rivoirard2012bernstein"

\end_inset

, we have, 
\begin_inset Formula 
\begin{equation}
\lim_{n\rightarrow\infty}\sup_{t\in\mathbb{R}}\left|P\left(\sqrt{nV_{0}^{-1}}\left(G\left(\theta,\tilde{\theta}\right)-\frac{1}{n}\sum_{i=1}^{n}g\left(X_{i},\tilde{\theta}\right)\right)\le t\mid X_{1},\ldots,X_{n}\right)-\Phi\left(t\right)\right|=0,\ascv\theta_{0},\label{eq:conv-in-dist-est-eq}
\end{equation}

\end_inset

where 
\begin_inset Formula 
\[
V_{0}=\int_{\mathbb{R}}\left(g\left(x,\tilde{\theta}\right)-\int_{\mathbb{R}}g\left(y,\tilde{\theta}\right)\pi\left(y\mid\theta_{0}\right)\diff y\right)^{2}\pi\left(x\mid\theta_{0}\right)\diff x.
\]

\end_inset

Hence, 
\begin_inset Formula $\sqrt{nV_{0}^{-1}}\left(G\left(\theta,\tilde{\theta}\right)-n^{-1}\sum_{i=1}^{n}g\left(X_{i},\tilde{\theta}\right)\right)$
\end_inset

 converges in distribution to standard normal distribution.
 By the definition of 
\begin_inset Formula $M$
\end_inset

-estimator, 
\begin_inset Formula $n^{-1}\sum_{i=1}^{n}g\left(X_{i},\tilde{\theta}\right)=0$
\end_inset

.
 Assume that for every 
\begin_inset Formula $\theta$
\end_inset

, the equation in 
\begin_inset Formula $t$
\end_inset

, 
\begin_inset Formula $\int_{\mathbb{R}}g\left(x,t\right)\pi\left(x\mid\theta\right)\diff x=0$
\end_inset

 has only one solution 
\begin_inset Formula $t=\theta$
\end_inset

, then 
\begin_inset Formula 
\[
G\left(\tilde{\theta},\tilde{\theta}\right)=\int_{\mathbb{R}}g\left(x,\tilde{\theta}\right)\pi\left(x\mid\tilde{\theta}\right)\diff x=0.
\]

\end_inset

Hence,
\begin_inset Formula 
\[
\sqrt{n\tilde{V}^{-1}}\left(\theta-\tilde{\theta}\right)=\sqrt{nV_{0}^{-1}}\left(G\left(\theta,\tilde{\theta}\right)-\frac{1}{n}\sum_{i=1}^{n}g\left(X_{i},\tilde{\theta}\right)\right)+\sqrt{V_{0}^{-1}}\left\{ \sqrt{n}\left[G_{1}\left(\tilde{\theta},\tilde{\theta}\right)\left(\theta-\tilde{\theta}\right)-G\left(\theta,\tilde{\theta}\right)\right]\right\} ,
\]

\end_inset

by Slutsky's theorem, converges in distribution to standard normal distribution.
 Hence for large 
\begin_inset Formula $N$
\end_inset

, 
\begin_inset Formula 
\[
\sup_{t\in\mathbb{R}}\left|P\left(\sqrt{n\tilde{V}^{-1}}\left(\theta-\tilde{\theta}\right)\le t\mid X_{1},\ldots,X_{n}\right)-\Phi\left(t\right)\right|\le\varepsilon,
\]

\end_inset

and 
\begin_inset Formula 
\begin{eqnarray}
 &  & C_{n}^{-1}E_{\pi\left(\theta\right)}E_{P^{\infty}\left(\theta_{0}\right)}I_{\Omega_{1}}\sup_{t\in\mathbb{R}}\left|P\left(\sqrt{n\tilde{V}^{-1}}\left(\theta-\tilde{\theta}\right)\le t\mid X_{1},\ldots,X_{n}\right)-\Phi\left(t\right)\right|\prod_{i=1}^{n}f\left(X_{i}\mid\theta\right)/f\left(X_{i}\mid\theta_{0}\right)\nonumber \\
 & \le & \varepsilon C_{n}^{-1}E_{\pi\left(\theta\right)}E_{P^{\infty}\left(\theta_{0}\right)}I_{\Omega_{1}}\prod_{i=1}^{n}f\left(X_{i}\mid\theta\right)/f\left(X_{i}\mid\theta_{0}\right)=\varepsilon.\label{eq:conv-with-omega1}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Proof
Next, considering the samples outside 
\begin_inset Formula $\Omega_{1}$
\end_inset

.
 It is trivial that 
\begin_inset Formula 
\[
\sup_{t\in\mathbb{R}}\left|P\left(\sqrt{n\tilde{V}^{-1}}\left(\theta-\tilde{\theta}\right)\le t\mid X_{1},\ldots,X_{n}\right)-\Phi\left(t\right)\right|\le2.
\]

\end_inset

By Assumption 
\begin_inset CommandInset ref
LatexCommand ref
reference "assu:theo-mle"

\end_inset

 and the strong law of large numbers, and the property of the Kullback-Leibler
 information number 
\begin_inset Formula 
\[
\prod_{i=1}^{n}f\left(X_{i}\mid\theta\right)/f\left(X_{i}\mid\theta_{0}\right)=\exp\left(n\left(\frac{1}{n}\sum_{i=1}^{n}\log f\left(X_{i}\mid\theta\right)-\frac{1}{n}\sum_{i=1}^{n}\log f\left(X_{i}\mid\theta_{0}\right)\right)\right)\le1.\ascv\left(P_{\theta_{0}}\right)
\]

\end_inset

Hence
\begin_inset Formula 
\begin{eqnarray}
 &  & C_{n}^{-1}E_{\pi\left(\theta\right)}E_{P^{\infty}\left(\theta_{0}\right)}I_{\Omega_{1}^{c}}\sup_{t\in\mathbb{R}}\left|P\left(\sqrt{n\tilde{V}^{-1}}\left(\theta-\tilde{\theta}\right)\le t\mid X_{1},\ldots,X_{n}\right)-\Phi\left(t\right)\right|\prod_{i=1}^{n}f\left(X_{i}\mid\theta\right)/f\left(X_{i}\mid\theta_{0}\right).\nonumber \\
 & \le & 2C_{n}^{-1}E_{\pi\left(\theta\right)}E_{P^{\infty}\left(\theta_{0}\right)}I_{\Omega_{1}^{c}}=2P^{\infty}\left(\Omega_{1}^{c}\mid\theta_{0}\right)=2\delta_{2}.\label{eq:conv-outside-omega1}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Proof
Hence, combining 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:conv-with-omega1"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:conv-outside-omega1"

\end_inset

, 
\begin_inset Formula 
\[
\sup_{t\in\mathbb{R}}\left|P\left(\left.\frac{\sqrt{n}\left(\theta-\tilde{\theta}\right)}{\sqrt{\tilde{V}}}\le t\right|\tilde{\theta}\in O\left(\theta_{0},\varepsilon\right)\right)-\Phi\left(t\right)\right|\le\varepsilon+2\delta_{2}.\ascv
\]

\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Proof-of-Theorem-3"

\end_inset

Proof of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:bernsten-von-mise-inconsist-multv"

\end_inset


\end_layout

\begin_layout Proof
By Theorem 2.1 in 
\begin_inset CommandInset citation
LatexCommand citet
key "rivoirard2012bernstein"

\end_inset

, we have 
\begin_inset Formula 
\begin{equation}
\lim_{n\rightarrow\infty}\sup_{t\in\mathbb{R}}\left|P\left(\sqrt{n\Var_{\theta_{0}}\left(a^{T}g\left(X\right)\right)^{-1}}\left(\int a^{T}g\left(x\right)f\left(x\mid\theta\right)\diff x-\frac{1}{n}\sum_{i=1}^{n}a^{T}g\left(X_{i}\right)\right)\le t\mid X_{1},\ldots,X_{n}\right)-\Phi\left(t\right)\right|=0,\ascv.\label{eq:thm2-1-inconsist}
\end{equation}

\end_inset

By Assumption 
\begin_inset CommandInset ref
LatexCommand ref
reference "assu:super-strong-consistent"

\end_inset

 and Slutsky's theorem, we know 
\begin_inset Formula $n^{-1/2}\sum_{i=1}^{n}g\left(X_{i}\right)$
\end_inset

 has the same asymptotic distribution as 
\begin_inset Formula $S$
\end_inset

.
 However, by central limit theorem, 
\begin_inset Formula $n^{-1/2}\sum_{i=1}^{n}g\left(X_{i}\right)$
\end_inset

 has an asymptotic normal distribution with variance matrix as 
\begin_inset Formula $\Var_{\theta_{0}}\left(g\left(X\right)\right)$
\end_inset

.
 Hence, 
\begin_inset Formula $\Var_{\theta_{0}}\left(g\left(X\right)\right)=\Sigma\left(\theta_{0}\right)=\lim_{n\rightarrow\infty}\tilde{\Sigma},\ascv.$
\end_inset

 Hence, we can replace 
\begin_inset Formula $\Var_{\theta_{0}}\left(g\left(X\right)\right)$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:thm2-1-inconsist"

\end_inset

 by its strong consistent estimator, and get
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\sup_{t\in\mathbb{R}}\left|P\left(\sqrt{n\left(a^{T}\tilde{\Sigma}a\right)^{-1}}\left(\int a^{T}g\left(x\right)f\left(x\mid\theta\right)\diff x-n^{-1}\sum_{i=1}^{n}a^{T}g\left(X_{i}\right)\right)\le t\mid X_{1},\ldots,X_{n}\right)-\Phi\left(t\right)\right|=0,\ascv.
\]

\end_inset

By Assumption 
\begin_inset CommandInset ref
LatexCommand ref
reference "assu:super-strong-consistent"

\end_inset

, we can replace 
\begin_inset Formula $\sqrt{n}\left(n^{-1}\sum_{i=1}^{n}a^{T}g\left(X_{i}\right)\right)$
\end_inset

 by 
\begin_inset Formula $\sqrt{n}a^{T}S$
\end_inset

, and finally obtain
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\sup_{t\in\mathbb{R}}\left|P\left(\sqrt{n\left(a^{T}\tilde{\Sigma}a\right)^{-1}}\left(\int a^{T}g\left(x\right)f\left(x\mid\theta\right)\diff x-a^{T}S\right)\le t\mid X_{1},\ldots,X_{n}\right)-\Phi\left(t\right)\right|=0,\ascv.
\]

\end_inset

The remainder of the proof uses the same argument used in the proof of Theorem
 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:bernstein-von-mises-mle"

\end_inset

.
 
\end_layout

\begin_layout Chapter*
Reference
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "dissertation,partialPostBib,asymExpBib"
options "elsarticle-harv"

\end_inset


\end_layout

\end_body
\end_document
